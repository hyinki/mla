{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Singapore Airlines' Service Through Automated Sentiment Analysis of Customer Reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singapore Airlines Customer Reviews Dataset Information\n",
    "\n",
    "The [Singapore Airlines Customer Reviews Dataset](https://www.kaggle.com/datasets/kanchana1990/singapore-airlines-reviews) aggregates 10,000 anonymized customer reviews, providing a broad perspective on the passenger experience with Singapore Airlines. \n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`published_date`**: Date and time of review publication.\n",
    "- **`published_platform`**: Platform where the review was posted.\n",
    "- **`rating`**: Customer satisfaction rating, from 1 (lowest) to 5 (highest).\n",
    "- **`type`**: Specifies the content as a review.\n",
    "- **`text`**: Detailed customer feedback.\n",
    "- **`title`**: Summary of the review.\n",
    "- **`helpful_votes`**: Number of users finding the review helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional web scraping of online reviews\n",
    "\n",
    "During our EDA, we noticed two main trends in the distribution of our dataset:\n",
    "1. Less than 10% of our reviews were published from the years 2022 to 2024, making it hard for us to capture recent trends in sentiment.\n",
    "2. Most of the reviews were highly positive, which could mean that SIA had mostly positive reviews, nevertheless we wanted to get more information on negative reviews to improve the robustness of our model.\n",
    "\n",
    "### TripAdvisor\n",
    "\n",
    "We scraped more data for airline reviews from TripAdvisor, specifically for the years 2022 to 2024. \n",
    "(https://www.tripadvisor.com.sg/Airline_Review-d8729151-Reviews-Singapore-Airlines)\n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`Year`**: Year of review publication.\n",
    "- **`Month`**: Month of review publication.\n",
    "- **`Title`**: Title of review publication.\n",
    "- **`Review Text`**: Main text content of review publication.\n",
    "- **`Rating`**: Numerical rating provided by reviewer (Scale: 1 to 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skytrax\n",
    "\n",
    "We also scraped from Skytrax, which is another data source for online reviews. \n",
    "(https://www.airlinequality.com/airline-reviews/singapore-airlines/?sortby=post_date%3ADesc&pagesize=100)\n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`Year`**: Year of review publication.\n",
    "- **`Month`**: Month of review publication.\n",
    "- **`Title`**: Title of review publication.\n",
    "- **`Review Text`**: Main text content of review publication.\n",
    "- **`Rating`**: Numerical rating provided by reviewer (Scale: 1 to 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Please uncomment the code box below to pip install relevant dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=2.0.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.24.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 2)) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 3)) (1.12.0)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 4)) (4.66.5)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 5)) (3.8.2)\n",
      "Requirement already satisfied: seaborn>=0.12.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
      "Requirement already satisfied: langdetect>=1.0.9 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 7)) (1.0.9)\n",
      "Requirement already satisfied: langid>=1.1.6 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 8)) (1.1.6)\n",
      "Requirement already satisfied: nltk>=3.8.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 9)) (3.9.1)\n",
      "Requirement already satisfied: wordcloud>=1.9.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 10)) (1.9.3)\n",
      "Collecting tensorflow>=2.17.1 (from -r requirements.txt (line 14))\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: scikeras>=0.10.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 15)) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2023.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.65.0->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (3.1.1)\n",
      "Requirement already satisfied: six in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langdetect>=1.0.9->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (2024.7.24)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow>=2.17.1->-r requirements.txt (line 14))\n",
      "  Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (74.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (1.66.1)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14))\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.5.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.4.0)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikeras>=0.10.0->-r requirements.txt (line 15)) (1.5.2)\n",
      "Requirement already satisfied: rich in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (13.8.0)\n",
      "Requirement already satisfied: namex in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.12.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras>=0.10.0->-r requirements.txt (line 15)) (3.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.44.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2023.11.17)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.0.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.1.4)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl (390.3 MB)\n",
      "   ---------------------------------------- 0.0/390.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.2/390.3 MB 22.9 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 10.0/390.3 MB 24.8 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 15.5/390.3 MB 25.6 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 21.0/390.3 MB 26.0 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 27.0/390.3 MB 26.7 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 32.8/390.3 MB 26.7 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 34.3/390.3 MB 26.6 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 34.3/390.3 MB 26.6 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 37.7/390.3 MB 20.0 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 40.4/390.3 MB 20.4 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 40.4/390.3 MB 20.4 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 40.4/390.3 MB 20.4 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 40.4/390.3 MB 20.4 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 45.1/390.3 MB 15.3 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 49.8/390.3 MB 16.1 MB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 49.8/390.3 MB 16.1 MB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 49.8/390.3 MB 16.1 MB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 50.3/390.3 MB 13.3 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 56.6/390.3 MB 14.2 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 59.0/390.3 MB 14.5 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 59.0/390.3 MB 14.5 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 59.0/390.3 MB 14.5 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 60.6/390.3 MB 12.5 MB/s eta 0:00:27\n",
      "   ------ --------------------------------- 66.1/390.3 MB 13.1 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 71.8/390.3 MB 13.7 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 72.6/390.3 MB 13.7 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 72.6/390.3 MB 13.7 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 76.5/390.3 MB 13.0 MB/s eta 0:00:25\n",
      "   -------- ------------------------------- 82.3/390.3 MB 13.5 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 82.3/390.3 MB 13.5 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 82.3/390.3 MB 13.5 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 82.3/390.3 MB 13.5 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 82.6/390.3 MB 12.2 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 82.8/390.3 MB 11.9 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 86.2/390.3 MB 11.7 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 91.0/390.3 MB 12.0 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 95.9/390.3 MB 12.3 MB/s eta 0:00:24\n",
      "   ---------- ---------------------------- 101.7/390.3 MB 12.7 MB/s eta 0:00:23\n",
      "   ---------- ---------------------------- 106.4/390.3 MB 13.0 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 109.8/390.3 MB 13.2 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 109.8/390.3 MB 13.2 MB/s eta 0:00:22\n",
      "   ----------- --------------------------- 111.4/390.3 MB 12.6 MB/s eta 0:00:23\n",
      "   ----------- --------------------------- 117.2/390.3 MB 13.0 MB/s eta 0:00:22\n",
      "   ------------ -------------------------- 123.2/390.3 MB 13.3 MB/s eta 0:00:21\n",
      "   ------------ -------------------------- 124.3/390.3 MB 13.4 MB/s eta 0:00:20\n",
      "   ------------ -------------------------- 124.3/390.3 MB 13.4 MB/s eta 0:00:20\n",
      "   ------------ -------------------------- 124.8/390.3 MB 12.8 MB/s eta 0:00:21\n",
      "   ------------ -------------------------- 125.6/390.3 MB 12.4 MB/s eta 0:00:22\n",
      "   ------------ -------------------------- 126.9/390.3 MB 12.3 MB/s eta 0:00:22\n",
      "   ------------ -------------------------- 129.0/390.3 MB 12.3 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 131.6/390.3 MB 12.3 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 134.7/390.3 MB 12.3 MB/s eta 0:00:21\n",
      "   ------------- ------------------------- 137.9/390.3 MB 12.4 MB/s eta 0:00:21\n",
      "   ------------- ------------------------- 137.9/390.3 MB 12.4 MB/s eta 0:00:21\n",
      "   ------------- ------------------------- 137.9/390.3 MB 12.4 MB/s eta 0:00:21\n",
      "   ------------- ------------------------- 138.1/390.3 MB 11.7 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 140.0/390.3 MB 11.7 MB/s eta 0:00:22\n",
      "   -------------- ------------------------ 145.0/390.3 MB 11.9 MB/s eta 0:00:21\n",
      "   --------------- ----------------------- 150.7/390.3 MB 12.1 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 151.8/390.3 MB 12.2 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 154.4/390.3 MB 12.0 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 159.6/390.3 MB 12.2 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 165.4/390.3 MB 12.5 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 166.2/390.3 MB 12.5 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 166.2/390.3 MB 12.5 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 170.4/390.3 MB 12.2 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 173.8/390.3 MB 12.4 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 173.8/390.3 MB 12.4 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 173.8/390.3 MB 12.4 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 173.8/390.3 MB 12.4 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 178.5/390.3 MB 11.9 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 184.5/390.3 MB 12.2 MB/s eta 0:00:17\n",
      "   ------------------ -------------------- 188.2/390.3 MB 12.3 MB/s eta 0:00:17\n",
      "   ------------------ -------------------- 188.2/390.3 MB 12.3 MB/s eta 0:00:17\n",
      "   ------------------ -------------------- 188.5/390.3 MB 12.0 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 192.2/390.3 MB 12.0 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 195.6/390.3 MB 12.1 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 195.6/390.3 MB 12.1 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 195.6/390.3 MB 12.1 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 197.1/390.3 MB 11.7 MB/s eta 0:00:17\n",
      "   -------------------- ------------------ 202.9/390.3 MB 11.9 MB/s eta 0:00:16\n",
      "   -------------------- ------------------ 208.9/390.3 MB 12.1 MB/s eta 0:00:15\n",
      "   -------------------- ------------------ 209.7/390.3 MB 12.1 MB/s eta 0:00:15\n",
      "   -------------------- ------------------ 209.7/390.3 MB 12.1 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 213.6/390.3 MB 11.9 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.4/390.3 MB 12.1 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.4/390.3 MB 12.1 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.4/390.3 MB 12.1 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.4/390.3 MB 12.1 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.7/390.3 MB 11.7 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.7/390.3 MB 11.7 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 220.2/390.3 MB 9.4 MB/s eta 0:00:19\n",
      "   ---------------------- ----------------- 221.2/390.3 MB 9.4 MB/s eta 0:00:18\n",
      "   ---------------------- ----------------- 223.3/390.3 MB 9.4 MB/s eta 0:00:18\n",
      "   ----------------------- ---------------- 225.2/390.3 MB 9.4 MB/s eta 0:00:18\n",
      "   ----------------------- ---------------- 228.1/390.3 MB 9.4 MB/s eta 0:00:18\n",
      "   ----------------------- ---------------- 230.2/390.3 MB 9.5 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 230.2/390.3 MB 9.5 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 230.9/390.3 MB 9.3 MB/s eta 0:00:18\n",
      "   ------------------------ --------------- 236.2/390.3 MB 9.4 MB/s eta 0:00:17\n",
      "   ------------------------ --------------- 239.9/390.3 MB 9.5 MB/s eta 0:00:16\n",
      "   ------------------------ --------------- 239.9/390.3 MB 9.5 MB/s eta 0:00:16\n",
      "   ------------------------ --------------- 239.9/390.3 MB 9.5 MB/s eta 0:00:16\n",
      "   ------------------------ --------------- 241.7/390.3 MB 9.4 MB/s eta 0:00:16\n",
      "   ------------------------- -------------- 246.9/390.3 MB 9.5 MB/s eta 0:00:16\n",
      "   ------------------------- -------------- 249.6/390.3 MB 9.5 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 249.6/390.3 MB 9.5 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 249.6/390.3 MB 9.5 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 253.0/390.3 MB 9.4 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 255.6/390.3 MB 9.5 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 255.6/390.3 MB 9.5 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 255.6/390.3 MB 9.5 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 255.6/390.3 MB 9.5 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 260.3/390.3 MB 9.3 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 266.9/390.3 MB 9.4 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 272.9/390.3 MB 9.4 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 278.7/390.3 MB 9.4 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 284.2/390.3 MB 9.4 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 289.7/390.3 MB 9.4 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 295.7/390.3 MB 9.4 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 298.3/390.3 MB 9.5 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 298.3/390.3 MB 9.5 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 299.4/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.1/390.3 MB 9.6 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 304.1/390.3 MB 9.6 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 304.1/390.3 MB 9.6 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 304.1/390.3 MB 9.6 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 310.1/390.3 MB 8.7 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 314.6/390.3 MB 8.8 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 320.1/390.3 MB 8.8 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 323.7/390.3 MB 9.0 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 323.7/390.3 MB 9.0 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 323.7/390.3 MB 9.0 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 324.5/390.3 MB 8.8 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 327.9/390.3 MB 8.7 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 332.7/390.3 MB 8.7 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 338.4/390.3 MB 8.9 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 344.7/390.3 MB 9.1 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 350.7/390.3 MB 9.2 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 357.0/390.3 MB 9.2 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 358.1/390.3 MB 9.2 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 358.1/390.3 MB 9.2 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 358.1/390.3 MB 9.2 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 362.5/390.3 MB 9.0 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 368.6/390.3 MB 9.1 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 372.5/390.3 MB 9.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 372.5/390.3 MB 9.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 372.5/390.3 MB 9.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 373.3/390.3 MB 9.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 373.6/390.3 MB 8.9 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 377.2/390.3 MB 8.9 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 379.6/390.3 MB 8.9 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 379.8/390.3 MB 8.8 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 380.4/390.3 MB 8.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  382.5/390.3 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  388.2/390.3 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  390.1/390.3 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  390.1/390.3 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 390.3/390.3 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.5/5.5 MB 25.8 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.17.1\n",
      "    Uninstalling tensorboard-2.17.1:\n",
      "      Successfully uninstalled tensorboard-2.17.1\n",
      "  Attempting uninstall: tensorflow-intel\n",
      "    Found existing installation: tensorflow-intel 2.17.0\n",
      "    Uninstalling tensorflow-intel-2.17.0:\n",
      "      Successfully uninstalled tensorflow-intel-2.17.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.17.0\n",
      "    Uninstalling tensorflow-2.17.0:\n",
      "      Successfully uninstalled tensorflow-2.17.0\n",
      "Successfully installed tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-intel-2.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~ensorflow'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "import nltk\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# For generating n-grams\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation (Loading CSV)\n",
    "\n",
    "Load the three CSV files into a pandas DataFrame `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processed_full_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>ok use airlin go singapor london heathrow issu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>don give money book paid receiv email confirm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "      <td>best airlin world best airlin world seat food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>premium economi seat singapor airlin not worth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>imposs get promis refund book flight full mont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month sentiment                              processed_full_review\n",
       "0  2024      3   Neutral  ok use airlin go singapor london heathrow issu...\n",
       "1  2024      3  Negative  don give money book paid receiv email confirm ...\n",
       "2  2024      3  Positive  best airlin world best airlin world seat food ...\n",
       "3  2024      3  Negative  premium economi seat singapor airlin not worth...\n",
       "4  2024      3  Negative  imposs get promis refund book flight full mont..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Positive    7913\n",
       "Negative    2441\n",
       "Neutral     1164\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2019    5129\n",
       "2018    2596\n",
       "2022    1184\n",
       "2023    1111\n",
       "2020     888\n",
       "2024     514\n",
       "2021      96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 60ms/step - accuracy: 0.6356 - loss: 0.8535 - val_accuracy: 0.8036 - val_loss: 0.5356\n",
      "Epoch 2/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 51ms/step - accuracy: 0.8384 - loss: 0.4481 - val_accuracy: 0.8150 - val_loss: 0.5042\n",
      "Epoch 3/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.8947 - loss: 0.2977 - val_accuracy: 0.7965 - val_loss: 0.5594\n",
      "Epoch 4/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.9629 - loss: 0.1397 - val_accuracy: 0.8258 - val_loss: 0.6575\n",
      "Epoch 5/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.9811 - loss: 0.0728 - val_accuracy: 0.7971 - val_loss: 0.6594\n",
      "Epoch 6/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.9890 - loss: 0.0487 - val_accuracy: 0.7645 - val_loss: 0.8196\n",
      "Epoch 7/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.9970 - loss: 0.0214 - val_accuracy: 0.7564 - val_loss: 0.8914\n",
      "Epoch 8/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.9988 - loss: 0.0121 - val_accuracy: 0.7710 - val_loss: 0.8929\n",
      "Epoch 9/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.9998 - loss: 0.0064 - val_accuracy: 0.7835 - val_loss: 0.9493\n",
      "Epoch 10/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.9999 - loss: 0.0039 - val_accuracy: 0.7884 - val_loss: 0.9711\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "Performance Metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7283    0.6617    0.6934       470\n",
      "     Neutral     0.2667    0.1579    0.1983       228\n",
      "    Positive     0.8754    0.9496    0.9110      1606\n",
      "\n",
      "    accuracy                         0.8125      2304\n",
      "   macro avg     0.6235    0.5897    0.6009      2304\n",
      "weighted avg     0.7852    0.8125    0.7961      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 5000         # Limit vocabulary to 5000 words\n",
    "embedding_dim = 128        # Embedding dimensions for each word\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "\n",
    "# Step 1: Tokenize and Pad the Text\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(data['processed_full_review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_full_review'])\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Define a Simple RNN Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length, trainable=True))\n",
    "model.add(SimpleRNN(64, activation='tanh'))\n",
    "model.add(Dropout(0.5))  # Add dropout for regularization\n",
    "model.add(Dense(3, activation='softmax'))   # Output layer for 3 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64,  validation_split=0.2, verbose=1)\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate and print classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4)\n",
    "print('Performance Metrics:\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN while accounting for imbalanced classes\n",
    "\n",
    "Overall F1 score drops very slightly\n",
    "\n",
    "By applying `class_weight` using `compute_class_weight`, the model pays more attention to minority classes, which may cause it to misclassify some instances of the majority class. This re-balncing can lower the overall F1 score if the model sacrifices performance on majority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 49ms/step - accuracy: 0.4260 - loss: 1.0870 - val_accuracy: 0.6772 - val_loss: 0.8115\n",
      "Epoch 2/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.7761 - loss: 0.7477 - val_accuracy: 0.7575 - val_loss: 0.5963\n",
      "Epoch 3/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.8885 - loss: 0.3859 - val_accuracy: 0.7596 - val_loss: 0.6410\n",
      "Epoch 4/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9447 - loss: 0.1741 - val_accuracy: 0.8074 - val_loss: 0.5949\n",
      "Epoch 5/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9728 - loss: 0.0915 - val_accuracy: 0.8101 - val_loss: 0.6316\n",
      "Epoch 6/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9858 - loss: 0.0437 - val_accuracy: 0.7862 - val_loss: 0.7083\n",
      "Epoch 7/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9903 - loss: 0.0290 - val_accuracy: 0.7488 - val_loss: 0.8350\n",
      "Epoch 8/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9843 - loss: 0.0404 - val_accuracy: 0.7482 - val_loss: 0.8632\n",
      "Epoch 9/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.9873 - loss: 0.0375 - val_accuracy: 0.7797 - val_loss: 0.8078\n",
      "Epoch 10/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 51ms/step - accuracy: 0.9938 - loss: 0.0152 - val_accuracy: 0.7873 - val_loss: 0.8534\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "Performance Metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.6606    0.6128    0.6358       470\n",
      "     Neutral     0.2275    0.2325    0.2299       228\n",
      "    Positive     0.9064    0.9228    0.9145      1606\n",
      "\n",
      "    accuracy                         0.7912      2304\n",
      "   macro avg     0.5981    0.5893    0.5934      2304\n",
      "weighted avg     0.7891    0.7912    0.7899      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 5000         # Limit vocabulary to 5000 words\n",
    "embedding_dim = 128        # Embedding dimensions for each word\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "\n",
    "# Step 1: Tokenize and Pad the Text\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(data['processed_full_review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_full_review'])\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Define a Simple RNN Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length, trainable=True))\n",
    "model.add(SimpleRNN(64, activation='tanh'))\n",
    "model.add(Dropout(0.5))  # Add dropout for regularization\n",
    "model.add(Dense(3, activation='softmax'))   # Output layer for 3 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64,  validation_split=0.2, verbose=1, class_weight=class_weights_dict)\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate and print classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4)\n",
    "print('Performance Metrics:\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN + Count Vectoriser\n",
    "\n",
    "### Loss of Sequential Information\n",
    "Poor performance because RNNs are not well-suited to the bag-of-words representation generated by `CountVectorizer`. Since `CountVectorizer` treats each document as a set of words without any order, words are represented only by their counts, not by their position in the text. Since RNNs are designed to work with ordered sequences, where the position and context of words matter, without preserving word order, the RNN cannot capture dependencies between words over time.\n",
    "\n",
    "### Sparse, non-contextual input\n",
    "`CountVectorizer` produces a sparse representation where each word is treated as an independent feature based on its frequency. There is no semantic or contextual relationship between words, and the word counts lack dense, meaningful relationships that an RNN could leverage, since RNNs perform best with dense, continuous data that represents meaningful relationships between words, typically achieved with word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.3968 - loss: 1.1084 - val_accuracy: 0.3793 - val_loss: 1.1062\n",
      "Epoch 2/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.3866 - loss: 1.0804 - val_accuracy: 0.2870 - val_loss: 1.1182\n",
      "Epoch 3/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.3607 - loss: 1.0819 - val_accuracy: 0.4742 - val_loss: 1.0924\n",
      "Epoch 4/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.3930 - loss: 1.0843 - val_accuracy: 0.5648 - val_loss: 1.0602\n",
      "Epoch 5/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.3768 - loss: 1.0897 - val_accuracy: 0.4954 - val_loss: 1.0961\n",
      "Epoch 6/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.3963 - loss: 1.0876 - val_accuracy: 0.2241 - val_loss: 1.2186\n",
      "Epoch 7/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.4199 - loss: 1.0880 - val_accuracy: 0.5475 - val_loss: 1.0867\n",
      "Epoch 8/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.4317 - loss: 1.0789 - val_accuracy: 0.2816 - val_loss: 1.1556\n",
      "Epoch 9/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.3424 - loss: 1.0832 - val_accuracy: 0.3717 - val_loss: 1.0958\n",
      "Epoch 10/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.3786 - loss: 1.1164 - val_accuracy: 0.2160 - val_loss: 1.1566\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "Performance Metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.1879    0.8383    0.3070       470\n",
      "     Neutral     0.1921    0.1272    0.1530       228\n",
      "    Positive     0.5357    0.0187    0.0361      1606\n",
      "\n",
      "    accuracy                         0.1966      2304\n",
      "   macro avg     0.3052    0.3281    0.1654      2304\n",
      "weighted avg     0.4308    0.1966    0.1029      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "max_features = 5000       # Limit vocabulary to 5000 words\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "\n",
    "# Step 1: Text Vectorization using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=max_features)\n",
    "X_counts = vectorizer.fit_transform(data['processed_full_review']).toarray()\n",
    "\n",
    "# Convert Counts to Sequences\n",
    "X_padded = pad_sequences(X_counts, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input to 3D for RNN (samples, timesteps, features)\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(64, activation='tanh', input_shape=(X_train_reshaped.shape[1], 1)))  # Input shape adjusted\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(3, activation='softmax'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "model.fit(X_train_reshaped, y_train, epochs=10, batch_size=64,  validation_split=0.2, verbose=1, class_weight=class_weights_dict)\n",
    "\n",
    "y_pred_prob = model.predict(X_test_reshaped)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate and print classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4)\n",
    "print('Performance Metrics:\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN + Count Vectoriser + Conversion to pseudo-sequences with word indices\n",
    "\n",
    "Performance is better than Basic RNN.\n",
    "\n",
    "Over here, we transform the `CountVectorizer` output into integer sequences which is compatible with the embedding layer. \n",
    "\n",
    "Why `CountVectorizer` is better here is because sentiment analysis often hinges more on the presence of certain key words rather than on the strict order of words in a sequence. Unlike other NLP tasks where the exact sequence of words matters (e.g. translation or grammar correction), sentiment analysis can often succeed with just the occurrence or frequency of these key items. `CountVectorizer` captures this by creating a bag-of-words representation that prioritises word presence and frequency, which is often enough for sentiment detection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 0.4720 - loss: 1.0589 - val_accuracy: 0.7911 - val_loss: 0.5905\n",
      "Epoch 2/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.8687 - loss: 0.5639 - val_accuracy: 0.7629 - val_loss: 0.5588\n",
      "Epoch 3/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - accuracy: 0.9391 - loss: 0.2436 - val_accuracy: 0.7466 - val_loss: 0.6518\n",
      "Epoch 4/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9750 - loss: 0.0925 - val_accuracy: 0.7982 - val_loss: 0.6056\n",
      "Epoch 5/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 53ms/step - accuracy: 0.9902 - loss: 0.0385 - val_accuracy: 0.8041 - val_loss: 0.6644\n",
      "Epoch 6/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.9983 - loss: 0.0145 - val_accuracy: 0.8041 - val_loss: 0.7157\n",
      "Epoch 7/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.9998 - loss: 0.0085 - val_accuracy: 0.7927 - val_loss: 0.7630\n",
      "Epoch 8/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.9991 - loss: 0.0089 - val_accuracy: 0.8068 - val_loss: 0.7934\n",
      "Epoch 9/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 0.7906 - val_loss: 0.8170\n",
      "Epoch 10/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 53ms/step - accuracy: 0.9994 - loss: 0.0037 - val_accuracy: 0.8095 - val_loss: 0.8638\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "Performance Metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7283    0.6957    0.7116       470\n",
      "     Neutral     0.2732    0.2193    0.2433       228\n",
      "    Positive     0.9067    0.9440    0.9250      1606\n",
      "\n",
      "    accuracy                         0.8216      2304\n",
      "   macro avg     0.6361    0.6197    0.6266      2304\n",
      "weighted avg     0.8076    0.8216    0.8140      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "max_features = 5000       # Limit vocabulary to 5000 words\n",
    "embedding_dim = 128        # Embedding dimensions for each word\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "\n",
    "# Step 1: Text Vectorization using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=max_features)\n",
    "X_counts = vectorizer.fit_transform(data['processed_full_review'])\n",
    "word_index = vectorizer.vocabulary_\n",
    "\n",
    "# Inverse vocabulary mapping for sequences creation\n",
    "index_to_word = {i: word for word, i in word_index.items()}\n",
    "\n",
    "def counts_to_sequences(X_counts):\n",
    "    sequences = []\n",
    "    for i in range(X_counts.shape[0]):\n",
    "        indices = X_counts[i].nonzero()[1]\n",
    "        words = [index_to_word[idx] for idx in indices]\n",
    "        seq = [word_index[word] + 1 for word in words]  # +1 because 0 is reserved for padding\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "sequences = counts_to_sequences(X_counts)\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length, trainable=True))\n",
    "model.add(SimpleRNN(64, activation='tanh', input_shape=(X_train_reshaped.shape[1], 1)))  # Input shape adjusted\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(3, activation='softmax'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64,  validation_split=0.2, verbose=1, class_weight=class_weights_dict)\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate and print classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4)\n",
    "print('Performance Metrics:\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN + Within model trained Word2Vec\n",
    "\n",
    "`Word2Vec` performs worse than `CountVectorizer`.\n",
    "\n",
    "Because our dataset is only 10k rows, Word2Vec embeddings might lack the depth needed for nuanced sentiment patterns, particularly without pre-training on a larger corpus. If Word2Vec embeddings do not generalise well or have insufficient context, the RNN might not capture subtle sentiment signals in the text, which can degrade model performance. In contrast, CountVectorizer builds a fixed vocab of words based on frequency, and does not need to learn semantic relationships among words, making it robust in cases where the model vocab size is small. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - accuracy: 0.4982 - loss: 1.0830 - val_accuracy: 0.7189 - val_loss: 0.6627\n",
      "Epoch 2/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.7076 - loss: 0.7795 - val_accuracy: 0.7699 - val_loss: 0.5509\n",
      "Epoch 3/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.7935 - loss: 0.6131 - val_accuracy: 0.7802 - val_loss: 0.5531\n",
      "Epoch 4/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.8305 - loss: 0.5432 - val_accuracy: 0.7694 - val_loss: 0.5675\n",
      "Epoch 5/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.8801 - loss: 0.3689 - val_accuracy: 0.5659 - val_loss: 0.9977\n",
      "Epoch 6/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.8229 - loss: 0.3752 - val_accuracy: 0.7482 - val_loss: 0.6872\n",
      "Epoch 7/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9089 - loss: 0.2250 - val_accuracy: 0.6983 - val_loss: 0.8755\n",
      "Epoch 8/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9336 - loss: 0.1594 - val_accuracy: 0.7895 - val_loss: 0.6954\n",
      "Epoch 9/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9630 - loss: 0.0958 - val_accuracy: 0.8030 - val_loss: 0.7601\n",
      "Epoch 10/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.9685 - loss: 0.0788 - val_accuracy: 0.7933 - val_loss: 0.7979\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "Performance Metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7047    0.6447    0.6733       470\n",
      "     Neutral     0.2879    0.3246    0.3052       228\n",
      "    Positive     0.9128    0.9191    0.9159      1606\n",
      "\n",
      "    accuracy                         0.8043      2304\n",
      "   macro avg     0.6351    0.6294    0.6315      2304\n",
      "weighted avg     0.8085    0.8043    0.8060      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 5000         # Limit vocabulary to 5000 words\n",
    "embedding_dim = 128        # Embedding dimensions for each word\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "\n",
    "# Step 1: Tokenize and Pad the Text\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(data['processed_full_review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_full_review'])\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "sentences = [text.split() for text in data['processed_full_review']]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=embedding_dim, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "# Create Embedding Matrix from Trained Word2Vec Model\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab_size:\n",
    "        # Retrieve the embedding vector for the word\n",
    "        embedding_vector = word2vec_model.wv[word] if word in word2vec_model.wv else None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Define a Simple RNN Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=True))\n",
    "model.add(SimpleRNN(64, activation='tanh'))\n",
    "model.add(Dropout(0.5))  # Add dropout for regularization\n",
    "model.add(Dense(3, activation='softmax'))   # Output layer for 3 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64,  validation_split=0.2, verbose=1, class_weight=class_weights_dict)\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate and print classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4)\n",
    "print('Performance Metrics:\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN + Pre-trained Word2Vec\n",
    "\n",
    "Pre-trained Word2Vec performs worse than within model trained Word2Vec.\n",
    "\n",
    "Google's Word2Vec embeddings were trained on very general Google News dataset, which may not align well with the context or vocabulary of our specific dataset, while custom embeddings trained directly on our dataset are tailored to the specific language and sentiment patterns within it.\n",
    "\n",
    "Since our dataset cotntains a lot of domain-specific terms and sentiment-heavy words that are less common in general news (like \"amazing\", \"terrible\", \"refund\"), pre-trained embeddings may not capture these terms accurately. Within-model embeddings can adapt specifically to the words and nuances in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.4276 - loss: 1.1304 - val_accuracy: 0.7184 - val_loss: 0.6579\n",
      "Epoch 2/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.6504 - loss: 0.8589 - val_accuracy: 0.6994 - val_loss: 0.7364\n",
      "Epoch 3/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.6428 - loss: 0.8659 - val_accuracy: 0.6804 - val_loss: 0.9888\n",
      "Epoch 4/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.5904 - loss: 1.0298 - val_accuracy: 0.7303 - val_loss: 0.6904\n",
      "Epoch 5/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.7149 - loss: 0.7646 - val_accuracy: 0.7656 - val_loss: 0.5968\n",
      "Epoch 6/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.7062 - loss: 0.7707 - val_accuracy: 0.7699 - val_loss: 0.5865\n",
      "Epoch 7/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.7538 - loss: 0.7063 - val_accuracy: 0.7781 - val_loss: 0.5772\n",
      "Epoch 8/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.7300 - loss: 0.7097 - val_accuracy: 0.7629 - val_loss: 0.5952\n",
      "Epoch 9/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.7806 - loss: 0.6444 - val_accuracy: 0.7694 - val_loss: 0.6012\n",
      "Epoch 10/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.7281 - loss: 0.6718 - val_accuracy: 0.4037 - val_loss: 1.3044\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "Performance Metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.1860    0.3787    0.2495       470\n",
      "     Neutral     0.1259    0.2982    0.1771       228\n",
      "    Positive     0.7348    0.3692    0.4915      1606\n",
      "\n",
      "    accuracy                         0.3641      2304\n",
      "   macro avg     0.3489    0.3487    0.3060      2304\n",
      "weighted avg     0.5626    0.3641    0.4110      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from gensim.models import KeyedVectors\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 5000         # Limit vocabulary to 5000 words\n",
    "embedding_dim = 300        # Embedding dimensions for each word\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "\n",
    "# Step 1: Tokenize and Pad the Text\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(data['processed_full_review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_full_review'])\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Create Embedding Matrix with Pre-trained Word2Vec\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab_size:\n",
    "        # Retrieve the embedding vector for the word\n",
    "        if word in word2vec_model:\n",
    "            embedding_matrix[i] = word2vec_model[word]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Define a Simple RNN Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "model.add(SimpleRNN(64, activation='tanh'))\n",
    "model.add(Dropout(0.5))  # Add dropout for regularization\n",
    "model.add(Dense(3, activation='softmax'))   # Output layer for 3 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64,  validation_split=0.2, verbose=1, class_weight=class_weights_dict)\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate and print classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4)\n",
    "print('Performance Metrics:\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN + Count Vectoriser + Conversion to pseudo-sequences with word indices + GridSearch CV\n",
    "\n",
    "Since we see that RNN + CountVectorizer + conversion to pseudo-sequences performs the best so far, we will perform GridSearchCV to select the best combination of hyperparameters to improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_deprecate_Xt_in_inverse_transform' from 'sklearn.utils.deprecation' (c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\deprecation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleRNN, Dense, Dropout\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasClassifier\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n",
      "File \u001b[1;32mc:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loss_name, metric_name\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom_state\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow_random_state\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassifierLabelEncoder, RegressorTargetEncoder\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseWrapper\u001b[39;00m(BaseEstimator):\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implementation of the scikit-learn classifier API for Keras.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m    Below are a list of SciKeras specific parameters. For details on other parameters,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m        The number of features seen during `fit`.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scikeras\\utils\\transformers.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, TransformerMixin\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NotFittedError\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_pipeline\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionTransformer, OneHotEncoder, OrdinalEncoder\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m type_of_target\n",
      "File \u001b[1;32mc:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _safe_tags\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_user_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _print_elapsed_time\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _deprecate_Xt_in_inverse_transform\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata_routing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     31\u001b[0m     MetadataRouter,\n\u001b[0;32m     32\u001b[0m     MethodMapping,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     process_routing,\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetaestimators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _BaseComposition, available_if\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_deprecate_Xt_in_inverse_transform' from 'sklearn.utils.deprecation' (c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\deprecation.py)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "max_features = 5000       # Limit vocabulary to 5000 words\n",
    "embedding_dim = 128        # Embedding dimensions for each word\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "\n",
    "# Step 1: Text Vectorization using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=max_features)\n",
    "X_counts = vectorizer.fit_transform(data['processed_full_review'])\n",
    "word_index = vectorizer.vocabulary_\n",
    "\n",
    "# Inverse vocabulary mapping for sequences creation\n",
    "index_to_word = {i: word for word, i in word_index.items()}\n",
    "\n",
    "def counts_to_sequences(X_counts):\n",
    "    sequences = []\n",
    "    for i in range(X_counts.shape[0]):\n",
    "        indices = X_counts[i].nonzero()[1]\n",
    "        words = [index_to_word[idx] for idx in indices]\n",
    "        seq = [word_index[word] + 1 for word in words]  # +1 because 0 is reserved for padding\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "sequences = counts_to_sequences(X_counts)\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "def build_model(embedding_dim=128, rnn_units=64, dropout_rate=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length, trainable=True))\n",
    "    model.add(SimpleRNN(rnn_units, activation='tanh'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=build_model, verbose=1, class_weight=class_weights_dict)\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'embedding_dim': [64, 128, 300],      # Different embedding dimensions\n",
    "    'rnn_units': [32, 64, 128],           # Number of units in SimpleRNN layer\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],      # Dropout rates\n",
    "    'batch_size': [32, 64, 128],          # Batch sizes\n",
    "    'epochs': [5, 10]                     # Number of epochs\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=3)\n",
    "\n",
    "# Run GridSearchCV\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters found: \", grid_result.best_params_)\n",
    "print(\"Best cross-validation accuracy: \", grid_result.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model = grid_result.best_estimator_\n",
    "y_pred_prob = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate and print classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4)\n",
    "print('Performance Metrics:\\n', report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
