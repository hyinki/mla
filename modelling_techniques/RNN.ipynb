{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Singapore Airlines' Service Through Automated Sentiment Analysis of Customer Reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singapore Airlines Customer Reviews Dataset Information\n",
    "\n",
    "The [Singapore Airlines Customer Reviews Dataset](https://www.kaggle.com/datasets/kanchana1990/singapore-airlines-reviews) aggregates 10,000 anonymized customer reviews, providing a broad perspective on the passenger experience with Singapore Airlines. \n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`published_date`**: Date and time of review publication.\n",
    "- **`published_platform`**: Platform where the review was posted.\n",
    "- **`rating`**: Customer satisfaction rating, from 1 (lowest) to 5 (highest).\n",
    "- **`type`**: Specifies the content as a review.\n",
    "- **`text`**: Detailed customer feedback.\n",
    "- **`title`**: Summary of the review.\n",
    "- **`helpful_votes`**: Number of users finding the review helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional web scraping of online reviews\n",
    "\n",
    "During our EDA, we noticed two main trends in the distribution of our dataset:\n",
    "1. Less than 10% of our reviews were published from the years 2022 to 2024, making it hard for us to capture recent trends in sentiment.\n",
    "2. Most of the reviews were highly positive, which could mean that SIA had mostly positive reviews, nevertheless we wanted to get more information on negative reviews to improve the robustness of our model.\n",
    "\n",
    "### TripAdvisor\n",
    "\n",
    "We scraped more data for airline reviews from TripAdvisor, specifically for the years 2022 to 2024. \n",
    "(https://www.tripadvisor.com.sg/Airline_Review-d8729151-Reviews-Singapore-Airlines)\n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`Year`**: Year of review publication.\n",
    "- **`Month`**: Month of review publication.\n",
    "- **`Title`**: Title of review publication.\n",
    "- **`Review Text`**: Main text content of review publication.\n",
    "- **`Rating`**: Numerical rating provided by reviewer (Scale: 1 to 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skytrax\n",
    "\n",
    "We also scraped from Skytrax, which is another data source for online reviews. \n",
    "(https://www.airlinequality.com/airline-reviews/singapore-airlines/?sortby=post_date%3ADesc&pagesize=100)\n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`Year`**: Year of review publication.\n",
    "- **`Month`**: Month of review publication.\n",
    "- **`Title`**: Title of review publication.\n",
    "- **`Review Text`**: Main text content of review publication.\n",
    "- **`Rating`**: Numerical rating provided by reviewer (Scale: 1 to 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Please uncomment the code box below to pip install relevant dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=2.0.0 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from -r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.24.0 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from -r requirements.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from -r requirements.txt (line 3)) (1.14.1)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from -r requirements.txt (line 4)) (4.66.5)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from -r requirements.txt (line 5)) (3.9.2)\n",
      "Requirement already satisfied: seaborn>=0.12.0 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
      "Requirement already satisfied: langdetect>=1.0.9 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from -r requirements.txt (line 7)) (1.0.9)\n",
      "Requirement already satisfied: langid>=1.1.6 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from -r requirements.txt (line 8)) (1.1.6)\n",
      "Requirement already satisfied: nltk>=3.8.1 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from -r requirements.txt (line 9)) (3.9.1)\n",
      "Requirement already satisfied: wordcloud>=1.9.0 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from -r requirements.txt (line 10)) (1.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\johnt\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from tqdm>=4.65.0->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (3.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from langdetect>=1.0.9->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\johnt\\coding\\anaconda\\envs\\py312\\lib\\site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (2024.9.11)\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "import nltk\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# For generating n-grams\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation (Loading CSV)\n",
    "\n",
    "Load the three CSV files into a pandas DataFrame `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processed_full_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>ok use airlin go singapor london heathrow issu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>don give money book paid receiv email confirm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "      <td>best airlin world best airlin world seat food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>premium economi seat singapor airlin not worth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>imposs get promis refund book flight full mont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month sentiment                              processed_full_review\n",
       "0  2024      3   Neutral  ok use airlin go singapor london heathrow issu...\n",
       "1  2024      3  Negative  don give money book paid receiv email confirm ...\n",
       "2  2024      3  Positive  best airlin world best airlin world seat food ...\n",
       "3  2024      3  Negative  premium economi seat singapor airlin not worth...\n",
       "4  2024      3  Negative  imposs get promis refund book flight full mont..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Positive    7913\n",
       "Negative    2441\n",
       "Neutral     1164\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2019    5129\n",
       "2018    2596\n",
       "2022    1184\n",
       "2023    1111\n",
       "2020     888\n",
       "2024     514\n",
       "2021      96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - accuracy: 0.6953 - loss: 0.7855 - val_accuracy: 0.8247 - val_loss: 0.4852\n",
      "Epoch 2/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.8643 - loss: 0.3740 - val_accuracy: 0.8285 - val_loss: 0.4735\n",
      "Epoch 3/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9305 - loss: 0.2286 - val_accuracy: 0.8193 - val_loss: 0.5020\n",
      "Epoch 4/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9719 - loss: 0.1102 - val_accuracy: 0.8209 - val_loss: 0.6072\n",
      "Epoch 5/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9843 - loss: 0.0586 - val_accuracy: 0.8166 - val_loss: 0.6437\n",
      "Epoch 6/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9974 - loss: 0.0218 - val_accuracy: 0.8068 - val_loss: 0.6902\n",
      "Epoch 7/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9938 - loss: 0.0267 - val_accuracy: 0.8199 - val_loss: 0.7366\n",
      "Epoch 8/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9993 - loss: 0.0114 - val_accuracy: 0.8209 - val_loss: 0.7974\n",
      "Epoch 9/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.9996 - loss: 0.0061 - val_accuracy: 0.8095 - val_loss: 0.8570\n",
      "Epoch 10/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.8161 - val_loss: 0.8836\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "Performance Metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7367    0.7085    0.7223       470\n",
      "     Neutral     0.2932    0.1711    0.2161       228\n",
      "    Positive     0.8906    0.9533    0.9209      1606\n",
      "\n",
      "    accuracy                         0.8260      2304\n",
      "   macro avg     0.6402    0.6110    0.6198      2304\n",
      "weighted avg     0.8001    0.8260    0.8106      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 5000         # Limit vocabulary to 5000 words\n",
    "embedding_dim = 128        # Embedding dimensions for each word\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "\n",
    "# Step 1: Tokenize and Pad the Text\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(data['processed_full_review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_full_review'])\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Define a Simple RNN Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length, trainable=True))\n",
    "model.add(SimpleRNN(64, activation='tanh'))\n",
    "model.add(Dropout(0.5))  # Add dropout for regularization\n",
    "model.add(Dense(3, activation='softmax'))   # Output layer for 3 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64,  validation_split=0.2, verbose=1)\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate and print classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4)\n",
    "print('Performance Metrics:\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN while accounting for imbalanced classes\n",
    "\n",
    "Overall F1 score drops very slightly\n",
    "\n",
    "By applying `class_weight` using `compute_class_weight`, the model pays more attention to minority classes, which may cause it to misclassify some instances of the majority class. This re-balncing can lower the overall F1 score if the model sacrifices performance on majority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 49ms/step - accuracy: 0.4801 - loss: 1.0711 - val_accuracy: 0.7618 - val_loss: 0.6118\n",
      "Epoch 2/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.8107 - loss: 0.6456 - val_accuracy: 0.7661 - val_loss: 0.5891\n",
      "Epoch 3/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9174 - loss: 0.2804 - val_accuracy: 0.7944 - val_loss: 0.5659\n",
      "Epoch 4/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9677 - loss: 0.1091 - val_accuracy: 0.7971 - val_loss: 0.6449\n",
      "Epoch 5/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9725 - loss: 0.0751 - val_accuracy: 0.7878 - val_loss: 0.6871\n",
      "Epoch 6/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9925 - loss: 0.0302 - val_accuracy: 0.7971 - val_loss: 0.7275\n",
      "Epoch 7/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - accuracy: 0.9937 - loss: 0.0218 - val_accuracy: 0.7623 - val_loss: 0.8378\n",
      "Epoch 8/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9931 - loss: 0.0233 - val_accuracy: 0.7944 - val_loss: 0.8444\n",
      "Epoch 9/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9992 - loss: 0.0091 - val_accuracy: 0.7851 - val_loss: 0.8739\n",
      "Epoch 10/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9995 - loss: 0.0070 - val_accuracy: 0.7900 - val_loss: 0.9331\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "Performance Metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7010    0.5936    0.6429       470\n",
      "     Neutral     0.2273    0.2193    0.2232       228\n",
      "    Positive     0.8980    0.9427    0.9198      1606\n",
      "\n",
      "    accuracy                         0.7999      2304\n",
      "   macro avg     0.6088    0.5852    0.5953      2304\n",
      "weighted avg     0.7914    0.7999    0.7944      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 5000         # Limit vocabulary to 5000 words\n",
    "embedding_dim = 128        # Embedding dimensions for each word\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "\n",
    "# Step 1: Tokenize and Pad the Text\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(data['processed_full_review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_full_review'])\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Define a Simple RNN Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length, trainable=True))\n",
    "model.add(SimpleRNN(64, activation='tanh'))\n",
    "model.add(Dropout(0.5))  # Add dropout for regularization\n",
    "model.add(Dense(3, activation='softmax'))   # Output layer for 3 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64,  validation_split=0.2, verbose=1, class_weight=class_weights_dict)\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate and print classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4)\n",
    "print('Performance Metrics:\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN + Count Vectoriser\n",
    "\n",
    "### Loss of Sequential Information\n",
    "Poor performance because RNNs are not well-suited to the bag-of-words representation generated by `CountVectorizer`. Since `CountVectorizer` treats each document as a set of words without any order, words are represented only by their counts, not by their position in the text. Since RNNs are designed to work with ordered sequences, where the position and context of words matter, without preserving word order, the RNN cannot capture dependencies between words over time.\n",
    "\n",
    "### Sparse, non-contextual input\n",
    "`CountVectorizer` produces a sparse representation where each word is treated as an independent feature based on its frequency. There is no semantic or contextual relationship between words, and the word counts lack dense, meaningful relationships that an RNN could leverage, since RNNs perform best with dense, continuous data that represents meaningful relationships between words, typically achieved with word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - accuracy: 0.3603 - loss: 1.0976 - val_accuracy: 0.2409 - val_loss: 1.0996\n",
      "Epoch 2/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.3883 - loss: 1.1024 - val_accuracy: 0.5144 - val_loss: 1.0559\n",
      "Epoch 3/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.4197 - loss: 1.0939 - val_accuracy: 0.4330 - val_loss: 1.0663\n",
      "Epoch 4/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.4909 - loss: 1.0910 - val_accuracy: 0.5279 - val_loss: 1.0767\n",
      "Epoch 5/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.4377 - loss: 1.0823 - val_accuracy: 0.5301 - val_loss: 1.0364\n",
      "Epoch 6/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.4464 - loss: 1.0977 - val_accuracy: 0.5383 - val_loss: 1.0831\n",
      "Epoch 7/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.4647 - loss: 1.0799 - val_accuracy: 0.5513 - val_loss: 1.0820\n",
      "Epoch 8/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.4520 - loss: 1.1047 - val_accuracy: 0.6137 - val_loss: 1.0681\n",
      "Epoch 9/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.4621 - loss: 1.0876 - val_accuracy: 0.1232 - val_loss: 1.1261\n",
      "Epoch 10/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.3489 - loss: 1.1087 - val_accuracy: 0.6430 - val_loss: 1.0250\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "Performance Metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.4971    0.1809    0.2652       470\n",
      "     Neutral     0.1478    0.1316    0.1392       228\n",
      "    Positive     0.7404    0.8898    0.8083      1606\n",
      "\n",
      "    accuracy                         0.6701      2304\n",
      "   macro avg     0.4618    0.4007    0.4042      2304\n",
      "weighted avg     0.6321    0.6701    0.6313      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Parameters\n",
    "max_features = 5000       # Limit vocabulary to 5000 words\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "\n",
    "# Step 1: Text Vectorization using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=max_features)\n",
    "X_counts = vectorizer.fit_transform(data['processed_full_review']).toarray()\n",
    "\n",
    "# Convert Counts to Sequences\n",
    "X_padded = pad_sequences(X_counts, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape input to 3D for RNN (samples, timesteps, features)\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(64, activation='tanh', input_shape=(X_train_reshaped.shape[1], 1)))  # Input shape adjusted\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(3, activation='softmax'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "model.fit(X_train_reshaped, y_train, epochs=10, batch_size=64,  validation_split=0.2, verbose=1, class_weight=class_weights_dict)\n",
    "\n",
    "y_pred_prob = model.predict(X_test_reshaped)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate and print classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4)\n",
    "print('Performance Metrics:\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN + Count Vectoriser + Conversion to pseudo-sequences with word indices\n",
    "\n",
    "Performance is better than Basic RNN.\n",
    "\n",
    "Over here, we transform the `CountVectorizer` output into integer sequences which is compatible with the embedding layer. \n",
    "\n",
    "Why `CountVectorizer` is better here is because sentiment analysis often hinges more on the presence of certain key words rather than on the strict order of words in a sequence. Unlike other NLP tasks where the exact sequence of words matters (e.g. translation or grammar correction), sentiment analysis can often succeed with just the occurrence or frequency of these key items. `CountVectorizer` captures this by creating a bag-of-words representation that prioritises word presence and frequency, which is often enough for sentiment detection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 46ms/step - accuracy: 0.4413 - loss: 1.0788 - val_accuracy: 0.6234 - val_loss: 0.9673\n",
      "Epoch 2/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.5211 - loss: 0.9851 - val_accuracy: 0.7835 - val_loss: 0.5874\n",
      "Epoch 3/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.8911 - loss: 0.4458 - val_accuracy: 0.7900 - val_loss: 0.5219\n",
      "Epoch 4/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9549 - loss: 0.2068 - val_accuracy: 0.8090 - val_loss: 0.5072\n",
      "Epoch 5/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9822 - loss: 0.0892 - val_accuracy: 0.8166 - val_loss: 0.5471\n",
      "Epoch 6/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9913 - loss: 0.0474 - val_accuracy: 0.8188 - val_loss: 0.5817\n",
      "Epoch 7/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - accuracy: 0.9959 - loss: 0.0291 - val_accuracy: 0.8237 - val_loss: 0.6318\n",
      "Epoch 8/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9988 - loss: 0.0176 - val_accuracy: 0.8161 - val_loss: 0.6818\n",
      "Epoch 9/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9994 - loss: 0.0113 - val_accuracy: 0.8220 - val_loss: 0.7023\n",
      "Epoch 10/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.0080 - val_accuracy: 0.8264 - val_loss: 0.7625\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "Performance Metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7455    0.7915    0.7678       470\n",
      "     Neutral     0.4570    0.3026    0.3641       228\n",
      "    Positive     0.9069    0.9340    0.9202      1606\n",
      "\n",
      "    accuracy                         0.8424      2304\n",
      "   macro avg     0.7031    0.6760    0.6841      2304\n",
      "weighted avg     0.8294    0.8424    0.8341      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Parameters\n",
    "max_features = 5000       # Limit vocabulary to 5000 words\n",
    "embedding_dim = 128        # Embedding dimensions for each word\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "\n",
    "# Step 1: Text Vectorization using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=max_features)\n",
    "X_counts = vectorizer.fit_transform(data['processed_full_review'])\n",
    "word_index = vectorizer.vocabulary_\n",
    "\n",
    "# Inverse vocabulary mapping for sequences creation\n",
    "index_to_word = {i: word for word, i in word_index.items()}\n",
    "\n",
    "def counts_to_sequences(X_counts):\n",
    "    sequences = []\n",
    "    for i in range(X_counts.shape[0]):\n",
    "        indices = X_counts[i].nonzero()[1]\n",
    "        words = [index_to_word[idx] for idx in indices]\n",
    "        seq = [word_index[word] + 1 for word in words]  # +1 because 0 is reserved for padding\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "sequences = counts_to_sequences(X_counts)\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length, trainable=True))\n",
    "model.add(SimpleRNN(64, activation='tanh', input_shape=(X_train_reshaped.shape[1], 1)))  # Input shape adjusted\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(3, activation='softmax'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64,  validation_split=0.2, verbose=1, class_weight=class_weights_dict)\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate and print classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4)\n",
    "print('Performance Metrics:\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN + Within model trained Word2Vec\n",
    "\n",
    "`Word2Vec` performs worse than `CountVectorizer`.\n",
    "\n",
    "Because our dataset is only 10k rows, Word2Vec embeddings might lack the depth needed for nuanced sentiment patterns, particularly without pre-training on a larger corpus. If Word2Vec embeddings do not generalise well or have insufficient context, the RNN might not capture subtle sentiment signals in the text, which can degrade model performance. In contrast, CountVectorizer builds a fixed vocab of words based on frequency, and does not need to learn semantic relationships among words, making it robust in cases where the model vocab size is small. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 90ms/step - accuracy: 0.5156 - loss: 1.0849 - val_accuracy: 0.7461 - val_loss: 0.6227\n",
      "Epoch 2/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 88ms/step - accuracy: 0.7304 - loss: 0.7668 - val_accuracy: 0.7428 - val_loss: 0.6520\n",
      "Epoch 3/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.7818 - loss: 0.6514 - val_accuracy: 0.7184 - val_loss: 0.6758\n",
      "Epoch 4/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - accuracy: 0.8472 - loss: 0.4936 - val_accuracy: 0.6815 - val_loss: 0.8051\n",
      "Epoch 5/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.8888 - loss: 0.3527 - val_accuracy: 0.7016 - val_loss: 0.7845\n",
      "Epoch 6/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 91ms/step - accuracy: 0.9197 - loss: 0.2260 - val_accuracy: 0.8128 - val_loss: 0.6503\n",
      "Epoch 7/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 82ms/step - accuracy: 0.8503 - loss: 0.3032 - val_accuracy: 0.7868 - val_loss: 0.6955\n",
      "Epoch 8/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 84ms/step - accuracy: 0.9620 - loss: 0.1100 - val_accuracy: 0.7922 - val_loss: 0.7262\n",
      "Epoch 9/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - accuracy: 0.9783 - loss: 0.0647 - val_accuracy: 0.7954 - val_loss: 0.7955\n",
      "Epoch 10/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 73ms/step - accuracy: 0.9677 - loss: 0.0835 - val_accuracy: 0.7878 - val_loss: 0.7719\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "Performance Metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.6561    0.7064    0.6803       470\n",
      "     Neutral     0.2941    0.3070    0.3004       228\n",
      "    Positive     0.9276    0.9010    0.9141      1606\n",
      "\n",
      "    accuracy                         0.8025      2304\n",
      "   macro avg     0.6259    0.6381    0.6316      2304\n",
      "weighted avg     0.8095    0.8025    0.8057      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 5000         # Limit vocabulary to 5000 words\n",
    "embedding_dim = 128        # Embedding dimensions for each word\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "\n",
    "# Step 1: Tokenize and Pad the Text\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(data['processed_full_review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_full_review'])\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "sentences = [text.split() for text in data['processed_full_review']]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=embedding_dim, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "# Create Embedding Matrix from Trained Word2Vec Model\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab_size:\n",
    "        # Retrieve the embedding vector for the word\n",
    "        embedding_vector = word2vec_model.wv[word] if word in word2vec_model.wv else None\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Define a Simple RNN Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=True))\n",
    "model.add(SimpleRNN(64, activation='tanh'))\n",
    "model.add(Dropout(0.5))  # Add dropout for regularization\n",
    "model.add(Dense(3, activation='softmax'))   # Output layer for 3 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64,  validation_split=0.2, verbose=1, class_weight=class_weights_dict)\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate and print classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4)\n",
    "print('Performance Metrics:\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN + Pre-trained Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 80ms/step - accuracy: 0.4800 - loss: 1.0975 - val_accuracy: 0.7401 - val_loss: 0.6825\n",
      "Epoch 2/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 76ms/step - accuracy: 0.6848 - loss: 0.8667 - val_accuracy: 0.6923 - val_loss: 0.7555\n",
      "Epoch 3/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 73ms/step - accuracy: 0.6901 - loss: 0.8259 - val_accuracy: 0.7352 - val_loss: 0.6554\n",
      "Epoch 4/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 70ms/step - accuracy: 0.6912 - loss: 0.8376 - val_accuracy: 0.7016 - val_loss: 0.7509\n",
      "Epoch 5/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 70ms/step - accuracy: 0.7443 - loss: 0.7090 - val_accuracy: 0.7314 - val_loss: 0.6768\n",
      "Epoch 6/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 72ms/step - accuracy: 0.7573 - loss: 0.6879 - val_accuracy: 0.7488 - val_loss: 0.6388\n",
      "Epoch 7/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 70ms/step - accuracy: 0.7775 - loss: 0.6320 - val_accuracy: 0.6983 - val_loss: 0.7235\n",
      "Epoch 8/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 70ms/step - accuracy: 0.8022 - loss: 0.5752 - val_accuracy: 0.7298 - val_loss: 0.6963\n",
      "Epoch 9/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 70ms/step - accuracy: 0.7999 - loss: 0.5446 - val_accuracy: 0.7401 - val_loss: 0.6856\n",
      "Epoch 10/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 71ms/step - accuracy: 0.8108 - loss: 0.5019 - val_accuracy: 0.7092 - val_loss: 0.7615\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n",
      "Performance Metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.5882    0.6170    0.6023       470\n",
      "     Neutral     0.2225    0.4605    0.3000       228\n",
      "    Positive     0.9320    0.7771    0.8475      1606\n",
      "\n",
      "    accuracy                         0.7131      2304\n",
      "   macro avg     0.5809    0.6182    0.5833      2304\n",
      "weighted avg     0.7917    0.7131    0.7433      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 5000         # Limit vocabulary to 5000 words\n",
    "embedding_dim = 300        # Embedding dimensions for each word\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "\n",
    "# Step 1: Tokenize and Pad the Text\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(data['processed_full_review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_full_review'])\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Create Embedding Matrix with Pre-trained Word2Vec\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab_size:\n",
    "        # Retrieve the embedding vector for the word\n",
    "        if word in word2vec_model:\n",
    "            embedding_matrix[i] = word2vec_model[word]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Define a Simple RNN Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "model.add(SimpleRNN(64, activation='tanh'))\n",
    "model.add(Dropout(0.5))  # Add dropout for regularization\n",
    "model.add(Dense(3, activation='softmax'))   # Output layer for 3 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64,  validation_split=0.2, verbose=1, class_weight=class_weights_dict)\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate and print classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4)\n",
    "print('Performance Metrics:\\n', report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
