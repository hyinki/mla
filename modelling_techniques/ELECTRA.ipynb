{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Singapore Airlines' Service Through Automated Sentiment Analysis of Customer Reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singapore Airlines Customer Reviews Dataset Information\n",
    "\n",
    "The [Singapore Airlines Customer Reviews Dataset](https://www.kaggle.com/datasets/kanchana1990/singapore-airlines-reviews) aggregates 10,000 anonymized customer reviews, providing a broad perspective on the passenger experience with Singapore Airlines. \n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`published_date`**: Date and time of review publication.\n",
    "- **`published_platform`**: Platform where the review was posted.\n",
    "- **`rating`**: Customer satisfaction rating, from 1 (lowest) to 5 (highest).\n",
    "- **`type`**: Specifies the content as a review.\n",
    "- **`text`**: Detailed customer feedback.\n",
    "- **`title`**: Summary of the review.\n",
    "- **`helpful_votes`**: Number of users finding the review helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional web scraping of online reviews\n",
    "\n",
    "During our EDA, we noticed two main trends in the distribution of our dataset:\n",
    "1. Less than 10% of our reviews were published from the years 2022 to 2024, making it hard for us to capture recent trends in sentiment.\n",
    "2. Most of the reviews were highly positive, which could mean that SIA had mostly positive reviews, nevertheless we wanted to get more information on negative reviews to improve the robustness of our model.\n",
    "\n",
    "### TripAdvisor\n",
    "\n",
    "We scraped more data for airline reviews from TripAdvisor, specifically for the years 2022 to 2024. \n",
    "(https://www.tripadvisor.com.sg/Airline_Review-d8729151-Reviews-Singapore-Airlines)\n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`Year`**: Year of review publication.\n",
    "- **`Month`**: Month of review publication.\n",
    "- **`Title`**: Title of review publication.\n",
    "- **`Review Text`**: Main text content of review publication.\n",
    "- **`Rating`**: Numerical rating provided by reviewer (Scale: 1 to 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skytrax\n",
    "\n",
    "We also scraped from Skytrax, which is another data source for online reviews. \n",
    "(https://www.airlinequality.com/airline-reviews/singapore-airlines/?sortby=post_date%3ADesc&pagesize=100)\n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`Year`**: Year of review publication.\n",
    "- **`Month`**: Month of review publication.\n",
    "- **`Title`**: Title of review publication.\n",
    "- **`Review Text`**: Main text content of review publication.\n",
    "- **`Rating`**: Numerical rating provided by reviewer (Scale: 1 to 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Please uncomment the code box below to pip install relevant dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=2.0.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.11.1)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (4.65.0)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (3.7.2)\n",
      "Requirement already satisfied: seaborn>=0.12.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (0.12.2)\n",
      "Requirement already satisfied: langdetect>=1.0.9 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.0.9)\n",
      "Requirement already satisfied: langid>=1.1.6 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (1.1.6)\n",
      "Requirement already satisfied: nltk>=3.8.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (3.8.1)\n",
      "Requirement already satisfied: wordcloud>=1.9.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (1.9.3)\n",
      "Requirement already satisfied: tensorflow>=2.17.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (2.18.0)\n",
      "Requirement already satisfied: scikeras>=0.10.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: six in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from langdetect>=1.0.9->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (2022.7.9)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (68.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (3.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.37.1)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from scikeras>=0.10.0->-r requirements.txt (line 12)) (1.5.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.38.4)\n",
      "Requirement already satisfied: rich in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (13.9.2)\n",
      "Requirement already satisfied: namex in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (2023.7.22)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.4.2->scikeras>=0.10.0->-r requirements.txt (line 12)) (3.5.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.17.1->-r requirements.txt (line 11)) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.17.1->-r requirements.txt (line 11)) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "import nltk\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# For generating n-grams\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation (Loading CSV)\n",
    "\n",
    "Load the three CSV files into a pandas DataFrame `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processed_full_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>ok use airlin go singapor london heathrow issu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>don give money book paid receiv email confirm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "      <td>best airlin world best airlin world seat food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>premium economi seat singapor airlin not worth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>imposs get promis refund book flight full mont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month sentiment                              processed_full_review\n",
       "0  2024      3   Neutral  ok use airlin go singapor london heathrow issu...\n",
       "1  2024      3  Negative  don give money book paid receiv email confirm ...\n",
       "2  2024      3  Positive  best airlin world best airlin world seat food ...\n",
       "3  2024      3  Negative  premium economi seat singapor airlin not worth...\n",
       "4  2024      3  Negative  imposs get promis refund book flight full mont..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Positive    7913\n",
       "Negative    2441\n",
       "Neutral     1164\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2019    5129\n",
       "2018    2596\n",
       "2022    1184\n",
       "2023    1111\n",
       "2020     888\n",
       "2024     514\n",
       "2021      96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)\n",
    "\n",
    "### Replaced Token Detection (RTD)\n",
    "- Unlike BERT, which masks words in the input and then tries to predict the, ELECTRA randomly replaces certain tokens with plausible alternatives generated by a small generator model and trains a larger discriminator model to detect whether each token is \"real\" (original) or \"replaced\" (fake).\n",
    "\n",
    "- This setup means that every token in the input sequence is used during training, not just the masked ones, which leads to more efficient training.\n",
    "\n",
    "### Two-Part Architecture\n",
    "- **Generator:** A smaller model (often a smaller BER) that replaces tokens with plausible alternatives. It essentially \"corrupts\" the input sentence by substituting some tokens with similar words.\n",
    "\n",
    "- **Discriminator:** The main ELECTRA model, which learns to classify each token as either \"real\" or \"fake\" based on whether the token was replaced by the generator. This part of the model is fine-tuned for downstream tasks after pretraining.\n",
    "\n",
    "### Performance\n",
    "- Typically outperforms BERT on NLP tasks. ELECTRA-small can perform similarly to BERT-based, and ELECTRA-base often surpasses BERT-base while being more efficient.\n",
    "\n",
    "\n",
    "We'll be using ELECTRA-based, which is a standard-size model, comparable to BERT-base in terms of size and performance. It has 110M parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15807f7410b1490aaebe0ec1fc85e2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Redbu\\.cache\\huggingface\\hub\\models--google--electra-base-discriminator. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0d47622388429999e57305f85677fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437fd48dd9d9411fade41f8ef8f9d552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13aa8b10dcb4cd38de8033d646545f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Initialize ELECTRA tokenizer\n",
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels mapping\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values  # Convert sentiments to numeric labels\n",
    "\n",
    "# Split dataset\n",
    "train_d, val_d, train_labels, val_labels = train_test_split(data['processed_full_review'], y, test_size=0.2, random_state=42)\n",
    "texts_train = list(train_d)\n",
    "texts_val = list(val_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "max_length = 64\n",
    "tokenized_texts_train = tokenizer(texts_train, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length)\n",
    "tokenized_texts_val = tokenizer(texts_val, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length)\n",
    "train_labels = torch.tensor(list(train_labels))\n",
    "val_labels = torch.tensor(list(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(tokenized_texts_train['input_ids'], tokenized_texts_train['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(tokenized_texts_val['input_ids'], tokenized_texts_val['attention_mask'], val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec39470087e64e0c92026dbda79fe85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize ELECTRA model for sequence classification with 3 output labels\n",
    "model = ElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator', num_labels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3d580c69c943f591c3c18ee1072682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up optimizer, criterion, and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size and DataLoader\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): GELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_epochs = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training: 100%|██████████| 144/144 [35:01<00:00, 14.59s/it]\n",
      "Epoch 1/3 - Validation: 100%|██████████| 36/36 [02:33<00:00,  4.27s/it]\n",
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "Training Loss: 0.7719, Training Accuracy: 0.6962\n",
      "Training Precision: 0.6182, Training Recall: 0.6962, Training F1: 0.6172\n",
      "Validation Loss: 0.5533, Validation Accuracy: 0.8151\n",
      "Validation Precision: 0.7306, Validation Recall: 0.8151, Validation F1: 0.7685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training: 100%|██████████| 144/144 [32:23<00:00, 13.49s/it]\n",
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Epoch 2/3 - Validation: 100%|██████████| 36/36 [02:33<00:00,  4.26s/it]\n",
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/3\n",
      "Training Loss: 0.5635, Training Accuracy: 0.8151\n",
      "Training Precision: 0.7348, Training Recall: 0.8151, Training F1: 0.7727\n",
      "Validation Loss: 0.5164, Validation Accuracy: 0.8294\n",
      "Validation Precision: 0.7507, Validation Recall: 0.8294, Validation F1: 0.7878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training: 100%|██████████| 144/144 [31:11<00:00, 13.00s/it]\n",
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Epoch 3/3 - Validation: 100%|██████████| 36/36 [02:32<00:00,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/3\n",
      "Training Loss: 0.5466, Training Accuracy: 0.8246\n",
      "Training Precision: 0.7468, Training Recall: 0.8246, Training F1: 0.7830\n",
      "Validation Loss: 0.5142, Validation Accuracy: 0.8320\n",
      "Validation Precision: 0.7538, Validation Recall: 0.8320, Validation F1: 0.7906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    tr_correct_preds = 0\n",
    "    all_tr_labels = []\n",
    "    all_tr_preds = []\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Training\"):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        tr_loss = outputs.loss\n",
    "        train_loss += tr_loss.item()\n",
    "        tr_loss.backward()\n",
    "\n",
    "        tr_logits = outputs.logits\n",
    "        tr_preds = torch.argmax(tr_logits, dim=1)\n",
    "        tr_correct_preds += torch.sum(tr_preds == labels).item()\n",
    "\n",
    "        # Collect predictions and true labels\n",
    "        all_tr_labels.extend(labels.cpu().numpy())\n",
    "        all_tr_preds.extend(tr_preds.cpu().numpy())\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracy = tr_correct_preds / len(train_d)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for training\n",
    "    train_precision = precision_score(all_tr_labels, all_tr_preds, average='weighted')\n",
    "    train_recall = recall_score(all_tr_labels, all_tr_preds, average='weighted')\n",
    "    train_f1 = f1_score(all_tr_labels, all_tr_preds, average='weighted')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    all_val_labels = []\n",
    "    all_val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Validation\"):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_preds += torch.sum(preds == labels).item()\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "            all_val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracy = correct_preds / len(val_d)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Calculate Precision, Recall, F1 for validation\n",
    "    val_precision = precision_score(all_val_labels, all_val_preds, average='weighted')\n",
    "    val_recall = recall_score(all_val_labels, all_val_preds, average='weighted')\n",
    "    val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Training Precision: {train_precision:.4f}, Training Recall: {train_recall:.4f}, Training F1: {train_f1:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation Precision: {val_precision:.4f}, Validation Recall: {val_recall:.4f}, Validation F1: {val_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
