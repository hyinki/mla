{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Singapore Airlines' Service Through Automated Sentiment Analysis of Customer Reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singapore Airlines Customer Reviews Dataset Information\n",
    "\n",
    "The [Singapore Airlines Customer Reviews Dataset](https://www.kaggle.com/datasets/kanchana1990/singapore-airlines-reviews) aggregates 10,000 anonymized customer reviews, providing a broad perspective on the passenger experience with Singapore Airlines. \n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`published_date`**: Date and time of review publication.\n",
    "- **`published_platform`**: Platform where the review was posted.\n",
    "- **`rating`**: Customer satisfaction rating, from 1 (lowest) to 5 (highest).\n",
    "- **`type`**: Specifies the content as a review.\n",
    "- **`text`**: Detailed customer feedback.\n",
    "- **`title`**: Summary of the review.\n",
    "- **`helpful_votes`**: Number of users finding the review helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional web scraping of online reviews\n",
    "\n",
    "During our EDA, we noticed two main trends in the distribution of our dataset:\n",
    "1. Less than 10% of our reviews were published from the years 2022 to 2024, making it hard for us to capture recent trends in sentiment.\n",
    "2. Most of the reviews were highly positive, which could mean that SIA had mostly positive reviews, nevertheless we wanted to get more information on negative reviews to improve the robustness of our model.\n",
    "\n",
    "### TripAdvisor\n",
    "\n",
    "We scraped more data for airline reviews from TripAdvisor, specifically for the years 2022 to 2024. \n",
    "(https://www.tripadvisor.com.sg/Airline_Review-d8729151-Reviews-Singapore-Airlines)\n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`Year`**: Year of review publication.\n",
    "- **`Month`**: Month of review publication.\n",
    "- **`Title`**: Title of review publication.\n",
    "- **`Review Text`**: Main text content of review publication.\n",
    "- **`Rating`**: Numerical rating provided by reviewer (Scale: 1 to 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skytrax\n",
    "\n",
    "We also scraped from Skytrax, which is another data source for online reviews. \n",
    "(https://www.airlinequality.com/airline-reviews/singapore-airlines/?sortby=post_date%3ADesc&pagesize=100)\n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`Year`**: Year of review publication.\n",
    "- **`Month`**: Month of review publication.\n",
    "- **`Title`**: Title of review publication.\n",
    "- **`Review Text`**: Main text content of review publication.\n",
    "- **`Rating`**: Numerical rating provided by reviewer (Scale: 1 to 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Please uncomment the code box below to pip install relevant dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=2.0.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.11.1)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (4.65.0)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (3.7.2)\n",
      "Requirement already satisfied: seaborn>=0.12.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (0.12.2)\n",
      "Requirement already satisfied: langdetect>=1.0.9 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.0.9)\n",
      "Requirement already satisfied: langid>=1.1.6 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (1.1.6)\n",
      "Requirement already satisfied: nltk>=3.8.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (3.8.1)\n",
      "Requirement already satisfied: wordcloud>=1.9.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (1.9.3)\n",
      "Collecting tensorflow>=2.17.1 (from -r requirements.txt (line 11))\n",
      "  Obtaining dependency information for tensorflow>=2.17.1 from https://files.pythonhosted.org/packages/26/08/556c4159675c1a30e077ec2a942eeeb81b457cc35c247a5b4a59a1274f05/tensorflow-2.18.0-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading tensorflow-2.18.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: six in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from langdetect>=1.0.9->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (2022.7.9)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for astunparse>=1.6.0 from https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for flatbuffers>=24.3.25 from https://files.pythonhosted.org/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 from https://files.pythonhosted.org/packages/a3/61/8001b38461d751cd1a0c3a6ae84346796a5758123f3ed97a1b121dfbf4f3/gast-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for google-pasta>=0.1.1 from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/4b/49/f5e3e7e1419872b69f6f5e82ba56e33955a74bd537d8a1f5f1eff2f3668a/libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for opt-einsum>=2.3.2 from https://files.pythonhosted.org/packages/23/cd/066e86230ae37ed0be70aae89aabf03ca8d9f39c8aea0dec8029455b5540/opt_einsum-3.4.0-py3-none-any.whl.metadata\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/1c/f2/baf397f3dd1d3e4af7e3f5a0382b868d25ac068eefe1ebde05132333436c/protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (68.0.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/7f/be/df630c387a0a054815d60be6a97eb4e8f17385d5d6fe660e1c02750062b4/termcolor-2.5.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/79/23/18730cca0d18ffde1de132a9230745a5c113cbc6dd8cde71c2288a21f5a3/grpcio-1.67.0-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading grpcio-1.67.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for tensorboard<2.19,>=2.18 from https://files.pythonhosted.org/packages/b1/de/021c1d407befb505791764ad2cbd56ceaaa53a746baed01d2e2143f05f18/tensorboard-2.18.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for keras>=3.5.0 from https://files.pythonhosted.org/packages/c2/88/eef50051a772dcb4433d1f3e4c1d6576ba450fe83e89d028d7e8b85a2122/keras-3.6.0-py3-none-any.whl.metadata\n",
      "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy>=1.24.0 (from -r requirements.txt (line 2))\n",
      "  Obtaining dependency information for numpy>=1.24.0 from https://files.pythonhosted.org/packages/1a/2e/151484f49fd03944c4a3ad9c418ed193cfd02724e138ac8a9505d056c582/numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.8/114.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=3.11.0 (from tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for h5py>=3.11.0 from https://files.pythonhosted.org/packages/95/9d/eb91a9076aa998bb2179d6b1788055ea09cdf9d6619cd967f1d3321ed056/h5py-3.12.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading h5py-3.12.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for ml-dtypes<0.5.0,>=0.4.0 from https://files.pythonhosted.org/packages/d1/76/9835c8609c29f2214359e88f29255fc4aad4ea0f613fb48aa8815ceda1b6/ml_dtypes-0.4.1-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading ml_dtypes-0.4.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/5b/cc/16634e76f3647fbec18187258da3ba11184a6232dcf9073dc44579076d36/tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.38.4)\n",
      "Requirement already satisfied: rich in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (13.9.2)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for namex from https://files.pythonhosted.org/packages/73/59/7854fbfb59f8ae35483ce93493708be5942ebb6328cd85b3a609df629736/namex-0.0.8-py3-none-any.whl.metadata\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for optree from https://files.pythonhosted.org/packages/ea/45/ebd8b91fc191465dd89f44ad067ca35b1919bda1d6e08c5effab483f9b10/optree-0.13.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading optree-0.13.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.17.1->-r requirements.txt (line 11)) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow>=2.17.1->-r requirements.txt (line 11))\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/7a/13/e503968fefabd4c6b2650af21e110aa8466fe21432cd7c43a84577a89438/tensorboard_data_server-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.17.1->-r requirements.txt (line 11)) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.1.0)\n",
      "Downloading tensorflow-2.18.0-cp311-cp311-macosx_12_0_arm64.whl (239.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.67.0-cp311-cp311-macosx_10_9_universal2.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp311-cp311-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp311-cp311-macosx_10_9_universal2.whl (397 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m398.0/398.0 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl (414 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.7/414.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-macosx_12_0_arm64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.0-cp311-cp311-macosx_11_0_arm64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.7/301.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: namex, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, ml-dtypes, h5py, keras, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.9.0\n",
      "    Uninstalling h5py-3.9.0:\n",
      "      Successfully uninstalled h5py-3.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.67.0 h5py-3.12.1 keras-3.6.0 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.8 numpy-1.26.4 opt-einsum-3.4.0 optree-0.13.0 protobuf-5.28.3 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "import nltk\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# For generating n-grams\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation (Loading CSV)\n",
    "\n",
    "Load the three CSV files into a pandas DataFrame `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processed_full_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>ok use airlin go singapor london heathrow issu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>don give money book paid receiv email confirm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "      <td>best airlin world best airlin world seat food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>premium economi seat singapor airlin not worth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>imposs get promis refund book flight full mont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month sentiment                              processed_full_review\n",
       "0  2024      3   Neutral  ok use airlin go singapor london heathrow issu...\n",
       "1  2024      3  Negative  don give money book paid receiv email confirm ...\n",
       "2  2024      3  Positive  best airlin world best airlin world seat food ...\n",
       "3  2024      3  Negative  premium economi seat singapor airlin not worth...\n",
       "4  2024      3  Negative  imposs get promis refund book flight full mont..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Positive    7913\n",
       "Negative    2441\n",
       "Neutral     1164\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2019    5129\n",
       "2018    2596\n",
       "2022    1184\n",
       "2023    1111\n",
       "2020     888\n",
       "2024     514\n",
       "2021      96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network\n",
    "\n",
    "A Simple Neural Network, or fully connected neural network (FCNN), is a basic deep learning model ideal for straightforward classification tasks. It consists mainly of fully connected layers that process flattened data inputs, making it versatile for many types of data, including text.\n",
    "\n",
    "Below is an explanation of how a simple NN works:\n",
    "\n",
    "1. Embedding Layer (for Text Data):\n",
    "\t- For text inputs, an embedding layer transforms words into numerical vectors that capture meaning and context.\n",
    "    \n",
    "2.\tFlattening:\n",
    "\t- The embeddings are flattened into a single long vector, allowing the network to process them as one input.\n",
    "\n",
    "3.\tDense (Fully Connected) Layers:\n",
    "\t- Dense layers are the core of an FCNN. Each neuron connects to all neurons in the previous layer, learning complex relationships.\n",
    "\t- Activation functions, such as ReLU, are applied here to introduce non-linearity, helping the network capture more intricate patterns.\n",
    "\n",
    "4.\tOutput Layer:\n",
    "\t- The final layer outputs class probabilities using a softmax activation (for multi-class classification) or sigmoid (for binary classification).\n",
    "\t- This layer helps the model predict the likelihood of each class for an input.\n",
    "\t\n",
    "5.\tTraining:\n",
    "\t- During training, the network adjusts its weights to minimize prediction errors, gradually improving its accuracy through backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayaung/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - accuracy: 0.6435 - loss: 0.9063 - val_accuracy: 0.6984 - val_loss: 0.6102\n",
      "Epoch 2/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.7104 - loss: 0.6659 - val_accuracy: 0.8368 - val_loss: 0.4321\n",
      "Epoch 3/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 53ms/step - accuracy: 0.8236 - loss: 0.4386 - val_accuracy: 0.8398 - val_loss: 0.4099\n",
      "Epoch 4/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 72ms/step - accuracy: 0.8627 - loss: 0.3280 - val_accuracy: 0.8477 - val_loss: 0.3986\n",
      "Epoch 5/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - accuracy: 0.8866 - loss: 0.2583 - val_accuracy: 0.8459 - val_loss: 0.4422\n",
      "Epoch 6/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - accuracy: 0.9252 - loss: 0.1976 - val_accuracy: 0.8485 - val_loss: 0.5799\n",
      "Epoch 7/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - accuracy: 0.9531 - loss: 0.1342 - val_accuracy: 0.8542 - val_loss: 0.6312\n",
      "Epoch 8/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.9753 - loss: 0.0802 - val_accuracy: 0.8494 - val_loss: 0.7612\n",
      "Epoch 9/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - accuracy: 0.9836 - loss: 0.0544 - val_accuracy: 0.8468 - val_loss: 0.7183\n",
      "Epoch 10/10\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - accuracy: 0.9914 - loss: 0.0300 - val_accuracy: 0.8533 - val_loss: 0.9004\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Evaluation Metrics:\n",
      "Accuracy: 0.8533\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7941    0.7957    0.7949       470\n",
      "     Neutral     0.4339    0.3596    0.3933       228\n",
      "    Positive     0.9185    0.9402    0.9292      1606\n",
      "\n",
      "    accuracy                         0.8533      2304\n",
      "   macro avg     0.7155    0.6985    0.7058      2304\n",
      "weighted avg     0.8451    0.8533    0.8488      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming 'data' is your DataFrame with 'processed_full_review' and 'sentiment' columns\n",
    "\n",
    "# Step 1: Tokenization and Padding\n",
    "max_words = 10000  # Maximum vocabulary size\n",
    "max_sequence_length = 300  # Maximum length of sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['processed_full_review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_full_review'])\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# One-hot encode the sentiment labels\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y = onehot_encoder.fit_transform(data[['sentiment']])\n",
    "\n",
    "# Step 2: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Define the Simple Neural Network Model\n",
    "def create_simple_nn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding layer\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length))\n",
    "    \n",
    "    # Flatten the embeddings to feed into dense layers\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Fully connected layers\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Output layer for three-class classification using softmax\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # Categorical cross-entropy is used for multi-class classification\n",
    "    return model\n",
    "\n",
    "# Step 4: Train the Model\n",
    "model = create_simple_nn()\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred, target_names=onehot_encoder.categories_[0], digits=4)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
