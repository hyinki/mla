{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Singapore Airlines' Service Through Automated Sentiment Analysis of Customer Reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singapore Airlines Customer Reviews Dataset Information\n",
    "\n",
    "The [Singapore Airlines Customer Reviews Dataset](https://www.kaggle.com/datasets/kanchana1990/singapore-airlines-reviews) aggregates 10,000 anonymized customer reviews, providing a broad perspective on the passenger experience with Singapore Airlines. \n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`published_date`**: Date and time of review publication.\n",
    "- **`published_platform`**: Platform where the review was posted.\n",
    "- **`rating`**: Customer satisfaction rating, from 1 (lowest) to 5 (highest).\n",
    "- **`type`**: Specifies the content as a review.\n",
    "- **`text`**: Detailed customer feedback.\n",
    "- **`title`**: Summary of the review.\n",
    "- **`helpful_votes`**: Number of users finding the review helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Please uncomment the code box below to pip install relevant dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# For concurrency (running functions in parallel)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# For caching (to speed up repeated function calls)\n",
    "from functools import lru_cache\n",
    "\n",
    "# For progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting and Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Language Detection packages\n",
    "# `langdetect` for detecting language\n",
    "from langdetect import detect as langdetect_detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "# `langid` for an alternative language detection method\n",
    "from langid import classify as langid_classify\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "# Regular expressions for text pattern matching\n",
    "import re\n",
    "\n",
    "# Word Cloud generation\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# For generating n-grams\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Libraries for Word2Vec and Logistic Regression\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation (Loading CSV)\n",
    "\n",
    "Load the `singapore_airline_reviews.csv` file into a pandas DataFrame `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   published_date      10000 non-null  object\n",
      " 1   published_platform  10000 non-null  object\n",
      " 2   rating              10000 non-null  int64 \n",
      " 3   type                10000 non-null  object\n",
      " 4   text                10000 non-null  object\n",
      " 5   title               10000 non-null  object\n",
      " 6   helpful_votes       10000 non-null  int64 \n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 547.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"singapore_airlines_reviews.csv\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published_date</th>\n",
       "      <th>published_platform</th>\n",
       "      <th>rating</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>helpful_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-12T14:41:14-04:00</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>3</td>\n",
       "      <td>review</td>\n",
       "      <td>We used this airline to go from Singapore to L...</td>\n",
       "      <td>Ok</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-11T19:39:13-04:00</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>5</td>\n",
       "      <td>review</td>\n",
       "      <td>The service on Singapore Airlines Suites Class...</td>\n",
       "      <td>The service in Suites Class makes one feel lik...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-11T12:20:23-04:00</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>1</td>\n",
       "      <td>review</td>\n",
       "      <td>Booked, paid and received email confirmation f...</td>\n",
       "      <td>Don’t give them your money</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-11T07:12:27-04:00</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>5</td>\n",
       "      <td>review</td>\n",
       "      <td>Best airline in the world, seats, food, servic...</td>\n",
       "      <td>Best Airline in the World</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-03-10T05:34:18-04:00</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>2</td>\n",
       "      <td>review</td>\n",
       "      <td>Premium Economy Seating on Singapore Airlines ...</td>\n",
       "      <td>Premium Economy Seating on Singapore Airlines ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              published_date published_platform  rating    type  \\\n",
       "0  2024-03-12T14:41:14-04:00            Desktop       3  review   \n",
       "1  2024-03-11T19:39:13-04:00            Desktop       5  review   \n",
       "2  2024-03-11T12:20:23-04:00            Desktop       1  review   \n",
       "3  2024-03-11T07:12:27-04:00            Desktop       5  review   \n",
       "4  2024-03-10T05:34:18-04:00            Desktop       2  review   \n",
       "\n",
       "                                                text  \\\n",
       "0  We used this airline to go from Singapore to L...   \n",
       "1  The service on Singapore Airlines Suites Class...   \n",
       "2  Booked, paid and received email confirmation f...   \n",
       "3  Best airline in the world, seats, food, servic...   \n",
       "4  Premium Economy Seating on Singapore Airlines ...   \n",
       "\n",
       "                                               title  helpful_votes  \n",
       "0                                                 Ok              0  \n",
       "1  The service in Suites Class makes one feel lik...              0  \n",
       "2                         Don’t give them your money              0  \n",
       "3                          Best Airline in the World              0  \n",
       "4  Premium Economy Seating on Singapore Airlines ...              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Remove Duplicate Rows\n",
    "\n",
    "- Drop duplicate rows from the dataframe (`data`) and reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new shape is:  (10000, 7)\n",
      "Remaining duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "data = data.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Display the new dataframe shape\n",
    "print(\"The new shape is: \", data.shape)\n",
    "\n",
    "# Make sure no more duplicates are present\n",
    "print(\"Remaining duplicate rows:\", data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Null Values\n",
    "\n",
    "- Here we check which features have null values using the `isnull()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "published_date        0\n",
       "published_platform    0\n",
       "rating                0\n",
       "type                  0\n",
       "text                  0\n",
       "title                 0\n",
       "helpful_votes         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this case only `title` feature has one null value, will fill it with empty string \" \"\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with empty string\n",
    "data = data.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "published_date        0\n",
       "published_platform    0\n",
       "rating                0\n",
       "type                  0\n",
       "text                  0\n",
       "title                 0\n",
       "helpful_votes         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that there are no missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data types\n",
    "\n",
    "Since the column `published_date` is in data type (`str`), we will\n",
    "- Convert `published_date` to a standard timezone (UTC) format as a new column `date`.\n",
    "- Drop the original `published_date` column after conversion and reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns, UTC]\n"
     ]
    }
   ],
   "source": [
    "# Set `utc=True` to convert the date to common timezone (UTC)\n",
    "data[\"date\"] = pd.to_datetime(data[\"published_date\"], utc=True)\n",
    "print(data[\"date\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published_platform</th>\n",
       "      <th>rating</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>3</td>\n",
       "      <td>review</td>\n",
       "      <td>We used this airline to go from Singapore to L...</td>\n",
       "      <td>Ok</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-12 18:41:14+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>5</td>\n",
       "      <td>review</td>\n",
       "      <td>The service on Singapore Airlines Suites Class...</td>\n",
       "      <td>The service in Suites Class makes one feel lik...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-11 23:39:13+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>1</td>\n",
       "      <td>review</td>\n",
       "      <td>Booked, paid and received email confirmation f...</td>\n",
       "      <td>Don’t give them your money</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-11 16:20:23+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>5</td>\n",
       "      <td>review</td>\n",
       "      <td>Best airline in the world, seats, food, servic...</td>\n",
       "      <td>Best Airline in the World</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-11 11:12:27+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>2</td>\n",
       "      <td>review</td>\n",
       "      <td>Premium Economy Seating on Singapore Airlines ...</td>\n",
       "      <td>Premium Economy Seating on Singapore Airlines ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-10 09:34:18+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  published_platform  rating    type  \\\n",
       "0            Desktop       3  review   \n",
       "1            Desktop       5  review   \n",
       "2            Desktop       1  review   \n",
       "3            Desktop       5  review   \n",
       "4            Desktop       2  review   \n",
       "\n",
       "                                                text  \\\n",
       "0  We used this airline to go from Singapore to L...   \n",
       "1  The service on Singapore Airlines Suites Class...   \n",
       "2  Booked, paid and received email confirmation f...   \n",
       "3  Best airline in the world, seats, food, servic...   \n",
       "4  Premium Economy Seating on Singapore Airlines ...   \n",
       "\n",
       "                                               title  helpful_votes  \\\n",
       "0                                                 Ok              0   \n",
       "1  The service in Suites Class makes one feel lik...              0   \n",
       "2                         Don’t give them your money              0   \n",
       "3                          Best Airline in the World              0   \n",
       "4  Premium Economy Seating on Singapore Airlines ...              0   \n",
       "\n",
       "                       date  \n",
       "0 2024-03-12 18:41:14+00:00  \n",
       "1 2024-03-11 23:39:13+00:00  \n",
       "2 2024-03-11 16:20:23+00:00  \n",
       "3 2024-03-11 11:12:27+00:00  \n",
       "4 2024-03-10 09:34:18+00:00  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop `published_date` column and reset the index\n",
    "data = data.drop(columns=[\"published_date\"]).reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Outliers\n",
    "\n",
    "### `text`\n",
    "\n",
    "The `text` column of `data`, which is of string (`str`) type, may contain values with unusually long lengths, indicating the presence of outliers. We will identify the outliers using [Z-score method].\n",
    "\n",
    "1. Create a new column `text_length` in the DataFrame `data` by calculating the length of each review. (Set the value as 0 if the correponding `text` column has NaN values.)\n",
    "\n",
    "2. Check the statistics of `text_length` using `describe()` method.\n",
    "\n",
    "3. Calculate the mean and standard deviation of the `text_length` column.\n",
    "\n",
    "4. Set the Z-score threshold for identifying outliers to 3.\n",
    "\n",
    "5. Identify outliers of the `text_length` column and set the corresponding `text` to np.nan.\n",
    "\n",
    "6. Drop the `text_length` column from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  published_platform  rating    type  \\\n",
      "0            Desktop       3  review   \n",
      "1            Desktop       5  review   \n",
      "2            Desktop       1  review   \n",
      "\n",
      "                                                text  \\\n",
      "0  We used this airline to go from Singapore to L...   \n",
      "1  The service on Singapore Airlines Suites Class...   \n",
      "2  Booked, paid and received email confirmation f...   \n",
      "\n",
      "                                               title  helpful_votes  \\\n",
      "0                                                 Ok              0   \n",
      "1  The service in Suites Class makes one feel lik...              0   \n",
      "2                         Don’t give them your money              0   \n",
      "\n",
      "                       date  text_length  \n",
      "0 2024-03-12 18:41:14+00:00         1352  \n",
      "1 2024-03-11 23:39:13+00:00         4666  \n",
      "2 2024-03-11 16:20:23+00:00          420  \n",
      "count    10000.000000\n",
      "mean       556.526800\n",
      "std        640.290638\n",
      "min        100.000000\n",
      "25%        228.000000\n",
      "50%        380.000000\n",
      "75%        665.000000\n",
      "max      18717.000000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data['text_length'] = data['text'].apply(lambda x: len(x) if pd.notna(x) else 0)\n",
    "print(data.head(3))\n",
    "\n",
    "TL = data[\"text_length\"]\n",
    "stats_TL = TL.describe()\n",
    "print(stats_TL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_TL = TL.mean()\n",
    "# print(mean_TL)\n",
    "\n",
    "sd_TL = TL.std()\n",
    "# print(sd_TL)\n",
    "\n",
    "threshold = 3\n",
    "\n",
    "z_score = zscore(TL)\n",
    "# print(z_score)\n",
    "\n",
    "# Remove 'text' of lengths that are greater than 3 standard deviations above the mean\n",
    "data.loc[abs(z_score) > threshold, 'text'] = np.nan\n",
    "# print(data.head(3))\n",
    "\n",
    "data = data.drop(\"text_length\", axis=1)\n",
    "\n",
    "data.head()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `title`\n",
    "\n",
    "Similarly, the `title` column of `data` (of type `str`) may also contain values with unusually long lengths, indicating the presence of outliers.\n",
    "\n",
    "1. Create a new column `title_length` in the DataFrame `data` by calculating the length of each price value. (Set the value as 0 if the correponding `title` column has NaN values.)\n",
    "\n",
    "2. Check the statistics of `title_length` using `describe()` method and display its unique values.\n",
    "\n",
    "3. Identify the outlier values by inspecting the content in `title` corresponding to the abnormal value in `title_length` and set the corresponding value of `title` to np.nan.\n",
    "\n",
    "4. Drop the `title_length` column from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  published_platform  rating    type  \\\n",
      "0            Desktop       3  review   \n",
      "1            Desktop       5  review   \n",
      "2            Desktop       1  review   \n",
      "\n",
      "                                                text  \\\n",
      "0  We used this airline to go from Singapore to L...   \n",
      "1                                                NaN   \n",
      "2  Booked, paid and received email confirmation f...   \n",
      "\n",
      "                                               title  helpful_votes  \\\n",
      "0                                                 Ok              0   \n",
      "1  The service in Suites Class makes one feel lik...              0   \n",
      "2                         Don’t give them your money              0   \n",
      "\n",
      "                       date  title_length  \n",
      "0 2024-03-12 18:41:14+00:00             2  \n",
      "1 2024-03-11 23:39:13+00:00            51  \n",
      "2 2024-03-11 16:20:23+00:00            26  \n",
      "count    10000.00000\n",
      "mean        28.40990\n",
      "std         17.27913\n",
      "min          2.00000\n",
      "25%         16.00000\n",
      "50%         24.00000\n",
      "75%         36.00000\n",
      "max        120.00000\n",
      "Name: title_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data['title_length'] = data['title'].apply(lambda x: len(x) if pd.notna(x) else 0)\n",
    "print(data.head(3))\n",
    "\n",
    "TL = data[\"title_length\"]\n",
    "stats_TL = TL.describe()\n",
    "print(stats_TL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published_platform</th>\n",
       "      <th>rating</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>3</td>\n",
       "      <td>review</td>\n",
       "      <td>We used this airline to go from Singapore to L...</td>\n",
       "      <td>Ok</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-12 18:41:14+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>5</td>\n",
       "      <td>review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The service in Suites Class makes one feel lik...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-11 23:39:13+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>1</td>\n",
       "      <td>review</td>\n",
       "      <td>Booked, paid and received email confirmation f...</td>\n",
       "      <td>Don’t give them your money</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-11 16:20:23+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>5</td>\n",
       "      <td>review</td>\n",
       "      <td>Best airline in the world, seats, food, servic...</td>\n",
       "      <td>Best Airline in the World</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-11 11:12:27+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>2</td>\n",
       "      <td>review</td>\n",
       "      <td>Premium Economy Seating on Singapore Airlines ...</td>\n",
       "      <td>Premium Economy Seating on Singapore Airlines ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-10 09:34:18+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  published_platform  rating    type  \\\n",
       "0            Desktop       3  review   \n",
       "1            Desktop       5  review   \n",
       "2            Desktop       1  review   \n",
       "3            Desktop       5  review   \n",
       "4            Desktop       2  review   \n",
       "\n",
       "                                                text  \\\n",
       "0  We used this airline to go from Singapore to L...   \n",
       "1                                                NaN   \n",
       "2  Booked, paid and received email confirmation f...   \n",
       "3  Best airline in the world, seats, food, servic...   \n",
       "4  Premium Economy Seating on Singapore Airlines ...   \n",
       "\n",
       "                                               title  helpful_votes  \\\n",
       "0                                                 Ok              0   \n",
       "1  The service in Suites Class makes one feel lik...              0   \n",
       "2                         Don’t give them your money              0   \n",
       "3                          Best Airline in the World              0   \n",
       "4  Premium Economy Seating on Singapore Airlines ...              0   \n",
       "\n",
       "                       date  \n",
       "0 2024-03-12 18:41:14+00:00  \n",
       "1 2024-03-11 23:39:13+00:00  \n",
       "2 2024-03-11 16:20:23+00:00  \n",
       "3 2024-03-11 11:12:27+00:00  \n",
       "4 2024-03-10 09:34:18+00:00  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_TL = TL.mean()\n",
    "# print(mean_TL)\n",
    "\n",
    "sd_TL = TL.std()\n",
    "# print(sd_TL)\n",
    "\n",
    "threshold = 3\n",
    "\n",
    "z_score = zscore(TL)\n",
    "# print(z_score)\n",
    "\n",
    "# Remove 'title' of lengths that are greater than 3 standard deviations above the mean\n",
    "data.loc[abs(z_score) > threshold, 'title'] = np.nan\n",
    "# print(data.head(3))\n",
    "\n",
    "data = data.drop(\"title_length\", axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype              \n",
      "---  ------              --------------  -----              \n",
      " 0   published_platform  10000 non-null  object             \n",
      " 1   rating              10000 non-null  int64              \n",
      " 2   type                10000 non-null  object             \n",
      " 3   text                9846 non-null   object             \n",
      " 4   title               9834 non-null   object             \n",
      " 5   helpful_votes       10000 non-null  int64              \n",
      " 6   date                10000 non-null  datetime64[ns, UTC]\n",
      "dtypes: datetime64[ns, UTC](1), int64(2), object(4)\n",
      "memory usage: 547.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "published_platform                 object\n",
      "rating                              int64\n",
      "type                               object\n",
      "text                               object\n",
      "title                              object\n",
      "helpful_votes                       int64\n",
      "date                  datetime64[ns, UTC]\n",
      "dtype: object\n",
      "Remaining duplicate rows: 0\n",
      "Unique ratings: [3 5 1 2 4]\n"
     ]
    }
   ],
   "source": [
    "#check data types of each column, make sure they are correct\n",
    "print(data.dtypes)\n",
    "\n",
    "# Make sure no more duplicates are present\n",
    "print(\"Remaining duplicate rows:\", data.duplicated().sum())\n",
    "\n",
    "# Check for outliers in ratings\n",
    "print(\"Unique ratings:\", data['rating'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "published_platform      0\n",
       "rating                  0\n",
       "type                    0\n",
       "text                  154\n",
       "title                 166\n",
       "helpful_votes           0\n",
       "date                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new column `full_review`\n",
    "Since there are some rows with empty `text` and `title`, we will concatenate both columns (`text` and `title`) to form a new column `full_review`.\n",
    "1. Replace `NaN` values in `text` and `title` with an empty string.\n",
    "\n",
    "2. Combine `text` and `title` into `full_review`.\n",
    "\n",
    "3. Strip any leading/trailing whitespaces in `full_review`.\n",
    "\n",
    "4. Drop `text` and `title` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  published_platform  rating    type  helpful_votes                      date  \\\n",
      "0            Desktop       3  review              0 2024-03-12 18:41:14+00:00   \n",
      "1            Desktop       5  review              0 2024-03-11 23:39:13+00:00   \n",
      "2            Desktop       1  review              0 2024-03-11 16:20:23+00:00   \n",
      "3            Desktop       5  review              0 2024-03-11 11:12:27+00:00   \n",
      "4            Desktop       2  review              0 2024-03-10 09:34:18+00:00   \n",
      "\n",
      "                                         full_review  \n",
      "0  We used this airline to go from Singapore to L...  \n",
      "1  The service in Suites Class makes one feel lik...  \n",
      "2  Booked, paid and received email confirmation f...  \n",
      "3  Best airline in the world, seats, food, servic...  \n",
      "4  Premium Economy Seating on Singapore Airlines ...  \n",
      "\n",
      "The old shape is: (10000, 6)\n"
     ]
    }
   ],
   "source": [
    "# 1) Fill NaN values in 'text' and 'title' with an empty string\n",
    "data['title'] = data['title'].fillna('')\n",
    "data['text'] = data['text'].fillna('')\n",
    "\n",
    "# 2) Combine 'text' and 'title' into 'full_review'\n",
    "data['full_review'] = data['text'] + \" \" + data['title']\n",
    "\n",
    "# 3) Strip any leading/trailing whitespace\n",
    "data['full_review'] = data['full_review'].str.strip()\n",
    "\n",
    "# 4) Drop `text` and `title` columns\n",
    "data = data.drop(columns = ['text', 'title'])\n",
    "\n",
    "# Check if the 'full_review' column was added and if 'text' and 'title' columns has been dropped\n",
    "print(data.head())\n",
    "print(\"\\nThe old shape is:\",data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove empty strings\n",
    "1. Drop rows where `full_review` are empty strings and reset the index.\n",
    "\n",
    "2. Check if there are no more null values in `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new shape is: (9989, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "published_platform    0\n",
       "rating                0\n",
       "type                  0\n",
       "helpful_votes         0\n",
       "date                  0\n",
       "full_review           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Drop rows where `full_review` are empty strings and reset the index\n",
    "data = data[data['full_review'] != \"\"].reset_index(drop=True)\n",
    "print(\"The new shape is:\",data.shape)\n",
    "\n",
    "# 2) Check if there are no more null values in `data`\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new column `language`\n",
    "In the case where there are rows where `full_review` are in different languages (e.g., French, Russian, etc.) other than English. We decided to use 2 different language detector libraries (`langdetect`, `langid`) on the `full_review` column and combined the predictions of all 2 libraries and selecting the most frequent predicted language.\n",
    "\n",
    "**Reason**: `langdetect` might perform well on longer texts while `langid` is more reliable on short texts, using multiple detectors reduces the likelihood of misclassification and mitigates individual detector errors, leading to more accurate overall predictions. Also, even if one detector fails or throws an error, the other can still provide predictions, therefore improving the robustness of the language detection.\n",
    "\n",
    "1. Set a seed for `langdetect` to ensure reproducibility.\n",
    "\n",
    "2. Preprocess the text in `full_review`:\n",
    "    - a\\) Function to remove non-alphabetic characters and normalise whitespaces in  `full_review`.\n",
    "    - b\\) Function to determine if the text is non-language (e.g., numbers, symbols only).\n",
    "\n",
    "3. Two functions for language detection:\n",
    "    - a\\) Using `langdetect`.\n",
    "    - b\\) Using `langid`.\n",
    "\n",
    "4. Function for calculating majority vote for each language.\n",
    "\n",
    "5. Function for parallel processing for efficiency.\n",
    "\n",
    "6. Caching function for repeated inputs\n",
    "\n",
    "7. Function for choosing language based on combined majority voting.\n",
    "\n",
    "8. Applying the combined function on `full_review` column.\n",
    "\n",
    "9. Display the resulting `data` DataFrame.\n",
    "\n",
    "### <span style=\"color:red\">The code below will take approximately 1 minute to run!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Language Detection: 100%|██████████| 9989/9989 [08:02<00:00, 20.69it/s]  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published_platform</th>\n",
       "      <th>rating</th>\n",
       "      <th>type</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>date</th>\n",
       "      <th>full_review</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>3</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-12 18:41:14+00:00</td>\n",
       "      <td>We used this airline to go from Singapore to L...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>5</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-11 23:39:13+00:00</td>\n",
       "      <td>The service in Suites Class makes one feel lik...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>1</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-11 16:20:23+00:00</td>\n",
       "      <td>Booked, paid and received email confirmation f...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>5</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-11 11:12:27+00:00</td>\n",
       "      <td>Best airline in the world, seats, food, servic...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>2</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-10 09:34:18+00:00</td>\n",
       "      <td>Premium Economy Seating on Singapore Airlines ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>5</td>\n",
       "      <td>review</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-06 07:48:21+00:00</td>\n",
       "      <td>First part done with Singapore Airlines - acce...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>Mobile</td>\n",
       "      <td>5</td>\n",
       "      <td>review</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-06 02:50:29+00:00</td>\n",
       "      <td>And again a great Flight with Singapore Air. G...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>5</td>\n",
       "      <td>review</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-08-06 02:47:06+00:00</td>\n",
       "      <td>We flew business class from Frankfurt, via Sin...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>4</td>\n",
       "      <td>review</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-08-06 00:32:03+00:00</td>\n",
       "      <td>As always, the A380 aircraft was spotlessly pr...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>4</td>\n",
       "      <td>review</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-08-06 00:19:51+00:00</td>\n",
       "      <td>As always, Singapore Airlines has done it agai...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9989 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     published_platform  rating    type  helpful_votes  \\\n",
       "0               Desktop       3  review              0   \n",
       "1               Desktop       5  review              0   \n",
       "2               Desktop       1  review              0   \n",
       "3               Desktop       5  review              0   \n",
       "4               Desktop       2  review              0   \n",
       "...                 ...     ...     ...            ...   \n",
       "9984            Desktop       5  review              1   \n",
       "9985             Mobile       5  review              1   \n",
       "9986            Desktop       5  review              1   \n",
       "9987            Desktop       4  review              2   \n",
       "9988            Desktop       4  review              3   \n",
       "\n",
       "                          date  \\\n",
       "0    2024-03-12 18:41:14+00:00   \n",
       "1    2024-03-11 23:39:13+00:00   \n",
       "2    2024-03-11 16:20:23+00:00   \n",
       "3    2024-03-11 11:12:27+00:00   \n",
       "4    2024-03-10 09:34:18+00:00   \n",
       "...                        ...   \n",
       "9984 2018-08-06 07:48:21+00:00   \n",
       "9985 2018-08-06 02:50:29+00:00   \n",
       "9986 2018-08-06 02:47:06+00:00   \n",
       "9987 2018-08-06 00:32:03+00:00   \n",
       "9988 2018-08-06 00:19:51+00:00   \n",
       "\n",
       "                                            full_review language  \n",
       "0     We used this airline to go from Singapore to L...       en  \n",
       "1     The service in Suites Class makes one feel lik...       en  \n",
       "2     Booked, paid and received email confirmation f...       en  \n",
       "3     Best airline in the world, seats, food, servic...       en  \n",
       "4     Premium Economy Seating on Singapore Airlines ...       en  \n",
       "...                                                 ...      ...  \n",
       "9984  First part done with Singapore Airlines - acce...       en  \n",
       "9985  And again a great Flight with Singapore Air. G...       en  \n",
       "9986  We flew business class from Frankfurt, via Sin...       en  \n",
       "9987  As always, the A380 aircraft was spotlessly pr...       en  \n",
       "9988  As always, Singapore Airlines has done it agai...       en  \n",
       "\n",
       "[9989 rows x 7 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Set a seed for langdetect to ensure reproducibility\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# 2a) Simplified preprocessing: only remove non-alphabetic characters\n",
    "def preprocess_text_simple(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    return text.strip()\n",
    "\n",
    "# 2b) Check if the text is non-language (e.g., numbers, symbols only)\n",
    "def is_non_language_text(text):\n",
    "    if re.match(r'^[^a-zA-Z]*$', text):  # Check if text has no alphabetic characters\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# 3a) Function to get langdetect prediction\n",
    "def get_langdetect_prediction(text):\n",
    "    try:\n",
    "        # Directly use text without preprocessing for efficiency\n",
    "        if len(text) < 10 or is_non_language_text(text):\n",
    "            return \"unknown\"\n",
    "        lang = langdetect_detect(text)\n",
    "        return lang\n",
    "    except LangDetectException:\n",
    "        return \"unknown\"\n",
    "\n",
    "# 3b) Function to get langid prediction\n",
    "def get_langid_prediction(text):\n",
    "    try:\n",
    "        lang, _ = langid_classify(text)\n",
    "        if len(text) < 10 or is_non_language_text(text):\n",
    "            return \"unknown\"\n",
    "        return lang\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "# 4) Function to calculate majority vote for each language\n",
    "def calculate_majority_vote(predictions):\n",
    "    vote_counts = {}\n",
    "    for lang in predictions:\n",
    "        if lang in vote_counts:\n",
    "            vote_counts[lang] += 1\n",
    "        else:\n",
    "            vote_counts[lang] = 1\n",
    "    return vote_counts\n",
    "\n",
    "# 5) Parallel processing for efficiency with limited workers\n",
    "def parallel_detection(text):\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(executor.map(lambda func: func(text), \n",
    "                                    [get_langdetect_prediction, get_langid_prediction]))\n",
    "    return results\n",
    "\n",
    "# 6) Caching function for repeated inputs\n",
    "@lru_cache(maxsize=500)\n",
    "def get_cached_language(text):\n",
    "    return combined_language_detection(text)\n",
    "\n",
    "# 7) Combined majority voting language detection function\n",
    "def combined_language_detection(text):\n",
    "    # Check if the text is non-language (e.g., numbers, symbols only)\n",
    "    if is_non_language_text(text):\n",
    "        return \"unknown\"\n",
    "    \n",
    "    # Run the detectors in parallel for efficiency\n",
    "    predictions = parallel_detection(text)\n",
    "    \n",
    "    # Calculate majority vote for each language based on predictions\n",
    "    vote_counts = calculate_majority_vote(predictions)\n",
    "    \n",
    "    # Determine the language with the highest majority vote\n",
    "    final_language = max(vote_counts, key=vote_counts.get)\n",
    "    \n",
    "    # If \"unknown\" is the most common or if all detectors fail, return \"unknown\"\n",
    "    if final_language == \"unknown\" or vote_counts[final_language] <= 1:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    return final_language\n",
    "\n",
    "# 8) Apply the cached function to each text in the DataFrame with a progress bar\n",
    "data['language'] = [get_cached_language(text) for text in tqdm(data['full_review'], desc=\"Language Detection\")]\n",
    "\n",
    "# 9) Display the DataFrame with detected languages\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en         9952\n",
       "unknown      32\n",
       "es            1\n",
       "de            1\n",
       "th            1\n",
       "fr            1\n",
       "sv            1\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See distribution of languages\n",
    "data[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9952, 7)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where language is NOT in english and reset the index\n",
    "data = data[data['language'] == 'en'].reset_index(drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop the `language` column since all values of `language` are `en` and all `full_review` are in the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9952 entries, 0 to 9951\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype              \n",
      "---  ------              --------------  -----              \n",
      " 0   published_platform  9952 non-null   object             \n",
      " 1   rating              9952 non-null   int64              \n",
      " 2   type                9952 non-null   object             \n",
      " 3   helpful_votes       9952 non-null   int64              \n",
      " 4   date                9952 non-null   datetime64[ns, UTC]\n",
      " 5   full_review         9952 non-null   object             \n",
      " 6   language            9952 non-null   object             \n",
      "dtypes: datetime64[ns, UTC](1), int64(2), object(4)\n",
      "memory usage: 544.4+ KB\n",
      "The new shape is: (9952, 6)\n"
     ]
    }
   ],
   "source": [
    "data.info()\n",
    "data.drop(columns=[\"language\"], inplace=True)\n",
    "print(\"The new shape is:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published_platform</th>\n",
       "      <th>rating</th>\n",
       "      <th>type</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>date</th>\n",
       "      <th>full_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>3</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-12 18:41:14+00:00</td>\n",
       "      <td>We used this airline to go from Singapore to L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>5</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-11 23:39:13+00:00</td>\n",
       "      <td>The service in Suites Class makes one feel lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>1</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-11 16:20:23+00:00</td>\n",
       "      <td>Booked, paid and received email confirmation f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>5</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-11 11:12:27+00:00</td>\n",
       "      <td>Best airline in the world, seats, food, servic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Desktop</td>\n",
       "      <td>2</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-03-10 09:34:18+00:00</td>\n",
       "      <td>Premium Economy Seating on Singapore Airlines ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  published_platform  rating    type  helpful_votes                      date  \\\n",
       "0            Desktop       3  review              0 2024-03-12 18:41:14+00:00   \n",
       "1            Desktop       5  review              0 2024-03-11 23:39:13+00:00   \n",
       "2            Desktop       1  review              0 2024-03-11 16:20:23+00:00   \n",
       "3            Desktop       5  review              0 2024-03-11 11:12:27+00:00   \n",
       "4            Desktop       2  review              0 2024-03-10 09:34:18+00:00   \n",
       "\n",
       "                                         full_review  \n",
       "0  We used this airline to go from Singapore to L...  \n",
       "1  The service in Suites Class makes one feel lik...  \n",
       "2  Booked, paid and received email confirmation f...  \n",
       "3  Best airline in the world, seats, food, servic...  \n",
       "4  Premium Economy Seating on Singapore Airlines ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing for NLP\n",
    "\n",
    "# count vectorisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hyinki/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/hyinki/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hyinki/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/hyinki/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hyinki/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/hyinki/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "## for Mac users, might have to install this manually\n",
    "\n",
    "# Ensure require NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 82.95%\n",
      "Predicted Sentiments: [2 2 2 ... 2 2 1]\n",
      "True Sentiments: [2 2 0 ... 2 2 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyinki/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Define X (features) and y (target) based on multiclass sentiment\n",
    "X = data['full_review']  # Review text\n",
    "\n",
    "# Multiclass sentiment: 0 for negative, 1 for neutral, 2 for positive\n",
    "def sentiment_label(rating):\n",
    "    if rating < 2:\n",
    "        return 0  # Negative\n",
    "    elif rating == 3:\n",
    "        return 1  # Neutral\n",
    "    else:\n",
    "        return 2  # Positive\n",
    "\n",
    "y = data['rating'].apply(sentiment_label)\n",
    "\n",
    "# Text preprocessing: vectorizing the text data\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Logistic regression model (for multiclass classification)\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting and evaluating\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Output the accuracy\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Optional: Output predictions and true values for comparison\n",
    "print(\"Predicted Sentiments:\", y_pred)\n",
    "print(\"True Sentiments:\", y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec + log regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Downloading gensim-4.3.3-cp312-cp312-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl (30.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.4/30.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy, gensim\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Successfully installed gensim-4.3.3 scipy-1.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"final_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 83.28%\n",
      "Cross-Validation Scores: [0.82421875 0.83072917 0.82552083 0.82587929 0.81676075]\n",
      "Average Cross-Validation Accuracy: 82.46%\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define X (features) and y (target) based on multiclass sentiment\n",
    "X = data['processed_full_review']  # Review text\n",
    "\n",
    "y= data['sentiment']\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in X]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=2)\n",
    "\n",
    "# Function to convert each sentence into an average word2vec vector\n",
    "def sentence_to_avg_vector(sentence, model):\n",
    "    words = [word for word in sentence if word in model.wv]  # Keep only words present in the Word2Vec vocabulary\n",
    "    if len(words) > 0:\n",
    "        return np.mean([model.wv[word] for word in words], axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Return a zero vector if no words from the sentence are in the vocabulary\n",
    "\n",
    "# Convert all tokenized sentences to their corresponding average word2vec vectors\n",
    "X_word2vec = np.array([sentence_to_avg_vector(sentence, word2vec_model) for sentence in tokenized_sentences])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X_vectors, y, cv=kf, scoring=make_scorer(accuracy_score))\n",
    "\n",
    "# Output cross-validation results\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Average Cross-Validation Accuracy: {np.mean(cv_scores) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec + SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyinki/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 82.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyinki/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hyinki/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hyinki/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hyinki/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hyinki/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.82725694 0.82074653 0.83637153 0.82023448 0.82978723]\n",
      "Average Cross-Validation Accuracy: 82.69%\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Define X (features) and y (target) based on multiclass sentiment\n",
    "X = data['processed_full_review']  # Review text\n",
    "y = data['sentiment']\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in X]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=2)\n",
    "\n",
    "# Function to convert each sentence into an average word2vec vector\n",
    "def sentence_to_avg_vector(sentence, model):\n",
    "    words = [word for word in sentence if word in model.wv]  # Keep only words present in the Word2Vec vocabulary\n",
    "    if len(words) > 0:\n",
    "        return np.mean([model.wv[word] for word in words], axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Return a zero vector if no words from the sentence are in the vocabulary\n",
    "\n",
    "# Convert all tokenized sentences to their corresponding average word2vec vectors\n",
    "X_word2vec = np.array([sentence_to_avg_vector(sentence, word2vec_model) for sentence in tokenized_sentences])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train an SVM model\n",
    "model = SVC(kernel='linear', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X_word2vec, y, cv=kf, scoring=make_scorer(accuracy_score))\n",
    "\n",
    "# Output cross-validation results\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Average Cross-Validation Accuracy: {np.mean(cv_scores) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec + random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 82.81%\n",
      "Cross-Validation Scores: [0.81987847 0.82074653 0.82638889 0.82023448 0.81676075]\n",
      "Average Cross-Validation Accuracy: 82.08%\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Define X (features) and y (target) based on multiclass sentiment\n",
    "X = data['processed_full_review']  # Review text\n",
    "y = data['sentiment']\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in X]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=2)\n",
    "\n",
    "# Function to convert each sentence into an average word2vec vector\n",
    "def sentence_to_avg_vector(sentence, model):\n",
    "    words = [word for word in sentence if word in model.wv]  # Keep only words present in the Word2Vec vocabulary\n",
    "    if len(words) > 0:\n",
    "        return np.mean([model.wv[word] for word in words], axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Return a zero vector if no words from the sentence are in the vocabulary\n",
    "\n",
    "# Convert all tokenized sentences to their corresponding average word2vec vectors\n",
    "X_word2vec = np.array([sentence_to_avg_vector(sentence, word2vec_model) for sentence in tokenized_sentences])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X_word2vec, y, cv=kf, scoring=make_scorer(accuracy_score))\n",
    "\n",
    "# Output cross-validation results\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Average Cross-Validation Accuracy: {np.mean(cv_scores) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText Embeddings + log regression\n",
    "FastText is an extension of Word2Vec developed by Facebook’s AI Research (FAIR). While Word2Vec treats each word as a unique token, FastText breaks words into character n-grams (subword information). This means that it can generate vectors for words that were not seen during training, as long as their subwords were seen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hyinki/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 82.32%\n",
      "Cross-Validation Scores: [0.82291667 0.82508681 0.82986111 0.82023448 0.8154581 ]\n",
      "Average Cross-Validation Accuracy: 82.27%\n"
     ]
    }
   ],
   "source": [
    "# Ensure you have the required NLTK package\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Define X (features) and y (target) based on multiclass sentiment\n",
    "X = data['processed_full_review']  # Review text\n",
    "\n",
    "y = data['sentiment']\n",
    "\n",
    "# Step 1: Tokenize the sentences\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence.lower()) for sentence in X]\n",
    "\n",
    "# Step 2: Train the FastText model\n",
    "fasttext_model = FastText(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 3: Function to average word vectors for each sentence\n",
    "def get_sentence_vector(sentence, model, vector_size):\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)  # Return a zero vector if no words are found\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Step 4: Convert each sentence to its FastText vector representation\n",
    "X_vectors = np.array([get_sentence_vector(sentence, fasttext_model, 100) for sentence in X])\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectors, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 6: Logistic regression model (for multiclass classification)\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predicting and evaluating\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Output the accuracy\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 8 (Optional): Implement Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X_vectors, y, cv=kf, scoring=make_scorer(accuracy_score))\n",
    "\n",
    "# Output cross-validation results\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Average Cross-Validation Accuracy: {np.mean(cv_scores) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText Embeddings + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hyinki/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/Users/hyinki/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 81.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyinki/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hyinki/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hyinki/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hyinki/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/Users/hyinki/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.81380208 0.82682292 0.81467014 0.81849761 0.7915762 ]\n",
      "Average Cross-Validation Accuracy: 81.31%\n"
     ]
    }
   ],
   "source": [
    "# Ensure you have the required NLTK package\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define X (features) and y (target) based on multiclass sentiment\n",
    "X = data['processed_full_review']  # Review text\n",
    "y = data['sentiment']\n",
    "\n",
    "# Step 1: Tokenize the sentences\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence.lower()) for sentence in X]\n",
    "\n",
    "# Step 2: Train the FastText model\n",
    "fasttext_model = FastText(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 3: Function to average word vectors for each sentence\n",
    "def get_sentence_vector(sentence, model, vector_size):\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)  # Return a zero vector if no words are found\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Step 4: Convert each sentence to its FastText vector representation\n",
    "X_vectors = np.array([get_sentence_vector(sentence, fasttext_model, 100) for sentence in X])\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectors, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 6: SVM model (for multiclass classification)\n",
    "model = SVC(kernel='linear', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predicting and evaluating\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Output the accuracy\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 8 (Optional): Implement Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X_vectors, y, cv=kf, scoring=make_scorer(accuracy_score))\n",
    "\n",
    "# Output cross-validation results\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Average Cross-Validation Accuracy: {np.mean(cv_scores) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText Embeddings + random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hyinki/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 81.92%\n",
      "Cross-Validation Scores: [0.82378472 0.82248264 0.81770833 0.81415545 0.80677377]\n",
      "Average Cross-Validation Accuracy: 81.70%\n"
     ]
    }
   ],
   "source": [
    "# Ensure you have the required NLTK package\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define X (features) and y (target) based on multiclass sentiment\n",
    "X = data['processed_full_review']  # Review text\n",
    "y = data['sentiment']\n",
    "\n",
    "# Step 1: Tokenize the sentences\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence.lower()) for sentence in X]\n",
    "\n",
    "# Step 2: Train the FastText model\n",
    "fasttext_model = FastText(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 3: Function to average word vectors for each sentence\n",
    "def get_sentence_vector(sentence, model, vector_size):\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)  # Return a zero vector if no words are found\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Step 4: Convert each sentence to its FastText vector representation\n",
    "X_vectors = np.array([get_sentence_vector(sentence, fasttext_model, 100) for sentence in X])\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectors, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 6: Random Forest model (for multiclass classification)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predicting and evaluating\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Output the accuracy\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 8 (Optional): Implement Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X_vectors, y, cv=kf, scoring=make_scorer(accuracy_score))\n",
    "\n",
    "# Output cross-validation results\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Average Cross-Validation Accuracy: {np.mean(cv_scores) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
