{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Singapore Airlines' Service Through Automated Sentiment Analysis of Customer Reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singapore Airlines Customer Reviews Dataset Information\n",
    "\n",
    "The [Singapore Airlines Customer Reviews Dataset](https://www.kaggle.com/datasets/kanchana1990/singapore-airlines-reviews) aggregates 10,000 anonymized customer reviews, providing a broad perspective on the passenger experience with Singapore Airlines. \n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`published_date`**: Date and time of review publication.\n",
    "- **`published_platform`**: Platform where the review was posted.\n",
    "- **`rating`**: Customer satisfaction rating, from 1 (lowest) to 5 (highest).\n",
    "- **`type`**: Specifies the content as a review.\n",
    "- **`text`**: Detailed customer feedback.\n",
    "- **`title`**: Summary of the review.\n",
    "- **`helpful_votes`**: Number of users finding the review helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional web scraping of online reviews\n",
    "\n",
    "During our EDA, we noticed two main trends in the distribution of our dataset:\n",
    "1. Less than 10% of our reviews were published from the years 2022 to 2024, making it hard for us to capture recent trends in sentiment.\n",
    "2. Most of the reviews were highly positive, which could mean that SIA had mostly positive reviews, nevertheless we wanted to get more information on negative reviews to improve the robustness of our model.\n",
    "\n",
    "### TripAdvisor\n",
    "\n",
    "We scraped more data for airline reviews from TripAdvisor, specifically for the years 2022 to 2024. \n",
    "(https://www.tripadvisor.com.sg/Airline_Review-d8729151-Reviews-Singapore-Airlines)\n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`Year`**: Year of review publication.\n",
    "- **`Month`**: Month of review publication.\n",
    "- **`Title`**: Title of review publication.\n",
    "- **`Review Text`**: Main text content of review publication.\n",
    "- **`Rating`**: Numerical rating provided by reviewer (Scale: 1 to 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skytrax\n",
    "\n",
    "We also scraped from Skytrax, which is another data source for online reviews. \n",
    "(https://www.airlinequality.com/airline-reviews/singapore-airlines/?sortby=post_date%3ADesc&pagesize=100)\n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`Year`**: Year of review publication.\n",
    "- **`Month`**: Month of review publication.\n",
    "- **`Title`**: Title of review publication.\n",
    "- **`Review Text`**: Main text content of review publication.\n",
    "- **`Rating`**: Numerical rating provided by reviewer (Scale: 1 to 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Please uncomment the code box below to pip install relevant dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# For concurrency (running functions in parallel)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# For caching (to speed up repeated function calls)\n",
    "from functools import lru_cache\n",
    "\n",
    "# For progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting and Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Language Detection packages\n",
    "# `langdetect` for detecting language\n",
    "from langdetect import detect as langdetect_detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "# `langid` for an alternative language detection method\n",
    "from langid import classify as langid_classify\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Regular expressions for text pattern matching\n",
    "import re\n",
    "\n",
    "# Word Cloud generation\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# For generating n-grams\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading punkt_tab: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: unable to get local issuer certificate\n",
      "[nltk_data]     (_ssl.c:1000)>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger_eng: <urlopen\n",
      "[nltk_data]     error [SSL: CERTIFICATE_VERIFY_FAILED] certificate\n",
      "[nltk_data]     verify failed: unable to get local issuer certificate\n",
      "[nltk_data]     (_ssl.c:1000)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## for Mac users, might have to install this manually\n",
    "\n",
    "# Ensure require NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation (Loading CSV)\n",
    "\n",
    "Load the final CSV file into a dataframe named `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_full_review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upper deck seat extremely comfortable service ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>currently stick plane cgk sin flight suppose d...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>travel via changi airport uk australia flight ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>take many singapore airline flight especially ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hour morning flight but crew manage great brea...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               processed_full_review sentiment  year  month\n",
       "0  upper deck seat extremely comfortable service ...  Positive  2018     10\n",
       "1  currently stick plane cgk sin flight suppose d...  Negative  2018     11\n",
       "2  travel via changi airport uk australia flight ...  Positive  2018     11\n",
       "3  take many singapore airline flight especially ...  Positive  2018     11\n",
       "4  hour morning flight but crew manage great brea...  Positive  2018     11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2019    5183\n",
       "2018    2613\n",
       "2022    1221\n",
       "2023    1157\n",
       "2020     902\n",
       "2024     511\n",
       "2021      94\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashing Vectorization\n",
    "We applied several text preprocessing techniques to prepare the dataset:\n",
    "- TF-IDF with N-grams: Captures word combinations (bigrams/trigrams) to represent more complex patterns in text.\n",
    "\n",
    "- Hashing Vectorizer: A memory-efficient method to represent text as a fixed-size sparse vector.\n",
    "\n",
    "- Latent Semantic Analysis (LSA): Used singular value decomposition (SVD) to reduce the dimensionality of the TF-IDF matrix while capturing important features\n",
    "\n",
    "- We combined these text features (from TF-IDF, Hashing, and LSA) with other numeric features like year and month to create the final feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able fly</th>\n",
       "      <th>able get</th>\n",
       "      <th>able sleep</th>\n",
       "      <th>absolutely fantastic</th>\n",
       "      <th>absolutely no</th>\n",
       "      <th>access lounge</th>\n",
       "      <th>actual flight</th>\n",
       "      <th>additional cost</th>\n",
       "      <th>address name</th>\n",
       "      <th>adelaide singapore</th>\n",
       "      <th>...</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020396</td>\n",
       "      <td>-0.014576</td>\n",
       "      <td>0.009496</td>\n",
       "      <td>-0.035269</td>\n",
       "      <td>-0.009489</td>\n",
       "      <td>-0.023210</td>\n",
       "      <td>-0.021754</td>\n",
       "      <td>0.014295</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.006666</td>\n",
       "      <td>-0.002058</td>\n",
       "      <td>-0.000754</td>\n",
       "      <td>0.008554</td>\n",
       "      <td>-0.010652</td>\n",
       "      <td>-0.005584</td>\n",
       "      <td>-0.013823</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028956</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>0.012730</td>\n",
       "      <td>0.004910</td>\n",
       "      <td>-0.001690</td>\n",
       "      <td>0.017463</td>\n",
       "      <td>-0.008606</td>\n",
       "      <td>-0.002797</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016996</td>\n",
       "      <td>-0.025058</td>\n",
       "      <td>-0.011830</td>\n",
       "      <td>-0.012667</td>\n",
       "      <td>0.015872</td>\n",
       "      <td>-0.028323</td>\n",
       "      <td>-0.003796</td>\n",
       "      <td>0.004330</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025894</td>\n",
       "      <td>0.017449</td>\n",
       "      <td>-0.052794</td>\n",
       "      <td>-0.019275</td>\n",
       "      <td>-0.038557</td>\n",
       "      <td>-0.074113</td>\n",
       "      <td>-0.002253</td>\n",
       "      <td>0.019201</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able fly  able get  able sleep  absolutely fantastic  absolutely no  \\\n",
       "0       0.0       0.0         0.0                   0.0            0.0   \n",
       "1       0.0       0.0         0.0                   0.0            0.0   \n",
       "2       0.0       0.0         0.0                   0.0            0.0   \n",
       "3       0.0       0.0         0.0                   0.0            0.0   \n",
       "4       0.0       0.0         0.0                   0.0            0.0   \n",
       "\n",
       "   access lounge  actual flight  additional cost  address name  \\\n",
       "0            0.0            0.0              0.0           0.0   \n",
       "1            0.0            0.0              0.0           0.0   \n",
       "2            0.0            0.0              0.0           0.0   \n",
       "3            0.0            0.0              0.0           0.0   \n",
       "4            0.0            0.0              0.0           0.0   \n",
       "\n",
       "   adelaide singapore  ...        92        93        94        95        96  \\\n",
       "0                 0.0  ...  0.020396 -0.014576  0.009496 -0.035269 -0.009489   \n",
       "1                 0.0  ...  0.002001  0.006666 -0.002058 -0.000754  0.008554   \n",
       "2                 0.0  ...  0.028956  0.003982  0.012730  0.004910 -0.001690   \n",
       "3                 0.0  ... -0.016996 -0.025058 -0.011830 -0.012667  0.015872   \n",
       "4                 0.0  ... -0.025894  0.017449 -0.052794 -0.019275 -0.038557   \n",
       "\n",
       "         97        98        99  year  month  \n",
       "0 -0.023210 -0.021754  0.014295  2018     10  \n",
       "1 -0.010652 -0.005584 -0.013823  2018     11  \n",
       "2  0.017463 -0.008606 -0.002797  2018     11  \n",
       "3 -0.028323 -0.003796  0.004330  2018     11  \n",
       "4 -0.074113 -0.002253  0.019201  2018     11  \n",
       "\n",
       "[5 rows x 5102 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Assuming your data is in a dataframe called 'df'\n",
    "# The text column is 'processed_full_review', and the target column is 'sentiment'\n",
    "\n",
    "# Step 1: TF-IDF with Bigrams/Trigrams\n",
    "tfidf_ngram_vectorizer = TfidfVectorizer(ngram_range=(2, 3), max_features=5000)\n",
    "X_tfidf_ngram = tfidf_ngram_vectorizer.fit_transform(data['processed_full_review'])\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame for readability\n",
    "tfidf_ngram_df = pd.DataFrame(X_tfidf_ngram.toarray(), columns=tfidf_ngram_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Step 2: Hashing Vectorizer\n",
    "hash_vectorizer = HashingVectorizer(n_features=5000, alternate_sign=False)\n",
    "X_hash = hash_vectorizer.fit_transform(data['processed_full_review'])\n",
    "\n",
    "# Step 3: LSA (Latent Semantic Analysis) via SVD\n",
    "# Applying SVD on the TF-IDF matrix for dimensionality reduction\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "X_lsa = svd.fit_transform(X_tfidf_ngram)\n",
    "\n",
    "# Combine features if needed\n",
    "X_combined = pd.concat([tfidf_ngram_df, pd.DataFrame(X_lsa)], axis=1)\n",
    "\n",
    "# Step 4: Select Other Columns (Year, Month, etc.)\n",
    "df_selected = data[['year', 'month']]  # Modify based on the features you want to select\n",
    "X_combined = pd.concat([X_combined, df_selected], axis=1)\n",
    "\n",
    "# Output the final dataframe for inspection\n",
    "X_combined.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.21%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.76      0.54      0.63       523\n",
      "     Neutral       0.67      0.01      0.01       264\n",
      "    Positive       0.76      0.96      0.85      1550\n",
      "\n",
      "    accuracy                           0.76      2337\n",
      "   macro avg       0.73      0.51      0.50      2337\n",
      "weighted avg       0.75      0.76      0.71      2337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming 'X_combined' is the feature matrix and 'data['sentiment']' is the target\n",
    "X_combined.columns = X_combined.columns.map(str)\n",
    "y = data['sentiment']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF with cross-validation\n",
    "\n",
    "Cross-validation is a technique used to evaluate the performance of a model by splitting the data into multiple subsets, training the model on some subsets, and validating it on others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy scores: [0.78598181 0.7811664  0.7870519  0.78063135 0.77408994]\n",
      "Mean cross-validation accuracy: 0.78\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.76      0.54      0.63       523\n",
      "     Neutral       0.67      0.01      0.01       264\n",
      "    Positive       0.76      0.96      0.85      1550\n",
      "\n",
      "    accuracy                           0.76      2337\n",
      "   macro avg       0.73      0.51      0.50      2337\n",
      "weighted avg       0.75      0.76      0.71      2337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Ensure X_combined columns are strings\n",
    "X_combined.columns = X_combined.columns.map(str)\n",
    "y = data['sentiment']\n",
    "\n",
    "# Split data into separate train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Cross-Validation on the training set only\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Cross-validation on the training set\n",
    "cv_scores = cross_val_score(rf_classifier, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Print Cross-validation results\n",
    "print(f\"Cross-validation accuracy scores: {cv_scores}\")\n",
    "print(f\"Mean cross-validation accuracy: {cv_scores.mean():.2f}\")\n",
    "\n",
    "# Step 2: Train on the full training set after cross-validation\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Evaluate on the separate test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Step 4: Classification report on test set\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF with Grid Search for Hyperparameter Tuning\n",
    "\n",
    "Instead of using default hyperparameters, we perform a Grid Search to find the best combination of hyperparameters (such as n_estimators, max_depth, and min_samples_split). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best cross-validation accuracy: 0.78\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.76      0.55      0.64       523\n",
      "     Neutral       0.75      0.01      0.02       264\n",
      "    Positive       0.77      0.97      0.85      1550\n",
      "\n",
      "    accuracy                           0.77      2337\n",
      "   macro avg       0.76      0.51      0.51      2337\n",
      "weighted avg       0.76      0.77      0.71      2337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ensure X_combined columns are strings\n",
    "X_combined.columns = X_combined.columns.map(str)\n",
    "y = data['sentiment']\n",
    "\n",
    "# Split data into separate train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Set up Grid Search\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the model with Grid Search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best cross-validation accuracy: {best_score:.2f}\")\n",
    "\n",
    "# Use the best model for predictions\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "y_pred = best_rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF with Out-of-Bag (OOB) Evaluation\n",
    "\n",
    "Random Forest has an internal method for evaluating performance called Out-of-Bag (OOB) score. It evaluates the model on samples not used during the training of individual trees, providing an internal cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Score: 0.68\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.62      0.97      0.75       178\n",
      "     Neutral       0.00      0.00      0.00        38\n",
      "    Positive       0.82      0.43      0.57       130\n",
      "\n",
      "    accuracy                           0.66       346\n",
      "   macro avg       0.48      0.47      0.44       346\n",
      "weighted avg       0.63      0.66      0.60       346\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritikabajpai/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/ritikabajpai/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/ritikabajpai/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Initialize Random Forest with OOB enabled\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the OOB score\n",
    "print(f\"OOB Score: {rf_classifier.oob_score_:.2f}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Stacking\n",
    "\n",
    "Stacking is a technique where you combine the predictions of multiple models, not just Random Forests, but any set of models, to make a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.73      0.90      0.81       178\n",
      "     Neutral       0.00      0.00      0.00        38\n",
      "    Positive       0.76      0.72      0.74       130\n",
      "\n",
      "    accuracy                           0.73       346\n",
      "   macro avg       0.49      0.54      0.51       346\n",
      "weighted avg       0.66      0.73      0.69       346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the base models\n",
    "base_estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svm', SVC(kernel='linear', probability=True, random_state=42))\n",
    "]\n",
    "\n",
    "# Define the final estimator (meta-learner)\n",
    "stacking_clf = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())\n",
    "\n",
    "# Train the stacking classifier\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7196531791907514\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.69      0.94      0.79       178\n",
      "     Neutral       0.00      0.00      0.00        38\n",
      "    Positive       0.80      0.62      0.70       130\n",
      "\n",
      "    accuracy                           0.72       346\n",
      "   macro avg       0.50      0.52      0.50       346\n",
      "weighted avg       0.65      0.72      0.67       346\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritikabajpai/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/ritikabajpai/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/ritikabajpai/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/ritikabajpai/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=42).fit(X_train, y_train)\n",
    "clf_predictions = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, clf_predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, clf_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
