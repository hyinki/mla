{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Airline Service Through Automated Sentiment Analysis of Customer Reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n",
    "Developed a a series of data preprocessing tasks, utilizing datasets from [Airlines Review Dataset](https://www.kaggle.com/datasets/juhibhojani/airline-reviews). Performed sentiment analysis and evaluated performance metrics using multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airline Customer Review Dataset Information\n",
    "\n",
    "The [Airline Customer Review Dataset](https://www.kaggle.com/datasets/juhibhojani/airline-reviews) contains customer review data for airline flights.\n",
    "\n",
    "- **Airline Name**: Name of Airline.\n",
    "- **Overall_Rating:** Rating given by the user.\n",
    "- **Review_Title:** Title of review.\n",
    "- **Review Date:** The date when review was entered (e.g., 1st January 2019).\n",
    "- **Verified:** Whether the reviewer is verified or not.\n",
    "- **Review:** Detailed review given by the user.\n",
    "- **Aircraft:** Type of aircraft.\n",
    "- **Type Of Traveller:** The type of traveller (e.g., Solo Leisure).\n",
    "- **Seat Type:** Categorical seat class type (e.g., Economy Class).\n",
    "- **Route:** Flight source and destination.\n",
    "- **Date Flown:** Month and year of flight (e.g., September 2019).\n",
    "- **Seat Comfort:** Rating out of 5.\n",
    "- **Cabin Staff Service:** Rating out of 5.\n",
    "- **Food & Beverages:** Rating out of 5.\n",
    "- **Ground Service:** Rating out of 5.\n",
    "- **Inflight Entertainment:** Rating out of 5.\n",
    "- **Wifi & Connectivity:** Rating out of 5.\n",
    "- **Value For Money:** Rating out of 5.\n",
    "- **Recommended:** Whether the flight is recommended or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Uncomment the line below to install the dependencies required for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation (Loading CSV)\n",
    "\n",
    "Load the Airline Review `csv` file into a pandas DataFrame `data_raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.info()\n",
    "print(\"Dataframe Shape: \", data_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Here we select the relevant features for sentiment analysis\n",
    "- 'Airline Name', 'Overall_Rating', 'Review_Title', 'Review Date', \n",
    "    'Recommended', 'Review', 'Type Of Traveller', 'Seat Type'\n",
    "- Create a new DataFrame (`data`) by selecting the specifc columns mentioned above from the original DataFrame `data_raw`.\n",
    "\n",
    "### Remove Duplicate Rows\n",
    "- Drop duplicate rows from the dataframe (`data`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the relevant features for sentiment analysis \n",
    "data = data_raw[[\n",
    "    'Airline Name', 'Overall_Rating', 'Review_Title', 'Review Date',\n",
    "    'Review', 'Type Of Traveller', 'Seat Type', 'Recommended'\n",
    "]]\n",
    "print(type(data))\n",
    "print(data.head())\n",
    "\n",
    "# Shape before dropping duplicates\n",
    "print(\"The old shape is: \", data.shape)\n",
    "\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Display the new dataframe shape\n",
    "print(\"The new shape is: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Outliers\n",
    "\n",
    "#### Review \n",
    "\n",
    "The `Review` column of `data`, which is of string type, may contain values with unusually long lengths, indicating the presence of outliers. We will identify the outliers using [Z-score method].\n",
    "\n",
    "1. Create a new column `review_length` in the DataFrame `data` by calculating the length of each review. (Set the value as 0 if the correponding `Review` column has NaN values.)\n",
    "2. Check the statistics of `review_length` using `describe()` method.\n",
    "3. Calculate the mean and standard deviation of the `review_length` column.\n",
    "4. Set the Z-score threshold for identifying outliers to 3.\n",
    "5. Identify outliers of the `review_length` column and set the corresponding `Review` to np.nan.\n",
    "6. Drop the `title_length` column from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review_length'] = data['Review'].apply(lambda x: len(x) if pd.notna(x) else 0)\n",
    "print(data.head(3))\n",
    "\n",
    "TL = data[\"review_length\"]\n",
    "stats_TL = TL.describe()\n",
    "print(stats_TL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_TL = TL.mean()\n",
    "# print(mean_TL)\n",
    "\n",
    "sd_TL = TL.std()\n",
    "# print(sd_TL)\n",
    "\n",
    "threshold = 3\n",
    "\n",
    "z_score = zscore(TL)\n",
    "# print(z_score)\n",
    "\n",
    "# Remove 'Review' of lengths that are greater than 3 standard deviations above the mean\n",
    "data.loc[abs(z_score) > threshold, 'Review'] = np.nan\n",
    "# print(data.head(3))\n",
    "\n",
    "data = data.drop(\"review_length\", axis=1)\n",
    "# print(data.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review_Title\n",
    "\n",
    "Similarly, the `Review_Title` column of `data` (of type `str`) may also contain values with unusually long lengths, indicating the presence of outliers.\n",
    "\n",
    "1. Create a new column `title_length` in the DataFrame `data` by calculating the length of each price value. (Set the value as 0 if the correponding `Review_Title` column has NaN values.)\n",
    "2. Check the statistics of `title_length` using `describe()` method and display its unique values.\n",
    "3. Identify the outlier values by inspecting the content in `Review_Title` corresponding to the abnormal value in `title_length` and set the corresponding value of `Review_Title` to np.nan.\n",
    "4. Drop the `title_length` column from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title_length'] = data['Review_Title'].apply(lambda x: len(x) if pd.notna(x) else 0)\n",
    "print(data.head(3))\n",
    "\n",
    "TL = data[\"title_length\"]\n",
    "stats_TL = TL.describe()\n",
    "print(stats_TL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_TL = TL.mean()\n",
    "# print(mean_TL)\n",
    "\n",
    "sd_TL = TL.std()\n",
    "# print(sd_TL)\n",
    "\n",
    "threshold = 3\n",
    "\n",
    "z_score = zscore(TL)\n",
    "# print(z_score)\n",
    "\n",
    "# Remove 'Review_Title' of lengths that are greater than 3 standard deviations above the mean\n",
    "data.loc[abs(z_score) > threshold, 'Review_Title'] = np.nan\n",
    "# print(data.head(3))\n",
    "\n",
    "data = data.drop(\"title_length\", axis=1)\n",
    "# print(data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Create new column `Full_Review`\n",
    "Since there are some rows with empty `Review_Title` and `Review`, we will concatenate both columns (`Review_Title` and `Review`) to form a new column `Full_Review`.\n",
    "1. Replace `NaN` values in `Review_Title` and `Review` with an empty string\n",
    "\n",
    "2. Strip starting and ending `\"` double inverted commas from `Review_Title`\n",
    "\n",
    "3. Combine `Review_Title` and `Review` into `Full_Review`\n",
    "\n",
    "4. Strip any leading/trailing whitespaces in `Full_Review`\n",
    "\n",
    "5. Drop `Review_Title` and `Review` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Fill NaN values in 'Review_Title' with an empty string\n",
    "data['Review_Title'] = data['Review_Title'].fillna('')\n",
    "data['Review'] = data['Review'].fillna('')\n",
    "\n",
    "# 2) Strip starting and ending `\"` double inverted commas from 'Review_Title'\n",
    "data['Review_Title'] = data['Review_Title'].str.strip('\"')\n",
    "\n",
    "# 3) Combine 'Review_Title' and 'Review' into 'Full_Review'\n",
    "data['Full_Review'] = data['Review_Title'] + \" \" + data['Review']\n",
    "\n",
    "# 4) Strip any leading/trailing whitespace\n",
    "data['Full_Review'] = data['Full_Review'].str.strip()\n",
    "\n",
    "# 5) Drop `Review_Title` and `Review` columns\n",
    "# data = data.drop(columns = ['Review_Title', 'Review'])\n",
    "\n",
    "# Check if the 'Full_Review' column was added correctly and whether 'Review_Title' and 'Review' columns has been dropped\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data[\"Overall_Rating\"] == \"1\") & (data[\"Recommended\"]==\"yes\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Overall_Rating to numeric \n",
    "unique_ratings = data['Overall_Rating'].unique()\n",
    "print(unique_ratings)\n",
    "\n",
    "# Step 2: Convert 'Overall_Rating' to numeric and handle non-numeric values (errors='coerce' converts non-numeric values to NaN)\n",
    "data['Overall_Rating'] = pd.to_numeric(data['Overall_Rating'], errors='coerce')\n",
    "\n",
    "# Check how many missing values were introduced in 'Overall_Rating'\n",
    "data['Overall_Rating'].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing 'Overall_Rating' values\n",
    "data = df_cleaned.dropna(subset=['Overall_Rating'])\n",
    "\n",
    "# Display the shape and info of the cleaned dataframe\n",
    "print(df_cleaned.shape)\n",
    "print(df_cleaned.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display first few rows of the df_cleaned\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode 'Recommended' as binary values\n",
    "df_cleaned['Recommended'] = df_cleaned['Recommended'].apply(lambda x: 1 if x.lower() == 'yes' else 0)\n",
    "\n",
    "# Handle missing values in 'Type Of Traveller' and 'Seat Type' by filling with 'Unknown'\n",
    "df_cleaned['Type Of Traveller'].fillna('Unknown', inplace=True)\n",
    "df_cleaned['Seat Type'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Display the final cleaned dataframe information and first few rows\n",
    "df_cleaned_info_final = df_cleaned.info()\n",
    "df_cleaned_head_final = df_cleaned.head()\n",
    "\n",
    "df_cleaned_info_final, df_cleaned_head_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#remove duplicates \n",
    "df_cleaned = df_cleaned.drop_duplicates()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.shape\n",
    "df_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of the \"Overall Rating\" dependent variable\n",
    "sns.countplot(x='Overall_Rating', data=df_cleaned)\n",
    "plt.title('Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percentage distribution of \"Overall Rating\"\n",
    "class_distribution_percentage = df_cleaned['Overall_Rating'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(class_distribution_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean text for \"Review\" column of df_cleaned\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to \"Review\"\n",
    "df_cleaned['Cleaned_Review'] = df_cleaned['Review'].apply(preprocess_text)\n",
    "df_cleaned[\"Cleaned_Review\"].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Text to Sequences\n",
    "\n",
    "#for an RNN, text data needs to be converted into numerical form\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer with a limit on vocabulary size\n",
    "max_words = 5000\n",
    "vectorizer = TfidfVectorizer(max_features=max_words)\n",
    "\n",
    "# Fit and transform the text data into numerical sequences\n",
    "sequences = vectorizer.fit_transform(df_cleaned['Cleaned_Review'])\n",
    "\n",
    "# Convert to array (if needed)\n",
    "sequences_array = sequences.toarray()\n",
    "\n",
    "# Check the shape of the output\n",
    "print(sequences_array.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Distribution of Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['review_length'] = df_cleaned['Cleaned_Review'].apply(lambda x: len(x.split()))\n",
    "df_cleaned['review_length'].hist(bins=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(df_cleaned['review_length'], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Common Words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_words = ' '.join([text for text in df_cleaned['Cleaned_Review']])\n",
    "word_counts = Counter(all_words.split())\n",
    "common_words = word_counts.most_common(20)\n",
    "\n",
    "print(common_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Cloud Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Generating word cloud from cleaned reviews\n",
    "text = ' '.join(df_cleaned['Cleaned_Review'].tolist())\n",
    "wordcloud = WordCloud(width=800, height=400, max_words=100).generate(text)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a count vectorizer for most common bigrams(Phrases of 2 words)\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), max_features=19)\n",
    "\n",
    "# Fit and transform the cleaned review data\n",
    "bigrams = vectorizer.fit_transform(df_cleaned['Cleaned_Review'])\n",
    "\n",
    "# Geting the bigram frequencies\n",
    "bigram_frequencies = pd.DataFrame(bigrams.toarray(), columns=vectorizer.get_feature_names_out()).sum().sort_values(ascending=False)\n",
    "\n",
    "print(bigram_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Sentiment Polarity Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Calculate polarity\n",
    "# Take note that this current polarity is calculated using the TextBlob library\n",
    "\n",
    "df_cleaned['polarity'] = df_cleaned['Cleaned_Review'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Plot the polarity distribution\n",
    "sns.histplot(df_cleaned['polarity'], bins=30)\n",
    "plt.title('Sentiment Polarity Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After additional columns added to df_cleaned, this is how it looks like now\n",
    "df_cleaned.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With only 1 numerical independent variable, the correlation matrix is as follows\n",
    "sns.heatmap(df_cleaned.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairplot of Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
