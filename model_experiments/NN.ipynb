{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Singapore Airlines' Service Through Automated Sentiment Analysis of Customer Reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singapore Airlines Customer Reviews Dataset Information\n",
    "\n",
    "The [Singapore Airlines Customer Reviews Dataset](https://www.kaggle.com/datasets/kanchana1990/singapore-airlines-reviews) aggregates 10,000 anonymized customer reviews, providing a broad perspective on the passenger experience with Singapore Airlines. \n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`published_date`**: Date and time of review publication.\n",
    "- **`published_platform`**: Platform where the review was posted.\n",
    "- **`rating`**: Customer satisfaction rating, from 1 (lowest) to 5 (highest).\n",
    "- **`type`**: Specifies the content as a review.\n",
    "- **`text`**: Detailed customer feedback.\n",
    "- **`title`**: Summary of the review.\n",
    "- **`helpful_votes`**: Number of users finding the review helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional web scraping of online reviews\n",
    "\n",
    "During our EDA, we noticed two main trends in the distribution of our dataset:\n",
    "1. Less than 10% of our reviews were published from the years 2022 to 2024, making it hard for us to capture recent trends in sentiment.\n",
    "2. Most of the reviews were highly positive, which could mean that SIA had mostly positive reviews, nevertheless we wanted to get more information on negative reviews to improve the robustness of our model.\n",
    "\n",
    "### TripAdvisor\n",
    "\n",
    "We scraped more data for airline reviews from TripAdvisor, specifically for the years 2022 to 2024. \n",
    "(https://www.tripadvisor.com.sg/Airline_Review-d8729151-Reviews-Singapore-Airlines)\n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`Year`**: Year of review publication.\n",
    "- **`Month`**: Month of review publication.\n",
    "- **`Title`**: Title of review publication.\n",
    "- **`Review Text`**: Main text content of review publication.\n",
    "- **`Rating`**: Numerical rating provided by reviewer (Scale: 1 to 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skytrax\n",
    "\n",
    "We also scraped from Skytrax, which is another data source for online reviews. \n",
    "(https://www.airlinequality.com/airline-reviews/singapore-airlines/?sortby=post_date%3ADesc&pagesize=100)\n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`Year`**: Year of review publication.\n",
    "- **`Month`**: Month of review publication.\n",
    "- **`Title`**: Title of review publication.\n",
    "- **`Review Text`**: Main text content of review publication.\n",
    "- **`Rating`**: Numerical rating provided by reviewer (Scale: 1 to 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Please uncomment the code box below to pip install relevant dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=2.0.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.11.1)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (4.65.0)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (3.7.2)\n",
      "Requirement already satisfied: seaborn>=0.12.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (0.12.2)\n",
      "Requirement already satisfied: langdetect>=1.0.9 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.0.9)\n",
      "Requirement already satisfied: langid>=1.1.6 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (1.1.6)\n",
      "Requirement already satisfied: nltk>=3.8.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (3.8.1)\n",
      "Requirement already satisfied: wordcloud>=1.9.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (1.9.3)\n",
      "Requirement already satisfied: tensorflow>=2.17.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (2.18.0)\n",
      "Requirement already satisfied: scikeras>=0.10.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: six in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from langdetect>=1.0.9->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (2022.7.9)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (68.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.37.1)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from scikeras>=0.10.0->-r requirements.txt (line 15)) (1.5.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.38.4)\n",
      "Requirement already satisfied: rich in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (13.9.2)\n",
      "Requirement already satisfied: namex in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2023.7.22)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.4.2->scikeras>=0.10.0->-r requirements.txt (line 15)) (3.5.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "import nltk\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# For generating n-grams\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation (Loading CSV)\n",
    "\n",
    "Load the three CSV files into a pandas DataFrame `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11518 entries, 0 to 11517\n",
      "Data columns (total 4 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   year                   11518 non-null  int64 \n",
      " 1   month                  11518 non-null  int64 \n",
      " 2   sentiment              11518 non-null  object\n",
      " 3   processed_full_review  11518 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 360.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Positive    7913\n",
       "Negative    2441\n",
       "Neutral     1164\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2019    5129\n",
       "2018    2596\n",
       "2022    1184\n",
       "2023    1111\n",
       "2020     888\n",
       "2024     514\n",
       "2021      96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network\n",
    "\n",
    "A Simple Neural Network, or fully connected neural network (FCNN), is a basic deep learning model ideal for straightforward classification tasks. It consists mainly of fully connected layers that process flattened data inputs, making it versatile for many types of data, including text.\n",
    "\n",
    "Below is an explanation of how a simple NN works:\n",
    "\n",
    "1. Embedding Layer (for Text Data):\n",
    "\t- For text inputs, an embedding layer transforms words into numerical vectors that capture meaning and context.\n",
    "    \n",
    "2.\tFlattening:\n",
    "\t- The embeddings are flattened into a single long vector, allowing the network to process them as one input.\n",
    "\n",
    "3.\tDense (Fully Connected) Layers:\n",
    "\t- Dense layers are the core of an FCNN. Each neuron connects to all neurons in the previous layer, learning complex relationships.\n",
    "\t- Activation functions, such as ReLU, are applied here to introduce non-linearity, helping the network capture more intricate patterns.\n",
    "\n",
    "4.\tOutput Layer:\n",
    "\t- The final layer outputs class probabilities using a softmax activation (for multi-class classification) or sigmoid (for binary classification).\n",
    "\t- This layer helps the model predict the likelihood of each class for an input.\n",
    "\t\n",
    "5.\tTraining:\n",
    "\t- During training, the network adjusts its weights to minimize prediction errors, gradually improving its accuracy through backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 1...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 1.4403 - accuracy: 0.6465 - val_loss: 0.9456 - val_accuracy: 0.6871\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8327 - accuracy: 0.7884 - val_loss: 0.6883 - val_accuracy: 0.8338\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.6466 - accuracy: 0.8534 - val_loss: 0.6297 - val_accuracy: 0.8498\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.5663 - accuracy: 0.8663 - val_loss: 0.5840 - val_accuracy: 0.8438\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.5045 - accuracy: 0.8770 - val_loss: 0.5859 - val_accuracy: 0.8494\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.4659 - accuracy: 0.8843 - val_loss: 0.5764 - val_accuracy: 0.8520\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.4423 - accuracy: 0.8951 - val_loss: 0.5937 - val_accuracy: 0.8485\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.4021 - accuracy: 0.9021 - val_loss: 0.5976 - val_accuracy: 0.8511\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.4061 - accuracy: 0.9130 - val_loss: 0.6025 - val_accuracy: 0.8433\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 1 Accuracy: 0.8520\n",
      "Fold 1 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7133    0.8770    0.7868       488\n",
      "           1     0.4000    0.0086    0.0168       233\n",
      "           2     0.9023    0.9684    0.9342      1583\n",
      "\n",
      "    accuracy                         0.8520      2304\n",
      "   macro avg     0.6719    0.6180    0.5793      2304\n",
      "weighted avg     0.8115    0.8520    0.8102      2304\n",
      "\n",
      "\n",
      "Training fold 2...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.3676 - accuracy: 0.6562 - val_loss: 0.9256 - val_accuracy: 0.6871\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.8191 - accuracy: 0.7919 - val_loss: 0.6793 - val_accuracy: 0.8346\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.6444 - accuracy: 0.8479 - val_loss: 0.5943 - val_accuracy: 0.8464\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.5660 - accuracy: 0.8616 - val_loss: 0.5888 - val_accuracy: 0.8477\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.5131 - accuracy: 0.8718 - val_loss: 0.5585 - val_accuracy: 0.8555\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.4798 - accuracy: 0.8803 - val_loss: 0.5654 - val_accuracy: 0.8516\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.4482 - accuracy: 0.8892 - val_loss: 0.5650 - val_accuracy: 0.8542\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.4108 - accuracy: 0.9026 - val_loss: 0.5779 - val_accuracy: 0.8498\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 2 Accuracy: 0.8555\n",
      "Fold 2 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6778    0.9180    0.7798       488\n",
      "           1     0.0000    0.0000    0.0000       233\n",
      "           2     0.9270    0.9621    0.9442      1583\n",
      "\n",
      "    accuracy                         0.8555      2304\n",
      "   macro avg     0.5349    0.6267    0.5747      2304\n",
      "weighted avg     0.7804    0.8555    0.8139      2304\n",
      "\n",
      "\n",
      "Training fold 3...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 1.3745 - accuracy: 0.6500 - val_loss: 0.9320 - val_accuracy: 0.6871\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.8016 - accuracy: 0.7859 - val_loss: 0.6622 - val_accuracy: 0.8312\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.6347 - accuracy: 0.8504 - val_loss: 0.6119 - val_accuracy: 0.8351\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.5482 - accuracy: 0.8674 - val_loss: 0.5881 - val_accuracy: 0.8381\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.4965 - accuracy: 0.8735 - val_loss: 0.5685 - val_accuracy: 0.8394\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.4525 - accuracy: 0.8813 - val_loss: 0.6095 - val_accuracy: 0.8342\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.4313 - accuracy: 0.8920 - val_loss: 0.5749 - val_accuracy: 0.8372\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.3992 - accuracy: 0.9006 - val_loss: 0.5895 - val_accuracy: 0.8429\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 3 Accuracy: 0.8394\n",
      "Fold 3 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6927    0.8545    0.7651       488\n",
      "           1     0.0000    0.0000    0.0000       233\n",
      "           2     0.8913    0.9583    0.9236      1583\n",
      "\n",
      "    accuracy                         0.8394      2304\n",
      "   macro avg     0.5280    0.6043    0.5629      2304\n",
      "weighted avg     0.7591    0.8394    0.7966      2304\n",
      "\n",
      "\n",
      "Training fold 4...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.3736 - accuracy: 0.6551 - val_loss: 0.9150 - val_accuracy: 0.6917\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8181 - accuracy: 0.7980 - val_loss: 0.7029 - val_accuracy: 0.8354\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.6397 - accuracy: 0.8510 - val_loss: 0.6389 - val_accuracy: 0.8432\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.5641 - accuracy: 0.8641 - val_loss: 0.6036 - val_accuracy: 0.8424\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.4923 - accuracy: 0.8766 - val_loss: 0.6060 - val_accuracy: 0.8428\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.4621 - accuracy: 0.8828 - val_loss: 0.6097 - val_accuracy: 0.8428\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.4323 - accuracy: 0.8925 - val_loss: 0.6126 - val_accuracy: 0.8419\n",
      "72/72 [==============================] - 0s 988us/step\n",
      "Fold 4 Accuracy: 0.8424\n",
      "Fold 4 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7109    0.8668    0.7812       488\n",
      "           1     0.0000    0.0000    0.0000       233\n",
      "           2     0.8882    0.9589    0.9222      1582\n",
      "\n",
      "    accuracy                         0.8424      2303\n",
      "   macro avg     0.5330    0.6086    0.5678      2303\n",
      "weighted avg     0.7608    0.8424    0.7990      2303\n",
      "\n",
      "\n",
      "Training fold 5...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.3711 - accuracy: 0.6402 - val_loss: 0.9045 - val_accuracy: 0.7534\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8185 - accuracy: 0.8115 - val_loss: 0.6657 - val_accuracy: 0.8406\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.6400 - accuracy: 0.8542 - val_loss: 0.6055 - val_accuracy: 0.8424\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.5565 - accuracy: 0.8671 - val_loss: 0.5821 - val_accuracy: 0.8476\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.5001 - accuracy: 0.8769 - val_loss: 0.5770 - val_accuracy: 0.8459\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.4689 - accuracy: 0.8888 - val_loss: 0.5909 - val_accuracy: 0.8480\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.4318 - accuracy: 0.9014 - val_loss: 0.6294 - val_accuracy: 0.8406\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.4053 - accuracy: 0.9135 - val_loss: 0.6085 - val_accuracy: 0.8472\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 5 Accuracy: 0.8459\n",
      "Fold 5 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6937    0.9080    0.7865       489\n",
      "           1     0.0000    0.0000    0.0000       232\n",
      "           2     0.9044    0.9507    0.9270      1582\n",
      "\n",
      "    accuracy                         0.8459      2303\n",
      "   macro avg     0.5327    0.6196    0.5712      2303\n",
      "weighted avg     0.7686    0.8459    0.8038      2303\n",
      "\n",
      "\n",
      "Average Metrics across folds:\n",
      "Average Accuracy: 0.8470\n",
      "Average Precision: 0.7761\n",
      "Average Recall: 0.8470\n",
      "Average F1 Score: 0.8047\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Assuming 'data' is your DataFrame with 'processed_full_review' and 'sentiment' columns\n",
    "\n",
    "# Step 1: Tokenization and Padding\n",
    "max_words = 10000  # Maximum vocabulary size\n",
    "max_sequence_length = 300  # Maximum length of sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['processed_full_review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_full_review'])\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# One-hot encode the sentiment labels\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y = onehot_encoder.fit_transform(data[['sentiment']])\n",
    "\n",
    "# Define the Simple Neural Network Model with L2 Regularization\n",
    "def create_simple_nn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding layer\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length))\n",
    "    \n",
    "    # Flatten the embeddings to feed into dense layers\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Fully connected layers with L2 regularization\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Output layer for three-class classification using softmax\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Stratified 5-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "y_labels = np.argmax(y, axis=1)  # Convert one-hot to single class labels for stratification\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X, y_labels)):\n",
    "    print(f\"\\nTraining fold {fold + 1}...\\n\")\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Initialize and train the model\n",
    "    model = create_simple_nn()\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test), \n",
    "              callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    precision_scores.append(report[\"weighted avg\"][\"precision\"])\n",
    "    recall_scores.append(report[\"weighted avg\"][\"recall\"])\n",
    "    f1_scores.append(report[\"weighted avg\"][\"f1-score\"])\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Classification Report:\\n\", classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "# Print average scores across all folds\n",
    "print(\"\\nAverage Metrics across folds:\")\n",
    "print(f\"Average Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Average Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Average Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network Accounting for Imbalanced Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class Weights Calculation: compute_class_weight calculates class weights based on the training labels, helping to handle the imbalance by giving higher weight to underrepresented classes.\n",
    "\n",
    "2. Passing class_weight in fit: By adding class_weight=class_weights_dict in model.fit, we inform the model to apply these weights during training, making it more sensitive to the underrepresented classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 1...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 1.6912 - accuracy: 0.3579 - val_loss: 1.3541 - val_accuracy: 0.4145\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.2907 - accuracy: 0.3503 - val_loss: 1.2201 - val_accuracy: 0.6940\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.1197 - accuracy: 0.6552 - val_loss: 0.7737 - val_accuracy: 0.7930\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.9431 - accuracy: 0.7811 - val_loss: 0.7244 - val_accuracy: 0.7908\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8437 - accuracy: 0.8080 - val_loss: 0.6585 - val_accuracy: 0.8151\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7715 - accuracy: 0.8410 - val_loss: 0.6429 - val_accuracy: 0.8351\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7017 - accuracy: 0.8655 - val_loss: 0.6495 - val_accuracy: 0.8351\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.6192 - accuracy: 0.8923 - val_loss: 0.7079 - val_accuracy: 0.8108\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.5542 - accuracy: 0.9147 - val_loss: 0.7110 - val_accuracy: 0.8234\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 1 Accuracy: 0.8351\n",
      "Fold 1 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8590    0.6865    0.7631       488\n",
      "           1     0.3663    0.5880    0.4514       233\n",
      "           2     0.9429    0.9172    0.9299      1583\n",
      "\n",
      "    accuracy                         0.8351      2304\n",
      "   macro avg     0.7227    0.7306    0.7148      2304\n",
      "weighted avg     0.8668    0.8351    0.8462      2304\n",
      "\n",
      "\n",
      "Training fold 2...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.6320 - accuracy: 0.3626 - val_loss: 1.3262 - val_accuracy: 0.5877\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.2397 - accuracy: 0.5474 - val_loss: 0.8892 - val_accuracy: 0.7665\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.0034 - accuracy: 0.7669 - val_loss: 0.7149 - val_accuracy: 0.8155\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8918 - accuracy: 0.8158 - val_loss: 0.6751 - val_accuracy: 0.8312\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8097 - accuracy: 0.8477 - val_loss: 0.7856 - val_accuracy: 0.8003\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7767 - accuracy: 0.8723 - val_loss: 0.7833 - val_accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.6893 - accuracy: 0.9028 - val_loss: 0.7242 - val_accuracy: 0.8507\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 2 Accuracy: 0.8312\n",
      "Fold 2 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8329    0.6639    0.7389       488\n",
      "           1     0.3410    0.5107    0.4089       233\n",
      "           2     0.9400    0.9299    0.9349      1583\n",
      "\n",
      "    accuracy                         0.8312      2304\n",
      "   macro avg     0.7046    0.7015    0.6942      2304\n",
      "weighted avg     0.8567    0.8312    0.8402      2304\n",
      "\n",
      "\n",
      "Training fold 3...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.6011 - accuracy: 0.3566 - val_loss: 1.2986 - val_accuracy: 0.6641\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.1411 - accuracy: 0.7033 - val_loss: 0.8604 - val_accuracy: 0.7886\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.9637 - accuracy: 0.8091 - val_loss: 0.7888 - val_accuracy: 0.7956\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8676 - accuracy: 0.8397 - val_loss: 0.7597 - val_accuracy: 0.8190\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7823 - accuracy: 0.8720 - val_loss: 0.8451 - val_accuracy: 0.7743\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.6863 - accuracy: 0.8995 - val_loss: 0.8123 - val_accuracy: 0.8103\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.6324 - accuracy: 0.9226 - val_loss: 0.8431 - val_accuracy: 0.8155\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 3 Accuracy: 0.8190\n",
      "Fold 3 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7756    0.7439    0.7594       488\n",
      "           1     0.3202    0.4549    0.3759       233\n",
      "           2     0.9422    0.8958    0.9184      1583\n",
      "\n",
      "    accuracy                         0.8190      2304\n",
      "   macro avg     0.6794    0.6982    0.6846      2304\n",
      "weighted avg     0.8440    0.8190    0.8299      2304\n",
      "\n",
      "\n",
      "Training fold 4...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.6202 - accuracy: 0.3301 - val_loss: 1.3078 - val_accuracy: 0.6878\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.2598 - accuracy: 0.4906 - val_loss: 1.0461 - val_accuracy: 0.6934\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.0130 - accuracy: 0.7379 - val_loss: 0.6863 - val_accuracy: 0.8276\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8852 - accuracy: 0.8037 - val_loss: 0.6613 - val_accuracy: 0.8341\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8074 - accuracy: 0.8347 - val_loss: 0.6738 - val_accuracy: 0.8254\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7647 - accuracy: 0.8558 - val_loss: 0.6923 - val_accuracy: 0.8263\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7365 - accuracy: 0.8739 - val_loss: 0.7843 - val_accuracy: 0.8003\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 4 Accuracy: 0.8341\n",
      "Fold 4 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7677    0.7787    0.7731       488\n",
      "           1     0.3534    0.3777    0.3651       233\n",
      "           2     0.9320    0.9185    0.9252      1582\n",
      "\n",
      "    accuracy                         0.8341      2303\n",
      "   macro avg     0.6844    0.6916    0.6878      2303\n",
      "weighted avg     0.8386    0.8341    0.8363      2303\n",
      "\n",
      "\n",
      "Training fold 5...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.6555 - accuracy: 0.3380 - val_loss: 1.3548 - val_accuracy: 0.5076\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 1.2048 - accuracy: 0.6309 - val_loss: 0.9079 - val_accuracy: 0.8076\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.9874 - accuracy: 0.7888 - val_loss: 0.7640 - val_accuracy: 0.8354\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8605 - accuracy: 0.8409 - val_loss: 0.8110 - val_accuracy: 0.8215\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7570 - accuracy: 0.8752 - val_loss: 0.8325 - val_accuracy: 0.8237\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.6591 - accuracy: 0.9085 - val_loss: 0.7970 - val_accuracy: 0.8267\n",
      "72/72 [==============================] - 0s 958us/step\n",
      "Fold 5 Accuracy: 0.8354\n",
      "Fold 5 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8085    0.8200    0.8142       489\n",
      "           1     0.3478    0.4138    0.3780       232\n",
      "           2     0.9321    0.9020    0.9168      1582\n",
      "\n",
      "    accuracy                         0.8354      2303\n",
      "   macro avg     0.6961    0.7120    0.7030      2303\n",
      "weighted avg     0.8470    0.8354    0.8407      2303\n",
      "\n",
      "\n",
      "Average Metrics across folds:\n",
      "Average Accuracy: 0.8310\n",
      "Average Precision: 0.8506\n",
      "Average Recall: 0.8310\n",
      "Average F1 Score: 0.8387\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Assuming 'data' is your DataFrame with 'processed_full_review' and 'sentiment' columns\n",
    "\n",
    "# Step 1: Tokenization and Padding\n",
    "max_words = 10000  # Maximum vocabulary size\n",
    "max_sequence_length = 300  # Maximum length of sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['processed_full_review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_full_review'])\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# One-hot encode the sentiment labels\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y = onehot_encoder.fit_transform(data[['sentiment']])\n",
    "\n",
    "# Convert one-hot encoded labels to single class labels for class weight calculation\n",
    "y_labels = np.argmax(y, axis=1)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_labels), y=y_labels)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Define the Simple Neural Network Model with L2 Regularization\n",
    "def create_simple_nn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding layer\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length))\n",
    "    \n",
    "    # Flatten the embeddings to feed into dense layers\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Fully connected layers with L2 regularization\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Output layer for three-class classification using softmax\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Stratified 5-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X, y_labels)):\n",
    "    print(f\"\\nTraining fold {fold + 1}...\\n\")\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Initialize and train the model with class weights\n",
    "    model = create_simple_nn()\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test), \n",
    "              callbacks=[early_stopping], class_weight=class_weights_dict, verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    precision_scores.append(report[\"weighted avg\"][\"precision\"])\n",
    "    recall_scores.append(report[\"weighted avg\"][\"recall\"])\n",
    "    f1_scores.append(report[\"weighted avg\"][\"f1-score\"])\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Classification Report:\\n\", classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "# Print average scores across all folds\n",
    "print(\"\\nAverage Metrics across folds:\")\n",
    "print(f\"Average Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Average Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Average Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN + CountVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 1...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.4873 - accuracy: 0.5987 - val_loss: 1.0033 - val_accuracy: 0.8025\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 1.0131 - accuracy: 0.7926 - val_loss: 0.7748 - val_accuracy: 0.8372\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8773 - accuracy: 0.8222 - val_loss: 0.6975 - val_accuracy: 0.8490\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.8111 - accuracy: 0.8443 - val_loss: 0.7106 - val_accuracy: 0.8273\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.7645 - accuracy: 0.8458 - val_loss: 0.6798 - val_accuracy: 0.8424\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.7413 - accuracy: 0.8568 - val_loss: 0.6809 - val_accuracy: 0.8407\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.7262 - accuracy: 0.8521 - val_loss: 0.6660 - val_accuracy: 0.8481\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.7141 - accuracy: 0.8582 - val_loss: 0.7277 - val_accuracy: 0.8121\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.7049 - accuracy: 0.8602 - val_loss: 0.6934 - val_accuracy: 0.8307\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.6990 - accuracy: 0.8644 - val_loss: 0.7168 - val_accuracy: 0.8212\n",
      "72/72 [==============================] - 0s 2ms/step\n",
      "Fold 1 Accuracy: 0.8481\n",
      "Fold 1 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8069    0.8135    0.8102       488\n",
      "           1     0.4294    0.6137    0.5053       233\n",
      "           2     0.9561    0.8932    0.9236      1583\n",
      "\n",
      "    accuracy                         0.8481      2304\n",
      "   macro avg     0.7308    0.7735    0.7464      2304\n",
      "weighted avg     0.8712    0.8481    0.8573      2304\n",
      "\n",
      "\n",
      "Training fold 2...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 1.5077 - accuracy: 0.5887 - val_loss: 1.0750 - val_accuracy: 0.8164\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 1.0330 - accuracy: 0.7960 - val_loss: 0.8552 - val_accuracy: 0.8320\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8847 - accuracy: 0.8309 - val_loss: 0.7483 - val_accuracy: 0.8381\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8151 - accuracy: 0.8412 - val_loss: 0.6899 - val_accuracy: 0.8529\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.7569 - accuracy: 0.8534 - val_loss: 0.7158 - val_accuracy: 0.8312\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.7443 - accuracy: 0.8484 - val_loss: 0.6914 - val_accuracy: 0.8468\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.7160 - accuracy: 0.8618 - val_loss: 0.7127 - val_accuracy: 0.8372\n",
      "72/72 [==============================] - 0s 2ms/step\n",
      "Fold 2 Accuracy: 0.8529\n",
      "Fold 2 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8422    0.7766    0.8081       488\n",
      "           1     0.4082    0.6395    0.4983       233\n",
      "           2     0.9651    0.9078    0.9355      1583\n",
      "\n",
      "    accuracy                         0.8529      2304\n",
      "   macro avg     0.7385    0.7746    0.7473      2304\n",
      "weighted avg     0.8827    0.8529    0.8643      2304\n",
      "\n",
      "\n",
      "Training fold 3...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.4844 - accuracy: 0.5972 - val_loss: 1.0530 - val_accuracy: 0.8030\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 1.0247 - accuracy: 0.7924 - val_loss: 0.8293 - val_accuracy: 0.8346\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8805 - accuracy: 0.8298 - val_loss: 0.7420 - val_accuracy: 0.8286\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.8119 - accuracy: 0.8389 - val_loss: 0.7386 - val_accuracy: 0.8181\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7644 - accuracy: 0.8485 - val_loss: 0.7107 - val_accuracy: 0.8234\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.7506 - accuracy: 0.8494 - val_loss: 0.7206 - val_accuracy: 0.8151\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7203 - accuracy: 0.8554 - val_loss: 0.6837 - val_accuracy: 0.8281\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.7112 - accuracy: 0.8560 - val_loss: 0.7122 - val_accuracy: 0.8164\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.6985 - accuracy: 0.8592 - val_loss: 0.7232 - val_accuracy: 0.8099\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.6887 - accuracy: 0.8671 - val_loss: 0.7085 - val_accuracy: 0.8147\n",
      "72/72 [==============================] - 0s 2ms/step\n",
      "Fold 3 Accuracy: 0.8281\n",
      "Fold 3 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8258    0.7480    0.7849       488\n",
      "           1     0.3848    0.7167    0.5007       233\n",
      "           2     0.9636    0.8692    0.9140      1583\n",
      "\n",
      "    accuracy                         0.8281      2304\n",
      "   macro avg     0.7247    0.7780    0.7332      2304\n",
      "weighted avg     0.8759    0.8281    0.8449      2304\n",
      "\n",
      "\n",
      "Training fold 4...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 1.4551 - accuracy: 0.5472 - val_loss: 1.0153 - val_accuracy: 0.8324\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.9917 - accuracy: 0.7847 - val_loss: 0.7812 - val_accuracy: 0.8393\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.8599 - accuracy: 0.8258 - val_loss: 0.7293 - val_accuracy: 0.8337\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.8012 - accuracy: 0.8421 - val_loss: 0.7276 - val_accuracy: 0.8215\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.7594 - accuracy: 0.8504 - val_loss: 0.7137 - val_accuracy: 0.8259\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.7451 - accuracy: 0.8477 - val_loss: 0.7114 - val_accuracy: 0.8241\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7262 - accuracy: 0.8603 - val_loss: 0.7303 - val_accuracy: 0.8194\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.7108 - accuracy: 0.8612 - val_loss: 0.6985 - val_accuracy: 0.8302\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7001 - accuracy: 0.8630 - val_loss: 0.7015 - val_accuracy: 0.8298\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.6884 - accuracy: 0.8634 - val_loss: 0.7092 - val_accuracy: 0.8285\n",
      "72/72 [==============================] - 0s 2ms/step\n",
      "Fold 4 Accuracy: 0.8285\n",
      "Fold 4 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7758    0.8156    0.7952       488\n",
      "           1     0.3921    0.6395    0.4861       233\n",
      "           2     0.9652    0.8603    0.9098      1582\n",
      "\n",
      "    accuracy                         0.8285      2303\n",
      "   macro avg     0.7111    0.7718    0.7304      2303\n",
      "weighted avg     0.8671    0.8285    0.8426      2303\n",
      "\n",
      "\n",
      "Training fold 5...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 1.5314 - accuracy: 0.5364 - val_loss: 1.0511 - val_accuracy: 0.8311\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.0378 - accuracy: 0.7742 - val_loss: 0.8058 - val_accuracy: 0.8437\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8791 - accuracy: 0.8204 - val_loss: 0.7702 - val_accuracy: 0.8220\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8236 - accuracy: 0.8383 - val_loss: 0.6909 - val_accuracy: 0.8498\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7707 - accuracy: 0.8457 - val_loss: 0.6910 - val_accuracy: 0.8393\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7405 - accuracy: 0.8525 - val_loss: 0.7453 - val_accuracy: 0.8133\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.7185 - accuracy: 0.8556 - val_loss: 0.7035 - val_accuracy: 0.8350\n",
      "72/72 [==============================] - 0s 2ms/step\n",
      "Fold 5 Accuracy: 0.8498\n",
      "Fold 5 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8268    0.8200    0.8234       489\n",
      "           1     0.4225    0.6466    0.5111       232\n",
      "           2     0.9610    0.8887    0.9235      1582\n",
      "\n",
      "    accuracy                         0.8498      2303\n",
      "   macro avg     0.7368    0.7851    0.7527      2303\n",
      "weighted avg     0.8783    0.8498    0.8607      2303\n",
      "\n",
      "\n",
      "Average Metrics across folds:\n",
      "Average Accuracy: 0.8415\n",
      "Average Precision: 0.8750\n",
      "Average Recall: 0.8415\n",
      "Average F1 Score: 0.8540\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Step 1: Use CountVectorizer for Bag-of-Words Representation\n",
    "max_features = 10000  # Maximum vocabulary size\n",
    "vectorizer = CountVectorizer(max_features=max_features)\n",
    "X = vectorizer.fit_transform(data['processed_full_review']).toarray()  # Convert to dense array\n",
    "\n",
    "# One-hot encode the sentiment labels\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y = onehot_encoder.fit_transform(data[['sentiment']])\n",
    "\n",
    "# Convert one-hot encoded labels to single class labels for class weight calculation\n",
    "y_labels = np.argmax(y, axis=1)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_labels), y=y_labels)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Define Neural Network Model\n",
    "def create_simple_nn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Fully connected layers with L2 regularization\n",
    "    model.add(Dense(64, activation='relu', input_shape=(max_features,), kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Output layer for three-class classification using softmax\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Stratified 5-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X, y_labels)):\n",
    "    print(f\"\\nTraining fold {fold + 1}...\\n\")\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Initialize and train the model with class weights\n",
    "    model = create_simple_nn()\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test), \n",
    "              callbacks=[early_stopping], class_weight=class_weights_dict, verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    precision_scores.append(report[\"weighted avg\"][\"precision\"])\n",
    "    recall_scores.append(report[\"weighted avg\"][\"recall\"])\n",
    "    f1_scores.append(report[\"weighted avg\"][\"f1-score\"])\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Classification Report:\\n\", classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "# Print average scores across all folds\n",
    "print(\"\\nAverage Metrics across folds:\")\n",
    "print(f\"Average Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Average Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Average Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network + Word2Vec\n",
    "\n",
    "1. Use the custom Word2Vec embeddings you created for initializing an embedding layer.\n",
    "\n",
    "2. Define a simple neural network that flattens the embeddings and then feeds them into dense layers.\n",
    "\n",
    "3. Add class_weight to handle the imbalanced classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 1...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 1.7681 - accuracy: 0.5376 - val_loss: 1.0942 - val_accuracy: 0.8095\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.1913 - accuracy: 0.6612 - val_loss: 0.8273 - val_accuracy: 0.7860\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.0165 - accuracy: 0.7240 - val_loss: 0.7698 - val_accuracy: 0.7891\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.9490 - accuracy: 0.7423 - val_loss: 0.7294 - val_accuracy: 0.7938\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8343 - accuracy: 0.7834 - val_loss: 0.6787 - val_accuracy: 0.8199\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8042 - accuracy: 0.7989 - val_loss: 0.7280 - val_accuracy: 0.8069\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7555 - accuracy: 0.8194 - val_loss: 0.6747 - val_accuracy: 0.8299\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7064 - accuracy: 0.8451 - val_loss: 0.7310 - val_accuracy: 0.8160\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.6877 - accuracy: 0.8606 - val_loss: 0.7156 - val_accuracy: 0.8277\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.6493 - accuracy: 0.8590 - val_loss: 0.7221 - val_accuracy: 0.8281\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 1 Accuracy: 0.8299\n",
      "Fold 1 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7829    0.8053    0.7939       488\n",
      "           1     0.3657    0.5494    0.4391       233\n",
      "           2     0.9580    0.8787    0.9166      1583\n",
      "\n",
      "    accuracy                         0.8299      2304\n",
      "   macro avg     0.7022    0.7445    0.7166      2304\n",
      "weighted avg     0.8610    0.8299    0.8424      2304\n",
      "\n",
      "\n",
      "Training fold 2...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.6781 - accuracy: 0.4850 - val_loss: 1.2435 - val_accuracy: 0.6914\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.1702 - accuracy: 0.6491 - val_loss: 0.8000 - val_accuracy: 0.7995\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.0022 - accuracy: 0.7104 - val_loss: 0.7258 - val_accuracy: 0.7847\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.9108 - accuracy: 0.7477 - val_loss: 0.6250 - val_accuracy: 0.8377\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8353 - accuracy: 0.7725 - val_loss: 0.7873 - val_accuracy: 0.7661\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8443 - accuracy: 0.7796 - val_loss: 0.6554 - val_accuracy: 0.8325\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7675 - accuracy: 0.8076 - val_loss: 0.6682 - val_accuracy: 0.8168\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 2 Accuracy: 0.8377\n",
      "Fold 2 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8233    0.7254    0.7712       488\n",
      "           1     0.3777    0.5236    0.4388       233\n",
      "           2     0.9375    0.9185    0.9279      1583\n",
      "\n",
      "    accuracy                         0.8377      2304\n",
      "   macro avg     0.7128    0.7225    0.7127      2304\n",
      "weighted avg     0.8567    0.8377    0.8453      2304\n",
      "\n",
      "\n",
      "Training fold 3...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.7152 - accuracy: 0.5059 - val_loss: 1.1672 - val_accuracy: 0.7873\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.1766 - accuracy: 0.6835 - val_loss: 0.9445 - val_accuracy: 0.7509\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 1.0126 - accuracy: 0.7423 - val_loss: 0.7693 - val_accuracy: 0.8008\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.9256 - accuracy: 0.7812 - val_loss: 0.7292 - val_accuracy: 0.8034\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8315 - accuracy: 0.8064 - val_loss: 0.8129 - val_accuracy: 0.7669\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7935 - accuracy: 0.8255 - val_loss: 0.8153 - val_accuracy: 0.7860\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7629 - accuracy: 0.8432 - val_loss: 0.7825 - val_accuracy: 0.7951\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 3 Accuracy: 0.8034\n",
      "Fold 3 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7159    0.7643    0.7393       488\n",
      "           1     0.3333    0.4807    0.3937       233\n",
      "           2     0.9440    0.8629    0.9017      1583\n",
      "\n",
      "    accuracy                         0.8034      2304\n",
      "   macro avg     0.6644    0.7026    0.6782      2304\n",
      "weighted avg     0.8340    0.8034    0.8159      2304\n",
      "\n",
      "\n",
      "Training fold 4...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.6947 - accuracy: 0.5324 - val_loss: 1.0153 - val_accuracy: 0.7772\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.1475 - accuracy: 0.6967 - val_loss: 0.8478 - val_accuracy: 0.7759\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.0038 - accuracy: 0.7330 - val_loss: 0.7156 - val_accuracy: 0.8315\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.9087 - accuracy: 0.7626 - val_loss: 0.7390 - val_accuracy: 0.7890\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8479 - accuracy: 0.7927 - val_loss: 0.6624 - val_accuracy: 0.8333\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7915 - accuracy: 0.8178 - val_loss: 0.6944 - val_accuracy: 0.8289\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8103 - accuracy: 0.8137 - val_loss: 0.8445 - val_accuracy: 0.7616\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7502 - accuracy: 0.8382 - val_loss: 0.7358 - val_accuracy: 0.8046\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 4 Accuracy: 0.8333\n",
      "Fold 4 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7900    0.8094    0.7996       488\n",
      "           1     0.3723    0.4506    0.4078       233\n",
      "           2     0.9329    0.8970    0.9146      1582\n",
      "\n",
      "    accuracy                         0.8333      2303\n",
      "   macro avg     0.6984    0.7190    0.7073      2303\n",
      "weighted avg     0.8459    0.8333    0.8390      2303\n",
      "\n",
      "\n",
      "Training fold 5...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.7870 - accuracy: 0.4769 - val_loss: 1.2190 - val_accuracy: 0.7647\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.2537 - accuracy: 0.6597 - val_loss: 0.8434 - val_accuracy: 0.8246\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.0671 - accuracy: 0.7231 - val_loss: 0.7959 - val_accuracy: 0.8159\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.9513 - accuracy: 0.7562 - val_loss: 0.7837 - val_accuracy: 0.7746\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8676 - accuracy: 0.7884 - val_loss: 0.7082 - val_accuracy: 0.8042\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.8244 - accuracy: 0.7966 - val_loss: 0.7292 - val_accuracy: 0.8029\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8091 - accuracy: 0.8140 - val_loss: 0.7329 - val_accuracy: 0.8185\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7547 - accuracy: 0.8398 - val_loss: 0.7226 - val_accuracy: 0.8189\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 5 Accuracy: 0.8042\n",
      "Fold 5 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7936    0.7628    0.7779       489\n",
      "           1     0.3354    0.6853    0.4504       232\n",
      "           2     0.9713    0.8344    0.8977      1582\n",
      "\n",
      "    accuracy                         0.8042      2303\n",
      "   macro avg     0.7001    0.7608    0.7087      2303\n",
      "weighted avg     0.8695    0.8042    0.8272      2303\n",
      "\n",
      "\n",
      "Average Metrics across folds:\n",
      "Average Accuracy: 0.8217\n",
      "Average Precision: 0.8534\n",
      "Average Recall: 0.8217\n",
      "Average F1 Score: 0.8339\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Step 1: Train Word2Vec Model\n",
    "tokenized_reviews = [review.split() for review in data['processed_full_review']]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_reviews, vector_size=128, window=5, min_count=1, sg=1, workers=4, seed=42)\n",
    "\n",
    "# Step 2: Prepare Embedding Matrix\n",
    "max_words = 10000\n",
    "embedding_dim = 128\n",
    "\n",
    "# Create a tokenizer with the vocabulary size of max_words\n",
    "word_index = {word: i for i, word in enumerate(word2vec_model.wv.index_to_key, start=1) if i < max_words}\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words and word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "# Convert the texts to sequences based on the Word2Vec vocabulary\n",
    "sequences = [[word_index.get(word, 0) for word in review.split()] for review in data['processed_full_review']]\n",
    "X = pad_sequences(sequences, maxlen=300)\n",
    "\n",
    "# One-hot encode the sentiment labels\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y = onehot_encoder.fit_transform(data[['sentiment']])\n",
    "\n",
    "# Convert one-hot encoded labels to single class labels for class weight calculation\n",
    "y_labels = np.argmax(y, axis=1)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_labels), y=y_labels)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Define Neural Network Model with Word2Vec Embeddings\n",
    "def create_simple_nn():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, weights=[embedding_matrix],\n",
    "                        input_length=300, trainable=True)) \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Fully connected layers with L2 regularization\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Output layer for three-class classification using softmax\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Stratified 5-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X, y_labels)):\n",
    "    print(f\"\\nTraining fold {fold + 1}...\\n\")\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Initialize and train the model with class weights\n",
    "    model = create_simple_nn()\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test), \n",
    "              callbacks=[early_stopping], class_weight=class_weights_dict, verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    precision_scores.append(report[\"weighted avg\"][\"precision\"])\n",
    "    recall_scores.append(report[\"weighted avg\"][\"recall\"])\n",
    "    f1_scores.append(report[\"weighted avg\"][\"f1-score\"])\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Classification Report:\\n\", classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "# Print average scores across all folds\n",
    "print(\"\\nAverage Metrics across folds:\")\n",
    "print(f\"Average Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Average Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Average Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN + FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 1...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.7214 - accuracy: 0.5403 - val_loss: 1.1013 - val_accuracy: 0.7812\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.1916 - accuracy: 0.6651 - val_loss: 0.8281 - val_accuracy: 0.7860\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.0195 - accuracy: 0.7122 - val_loss: 0.7527 - val_accuracy: 0.7435\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.9372 - accuracy: 0.7178 - val_loss: 0.6039 - val_accuracy: 0.8203\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8550 - accuracy: 0.7354 - val_loss: 0.5823 - val_accuracy: 0.8173\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8355 - accuracy: 0.7317 - val_loss: 0.6036 - val_accuracy: 0.7869\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8187 - accuracy: 0.7351 - val_loss: 0.6157 - val_accuracy: 0.7925\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8022 - accuracy: 0.7464 - val_loss: 0.6020 - val_accuracy: 0.8177\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 1 Accuracy: 0.8173\n",
      "Fold 1 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8595    0.6516    0.7413       488\n",
      "           1     0.3440    0.6009    0.4375       233\n",
      "           2     0.9332    0.9002    0.9164      1583\n",
      "\n",
      "    accuracy                         0.8173      2304\n",
      "   macro avg     0.7122    0.7176    0.6984      2304\n",
      "weighted avg     0.8580    0.8173    0.8309      2304\n",
      "\n",
      "\n",
      "Training fold 2...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.7455 - accuracy: 0.4738 - val_loss: 1.3545 - val_accuracy: 0.5543\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.2260 - accuracy: 0.6481 - val_loss: 0.8567 - val_accuracy: 0.7843\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.0203 - accuracy: 0.7079 - val_loss: 0.7572 - val_accuracy: 0.7912\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.9310 - accuracy: 0.7402 - val_loss: 0.7202 - val_accuracy: 0.7977\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8654 - accuracy: 0.7688 - val_loss: 0.7441 - val_accuracy: 0.7635\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8490 - accuracy: 0.7681 - val_loss: 0.7328 - val_accuracy: 0.7817\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7973 - accuracy: 0.7836 - val_loss: 0.6168 - val_accuracy: 0.8368\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7506 - accuracy: 0.8064 - val_loss: 0.6633 - val_accuracy: 0.8073\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7377 - accuracy: 0.8109 - val_loss: 0.6525 - val_accuracy: 0.8181\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.6963 - accuracy: 0.8275 - val_loss: 0.6414 - val_accuracy: 0.8238\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 2 Accuracy: 0.8368\n",
      "Fold 2 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8288    0.7541    0.7897       488\n",
      "           1     0.3704    0.6009    0.4583       233\n",
      "           2     0.9582    0.8970    0.9266      1583\n",
      "\n",
      "    accuracy                         0.8368      2304\n",
      "   macro avg     0.7191    0.7507    0.7249      2304\n",
      "weighted avg     0.8713    0.8368    0.8502      2304\n",
      "\n",
      "\n",
      "Training fold 3...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.6775 - accuracy: 0.5056 - val_loss: 1.1532 - val_accuracy: 0.7812\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.1568 - accuracy: 0.6601 - val_loss: 0.9378 - val_accuracy: 0.7365\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.0030 - accuracy: 0.7083 - val_loss: 0.7231 - val_accuracy: 0.8086\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.9033 - accuracy: 0.7377 - val_loss: 0.6607 - val_accuracy: 0.8108\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8492 - accuracy: 0.7581 - val_loss: 0.7177 - val_accuracy: 0.8038\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8199 - accuracy: 0.7697 - val_loss: 0.7316 - val_accuracy: 0.7756\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7450 - accuracy: 0.8014 - val_loss: 0.6838 - val_accuracy: 0.7904\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 3 Accuracy: 0.8108\n",
      "Fold 3 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7606    0.7684    0.7645       488\n",
      "           1     0.3105    0.4077    0.3525       233\n",
      "           2     0.9289    0.8831    0.9054      1583\n",
      "\n",
      "    accuracy                         0.8108      2304\n",
      "   macro avg     0.6667    0.6864    0.6742      2304\n",
      "weighted avg     0.8307    0.8108    0.8197      2304\n",
      "\n",
      "\n",
      "Training fold 4...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 1.6705 - accuracy: 0.5482 - val_loss: 1.0699 - val_accuracy: 0.7343\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.1532 - accuracy: 0.6713 - val_loss: 0.8993 - val_accuracy: 0.7612\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 1.0059 - accuracy: 0.7227 - val_loss: 0.7140 - val_accuracy: 0.8102\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.9213 - accuracy: 0.7495 - val_loss: 0.6493 - val_accuracy: 0.8220\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8681 - accuracy: 0.7626 - val_loss: 0.7141 - val_accuracy: 0.8007\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8258 - accuracy: 0.7804 - val_loss: 0.6794 - val_accuracy: 0.8116\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7923 - accuracy: 0.8009 - val_loss: 0.7117 - val_accuracy: 0.7825\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 4 Accuracy: 0.8220\n",
      "Fold 4 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8539    0.6947    0.7661       488\n",
      "           1     0.3342    0.5408    0.4131       233\n",
      "           2     0.9339    0.9027    0.9180      1582\n",
      "\n",
      "    accuracy                         0.8220      2303\n",
      "   macro avg     0.7074    0.7127    0.6991      2303\n",
      "weighted avg     0.8563    0.8220    0.8348      2303\n",
      "\n",
      "\n",
      "Training fold 5...\n",
      "\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 1.8100 - accuracy: 0.4654 - val_loss: 1.2223 - val_accuracy: 0.7677\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 1.2548 - accuracy: 0.6193 - val_loss: 0.9303 - val_accuracy: 0.8046\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 1.0703 - accuracy: 0.6902 - val_loss: 0.8535 - val_accuracy: 0.7755\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.9430 - accuracy: 0.7297 - val_loss: 0.6886 - val_accuracy: 0.8129\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8686 - accuracy: 0.7553 - val_loss: 0.7298 - val_accuracy: 0.7660\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8855 - accuracy: 0.7479 - val_loss: 0.7273 - val_accuracy: 0.7825\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.8262 - accuracy: 0.7606 - val_loss: 0.6954 - val_accuracy: 0.7920\n",
      "72/72 [==============================] - 0s 1ms/step\n",
      "Fold 5 Accuracy: 0.8129\n",
      "Fold 5 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7629    0.8160    0.7885       489\n",
      "           1     0.3260    0.5129    0.3987       232\n",
      "           2     0.9569    0.8559    0.9036      1582\n",
      "\n",
      "    accuracy                         0.8129      2303\n",
      "   macro avg     0.6819    0.7283    0.6969      2303\n",
      "weighted avg     0.8521    0.8129    0.8283      2303\n",
      "\n",
      "\n",
      "Average Metrics across folds:\n",
      "Average Accuracy: 0.8199\n",
      "Average Precision: 0.8537\n",
      "Average Recall: 0.8199\n",
      "Average F1 Score: 0.8328\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from gensim.models import FastText\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Step 1: Train FastText Model\n",
    "tokenized_reviews = [review.split() for review in data['processed_full_review']]\n",
    "fasttext_model = FastText(sentences=tokenized_reviews, vector_size=128, window=5, min_count=1, sg=1, workers=4, seed=42)\n",
    "\n",
    "# Step 2: Prepare Embedding Matrix\n",
    "max_words = 10000\n",
    "embedding_dim = 128\n",
    "\n",
    "# Create a tokenizer with the vocabulary size of max_words\n",
    "word_index = {word: i for i, word in enumerate(fasttext_model.wv.index_to_key, start=1) if i < max_words}\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words and word in fasttext_model.wv:\n",
    "        embedding_matrix[i] = fasttext_model.wv[word]\n",
    "\n",
    "# Convert the texts to sequences based on the FastText vocabulary\n",
    "sequences = [[word_index.get(word, 0) for word in review.split()] for review in data['processed_full_review']]\n",
    "X = pad_sequences(sequences, maxlen=300)\n",
    "\n",
    "# One-hot encode the sentiment labels\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y = onehot_encoder.fit_transform(data[['sentiment']])\n",
    "\n",
    "# Convert one-hot encoded labels to single class labels for class weight calculation\n",
    "y_labels = np.argmax(y, axis=1)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_labels), y=y_labels)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Define Neural Network Model with FastText Embeddings\n",
    "def create_simple_nn():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, weights=[embedding_matrix],\n",
    "                        input_length=300, trainable=True)) \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Fully connected layers with L2 regularization\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Output layer for three-class classification using softmax\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Stratified 5-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X, y_labels)):\n",
    "    print(f\"\\nTraining fold {fold + 1}...\\n\")\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Initialize and train the model with class weights\n",
    "    model = create_simple_nn()\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test), \n",
    "              callbacks=[early_stopping], class_weight=class_weights_dict, verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    precision_scores.append(report[\"weighted avg\"][\"precision\"])\n",
    "    recall_scores.append(report[\"weighted avg\"][\"recall\"])\n",
    "    f1_scores.append(report[\"weighted avg\"][\"f1-score\"])\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Classification Report:\\n\", classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "\n",
    "# Print average scores across all folds\n",
    "print(\"\\nAverage Metrics across folds:\")\n",
    "print(f\"Average Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Average Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Average Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
