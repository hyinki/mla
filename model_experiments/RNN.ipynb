{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=2.0.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.24.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 2)) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 3)) (1.12.0)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 4)) (4.66.5)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 5)) (3.8.2)\n",
      "Requirement already satisfied: seaborn>=0.12.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
      "Requirement already satisfied: langdetect>=1.0.9 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 7)) (1.0.9)\n",
      "Requirement already satisfied: langid>=1.1.6 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 8)) (1.1.6)\n",
      "Requirement already satisfied: nltk>=3.8.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 9)) (3.9.1)\n",
      "Requirement already satisfied: wordcloud>=1.9.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 10)) (1.9.3)\n",
      "Collecting tensorflow>=2.17.1 (from -r requirements.txt (line 14))\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: scikeras>=0.10.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 15)) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2023.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.65.0->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (3.1.1)\n",
      "Requirement already satisfied: six in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langdetect>=1.0.9->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (2024.7.24)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow>=2.17.1->-r requirements.txt (line 14))\n",
      "  Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (74.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (1.66.1)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14))\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.5.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.4.0)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikeras>=0.10.0->-r requirements.txt (line 15)) (1.5.2)\n",
      "Requirement already satisfied: rich in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (13.8.0)\n",
      "Requirement already satisfied: namex in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.12.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras>=0.10.0->-r requirements.txt (line 15)) (3.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.44.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2023.11.17)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.0.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\redbu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow>=2.17.1->-r requirements.txt (line 14)) (2.1.4)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl (390.3 MB)\n",
      "   ---------------------------------------- 0.0/390.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.2/390.3 MB 22.9 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 10.0/390.3 MB 24.8 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 15.5/390.3 MB 25.6 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 21.0/390.3 MB 26.0 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 27.0/390.3 MB 26.7 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 32.8/390.3 MB 26.7 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 34.3/390.3 MB 26.6 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 34.3/390.3 MB 26.6 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 37.7/390.3 MB 20.0 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 40.4/390.3 MB 20.4 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 40.4/390.3 MB 20.4 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 40.4/390.3 MB 20.4 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 40.4/390.3 MB 20.4 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 45.1/390.3 MB 15.3 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 49.8/390.3 MB 16.1 MB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 49.8/390.3 MB 16.1 MB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 49.8/390.3 MB 16.1 MB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 50.3/390.3 MB 13.3 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 56.6/390.3 MB 14.2 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 59.0/390.3 MB 14.5 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 59.0/390.3 MB 14.5 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 59.0/390.3 MB 14.5 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 60.6/390.3 MB 12.5 MB/s eta 0:00:27\n",
      "   ------ --------------------------------- 66.1/390.3 MB 13.1 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 71.8/390.3 MB 13.7 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 72.6/390.3 MB 13.7 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 72.6/390.3 MB 13.7 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 76.5/390.3 MB 13.0 MB/s eta 0:00:25\n",
      "   -------- ------------------------------- 82.3/390.3 MB 13.5 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 82.3/390.3 MB 13.5 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 82.3/390.3 MB 13.5 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 82.3/390.3 MB 13.5 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 82.6/390.3 MB 12.2 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 82.8/390.3 MB 11.9 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 86.2/390.3 MB 11.7 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 91.0/390.3 MB 12.0 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 95.9/390.3 MB 12.3 MB/s eta 0:00:24\n",
      "   ---------- ---------------------------- 101.7/390.3 MB 12.7 MB/s eta 0:00:23\n",
      "   ---------- ---------------------------- 106.4/390.3 MB 13.0 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 109.8/390.3 MB 13.2 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 109.8/390.3 MB 13.2 MB/s eta 0:00:22\n",
      "   ----------- --------------------------- 111.4/390.3 MB 12.6 MB/s eta 0:00:23\n",
      "   ----------- --------------------------- 117.2/390.3 MB 13.0 MB/s eta 0:00:22\n",
      "   ------------ -------------------------- 123.2/390.3 MB 13.3 MB/s eta 0:00:21\n",
      "   ------------ -------------------------- 124.3/390.3 MB 13.4 MB/s eta 0:00:20\n",
      "   ------------ -------------------------- 124.3/390.3 MB 13.4 MB/s eta 0:00:20\n",
      "   ------------ -------------------------- 124.8/390.3 MB 12.8 MB/s eta 0:00:21\n",
      "   ------------ -------------------------- 125.6/390.3 MB 12.4 MB/s eta 0:00:22\n",
      "   ------------ -------------------------- 126.9/390.3 MB 12.3 MB/s eta 0:00:22\n",
      "   ------------ -------------------------- 129.0/390.3 MB 12.3 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 131.6/390.3 MB 12.3 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 134.7/390.3 MB 12.3 MB/s eta 0:00:21\n",
      "   ------------- ------------------------- 137.9/390.3 MB 12.4 MB/s eta 0:00:21\n",
      "   ------------- ------------------------- 137.9/390.3 MB 12.4 MB/s eta 0:00:21\n",
      "   ------------- ------------------------- 137.9/390.3 MB 12.4 MB/s eta 0:00:21\n",
      "   ------------- ------------------------- 138.1/390.3 MB 11.7 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 140.0/390.3 MB 11.7 MB/s eta 0:00:22\n",
      "   -------------- ------------------------ 145.0/390.3 MB 11.9 MB/s eta 0:00:21\n",
      "   --------------- ----------------------- 150.7/390.3 MB 12.1 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 151.8/390.3 MB 12.2 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 154.4/390.3 MB 12.0 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 159.6/390.3 MB 12.2 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 165.4/390.3 MB 12.5 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 166.2/390.3 MB 12.5 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 166.2/390.3 MB 12.5 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 170.4/390.3 MB 12.2 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 173.8/390.3 MB 12.4 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 173.8/390.3 MB 12.4 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 173.8/390.3 MB 12.4 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 173.8/390.3 MB 12.4 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 178.5/390.3 MB 11.9 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 184.5/390.3 MB 12.2 MB/s eta 0:00:17\n",
      "   ------------------ -------------------- 188.2/390.3 MB 12.3 MB/s eta 0:00:17\n",
      "   ------------------ -------------------- 188.2/390.3 MB 12.3 MB/s eta 0:00:17\n",
      "   ------------------ -------------------- 188.5/390.3 MB 12.0 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 192.2/390.3 MB 12.0 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 195.6/390.3 MB 12.1 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 195.6/390.3 MB 12.1 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 195.6/390.3 MB 12.1 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 197.1/390.3 MB 11.7 MB/s eta 0:00:17\n",
      "   -------------------- ------------------ 202.9/390.3 MB 11.9 MB/s eta 0:00:16\n",
      "   -------------------- ------------------ 208.9/390.3 MB 12.1 MB/s eta 0:00:15\n",
      "   -------------------- ------------------ 209.7/390.3 MB 12.1 MB/s eta 0:00:15\n",
      "   -------------------- ------------------ 209.7/390.3 MB 12.1 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 213.6/390.3 MB 11.9 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.4/390.3 MB 12.1 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.4/390.3 MB 12.1 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.4/390.3 MB 12.1 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.4/390.3 MB 12.1 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.7/390.3 MB 11.7 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.7/390.3 MB 11.7 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 219.9/390.3 MB 11.5 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 220.2/390.3 MB 9.4 MB/s eta 0:00:19\n",
      "   ---------------------- ----------------- 221.2/390.3 MB 9.4 MB/s eta 0:00:18\n",
      "   ---------------------- ----------------- 223.3/390.3 MB 9.4 MB/s eta 0:00:18\n",
      "   ----------------------- ---------------- 225.2/390.3 MB 9.4 MB/s eta 0:00:18\n",
      "   ----------------------- ---------------- 228.1/390.3 MB 9.4 MB/s eta 0:00:18\n",
      "   ----------------------- ---------------- 230.2/390.3 MB 9.5 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 230.2/390.3 MB 9.5 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 230.9/390.3 MB 9.3 MB/s eta 0:00:18\n",
      "   ------------------------ --------------- 236.2/390.3 MB 9.4 MB/s eta 0:00:17\n",
      "   ------------------------ --------------- 239.9/390.3 MB 9.5 MB/s eta 0:00:16\n",
      "   ------------------------ --------------- 239.9/390.3 MB 9.5 MB/s eta 0:00:16\n",
      "   ------------------------ --------------- 239.9/390.3 MB 9.5 MB/s eta 0:00:16\n",
      "   ------------------------ --------------- 241.7/390.3 MB 9.4 MB/s eta 0:00:16\n",
      "   ------------------------- -------------- 246.9/390.3 MB 9.5 MB/s eta 0:00:16\n",
      "   ------------------------- -------------- 249.6/390.3 MB 9.5 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 249.6/390.3 MB 9.5 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 249.6/390.3 MB 9.5 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 253.0/390.3 MB 9.4 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 255.6/390.3 MB 9.5 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 255.6/390.3 MB 9.5 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 255.6/390.3 MB 9.5 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 255.6/390.3 MB 9.5 MB/s eta 0:00:15\n",
      "   -------------------------- ------------- 260.3/390.3 MB 9.3 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 266.9/390.3 MB 9.4 MB/s eta 0:00:14\n",
      "   --------------------------- ------------ 272.9/390.3 MB 9.4 MB/s eta 0:00:13\n",
      "   ---------------------------- ----------- 278.7/390.3 MB 9.4 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 284.2/390.3 MB 9.4 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 289.7/390.3 MB 9.4 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 295.7/390.3 MB 9.4 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 298.3/390.3 MB 9.5 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 298.3/390.3 MB 9.5 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 299.4/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.1/390.3 MB 9.6 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 304.1/390.3 MB 9.6 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 304.1/390.3 MB 9.6 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 304.1/390.3 MB 9.6 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 304.3/390.3 MB 9.4 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 310.1/390.3 MB 8.7 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 314.6/390.3 MB 8.8 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 320.1/390.3 MB 8.8 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 323.7/390.3 MB 9.0 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 323.7/390.3 MB 9.0 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 323.7/390.3 MB 9.0 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 324.5/390.3 MB 8.8 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 327.9/390.3 MB 8.7 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 332.7/390.3 MB 8.7 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 338.4/390.3 MB 8.9 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 344.7/390.3 MB 9.1 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 350.7/390.3 MB 9.2 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 357.0/390.3 MB 9.2 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 358.1/390.3 MB 9.2 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 358.1/390.3 MB 9.2 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 358.1/390.3 MB 9.2 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 362.5/390.3 MB 9.0 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 368.6/390.3 MB 9.1 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 372.5/390.3 MB 9.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 372.5/390.3 MB 9.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 372.5/390.3 MB 9.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 373.3/390.3 MB 9.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 373.6/390.3 MB 8.9 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 377.2/390.3 MB 8.9 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 379.6/390.3 MB 8.9 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 379.8/390.3 MB 8.8 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 380.4/390.3 MB 8.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  382.5/390.3 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  388.2/390.3 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  390.1/390.3 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  390.1/390.3 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 390.3/390.3 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.5/5.5 MB 25.8 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.17.1\n",
      "    Uninstalling tensorboard-2.17.1:\n",
      "      Successfully uninstalled tensorboard-2.17.1\n",
      "  Attempting uninstall: tensorflow-intel\n",
      "    Found existing installation: tensorflow-intel 2.17.0\n",
      "    Uninstalling tensorflow-intel-2.17.0:\n",
      "      Successfully uninstalled tensorflow-intel-2.17.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.17.0\n",
      "    Uninstalling tensorflow-2.17.0:\n",
      "      Successfully uninstalled tensorflow-2.17.0\n",
      "Successfully installed tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-intel-2.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~ensorflow'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "import nltk\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# For generating n-grams\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation (Loading CSV)\n",
    "\n",
    "Load the three CSV files into a pandas DataFrame `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11518 entries, 0 to 11517\n",
      "Data columns (total 4 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   year                   11518 non-null  int64 \n",
      " 1   month                  11518 non-null  int64 \n",
      " 2   sentiment              11518 non-null  object\n",
      " 3   processed_full_review  11518 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 360.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Positive    7913\n",
       "Negative    2441\n",
       "Neutral     1164\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2019    5129\n",
       "2018    2596\n",
       "2022    1184\n",
       "2023    1111\n",
       "2020     888\n",
       "2024     514\n",
       "2021      96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RNN + Tokenizer Self-Trained Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 23:38:24.371102: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-09 23:38:24.424690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731166704.446436  293354 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731166704.452657  293354 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-09 23:38:24.507969: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 1...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dariusng2103/projects/mla_project/tf217/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2024-11-09 23:38:26.428189: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.4357 - loss: 1.8302 - val_accuracy: 0.3956 - val_loss: 1.6660\n",
      "Epoch 2/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.7336 - loss: 1.2637 - val_accuracy: 0.7292 - val_loss: 1.0412\n",
      "Epoch 3/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.8249 - loss: 0.9020 - val_accuracy: 0.7727 - val_loss: 0.8710\n",
      "Epoch 4/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8206 - loss: 0.8082 - val_accuracy: 0.7477 - val_loss: 0.8637\n",
      "Epoch 5/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9028 - loss: 0.6364 - val_accuracy: 0.6262 - val_loss: 0.9345\n",
      "Epoch 6/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9344 - loss: 0.4802 - val_accuracy: 0.7157 - val_loss: 0.8837\n",
      "Epoch 7/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9447 - loss: 0.4028 - val_accuracy: 0.7184 - val_loss: 0.8950\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "Fold 1 Accuracy: 0.8056\n",
      "Fold 1 F1 Score: 0.8059\n",
      "Fold 1 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.6929    0.7213    0.7068       488\n",
      "     Neutral     0.2664    0.2618    0.2641       233\n",
      "    Positive     0.9209    0.9116    0.9162      1583\n",
      "\n",
      "    accuracy                         0.8056      2304\n",
      "   macro avg     0.6267    0.6316    0.6290      2304\n",
      "weighted avg     0.8064    0.8056    0.8059      2304\n",
      "\n",
      "\n",
      "Training fold 2...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dariusng2103/projects/mla_project/tf217/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.5010 - loss: 1.7917 - val_accuracy: 0.4645 - val_loss: 1.5147\n",
      "Epoch 2/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.7643 - loss: 1.2497 - val_accuracy: 0.6180 - val_loss: 1.1050\n",
      "Epoch 3/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.8617 - loss: 0.8723 - val_accuracy: 0.6348 - val_loss: 1.0092\n",
      "Epoch 4/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9111 - loss: 0.6058 - val_accuracy: 0.6782 - val_loss: 0.9567\n",
      "Epoch 5/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9131 - loss: 0.5440 - val_accuracy: 0.6663 - val_loss: 1.0103\n",
      "Epoch 6/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9268 - loss: 0.4909 - val_accuracy: 0.6647 - val_loss: 0.9894\n",
      "Epoch 7/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9554 - loss: 0.3548 - val_accuracy: 0.6701 - val_loss: 0.9728\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "Fold 2 Accuracy: 0.7743\n",
      "Fold 2 F1 Score: 0.7878\n",
      "Fold 2 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.6658    0.5512    0.6031       488\n",
      "     Neutral     0.2440    0.3948    0.3016       233\n",
      "    Positive     0.9343    0.8989    0.9163      1583\n",
      "\n",
      "    accuracy                         0.7743      2304\n",
      "   macro avg     0.6147    0.6150    0.6070      2304\n",
      "weighted avg     0.8077    0.7743    0.7878      2304\n",
      "\n",
      "\n",
      "Training fold 3...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dariusng2103/projects/mla_project/tf217/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.5040 - loss: 1.7974 - val_accuracy: 0.5952 - val_loss: 1.3694\n",
      "Epoch 2/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.8048 - loss: 1.1338 - val_accuracy: 0.6826 - val_loss: 1.0879\n",
      "Epoch 3/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.8575 - loss: 0.8042 - val_accuracy: 0.6576 - val_loss: 1.0388\n",
      "Epoch 4/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9067 - loss: 0.6242 - val_accuracy: 0.6712 - val_loss: 1.0161\n",
      "Epoch 5/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9331 - loss: 0.4828 - val_accuracy: 0.6614 - val_loss: 0.9654\n",
      "Epoch 6/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9194 - loss: 0.4481 - val_accuracy: 0.6571 - val_loss: 1.1634\n",
      "Epoch 7/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9277 - loss: 0.4235 - val_accuracy: 0.7005 - val_loss: 1.0317\n",
      "Epoch 8/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9656 - loss: 0.3268 - val_accuracy: 0.6902 - val_loss: 1.0898\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "Fold 3 Accuracy: 0.7335\n",
      "Fold 3 F1 Score: 0.7571\n",
      "Fold 3 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.5976    0.6209    0.6090       488\n",
      "     Neutral     0.2225    0.3906    0.2835       233\n",
      "    Positive     0.9337    0.8187    0.8724      1583\n",
      "\n",
      "    accuracy                         0.7335      2304\n",
      "   macro avg     0.5846    0.6101    0.5883      2304\n",
      "weighted avg     0.7906    0.7335    0.7571      2304\n",
      "\n",
      "\n",
      "Training fold 4...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dariusng2103/projects/mla_project/tf217/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.4309 - loss: 1.8439 - val_accuracy: 0.4520 - val_loss: 1.5386\n",
      "Epoch 2/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.7868 - loss: 1.2306 - val_accuracy: 0.5741 - val_loss: 1.1742\n",
      "Epoch 3/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8850 - loss: 0.7921 - val_accuracy: 0.5410 - val_loss: 1.2520\n",
      "Epoch 4/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9015 - loss: 0.6435 - val_accuracy: 0.6750 - val_loss: 1.0003\n",
      "Epoch 5/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9182 - loss: 0.5311 - val_accuracy: 0.6858 - val_loss: 1.0287\n",
      "Epoch 6/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9261 - loss: 0.4792 - val_accuracy: 0.6804 - val_loss: 0.9915\n",
      "Epoch 7/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9145 - loss: 0.4633 - val_accuracy: 0.6972 - val_loss: 0.9435\n",
      "Epoch 8/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9713 - loss: 0.3167 - val_accuracy: 0.6766 - val_loss: 1.0689\n",
      "Epoch 9/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9809 - loss: 0.2636 - val_accuracy: 0.7271 - val_loss: 0.9354\n",
      "Epoch 10/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9867 - loss: 0.2286 - val_accuracy: 0.7070 - val_loss: 1.0469\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "Fold 4 Accuracy: 0.8220\n",
      "Fold 4 F1 Score: 0.8091\n",
      "Fold 4 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7648    0.7131    0.7381       488\n",
      "     Neutral     0.3311    0.2146    0.2604       233\n",
      "    Positive     0.8810    0.9450    0.9119      1582\n",
      "\n",
      "    accuracy                         0.8220      2303\n",
      "   macro avg     0.6590    0.6242    0.6368      2303\n",
      "weighted avg     0.8007    0.8220    0.8091      2303\n",
      "\n",
      "\n",
      "Training fold 5...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dariusng2103/projects/mla_project/tf217/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.3388 - loss: 1.8777 - val_accuracy: 0.4216 - val_loss: 1.6282\n",
      "Epoch 2/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.6652 - loss: 1.3618 - val_accuracy: 0.4590 - val_loss: 1.4496\n",
      "Epoch 3/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8044 - loss: 1.0192 - val_accuracy: 0.5768 - val_loss: 1.2406\n",
      "Epoch 4/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8914 - loss: 0.6887 - val_accuracy: 0.6462 - val_loss: 1.1402\n",
      "Epoch 5/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9398 - loss: 0.4995 - val_accuracy: 0.6571 - val_loss: 1.1858\n",
      "Epoch 6/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9609 - loss: 0.4129 - val_accuracy: 0.6658 - val_loss: 1.1771\n",
      "Epoch 7/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9502 - loss: 0.4323 - val_accuracy: 0.6902 - val_loss: 1.0578\n",
      "Epoch 8/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9313 - loss: 0.4681 - val_accuracy: 0.6706 - val_loss: 1.0873\n",
      "Epoch 9/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.9695 - loss: 0.3403 - val_accuracy: 0.6929 - val_loss: 1.0226\n",
      "Epoch 10/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9834 - loss: 0.2793 - val_accuracy: 0.6761 - val_loss: 1.0668\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "Fold 5 Accuracy: 0.7816\n",
      "Fold 5 F1 Score: 0.7799\n",
      "Fold 5 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.6740    0.6851    0.6795       489\n",
      "     Neutral     0.2535    0.2371    0.2450       232\n",
      "    Positive     0.8874    0.8913    0.8893      1582\n",
      "\n",
      "    accuracy                         0.7816      2303\n",
      "   macro avg     0.6050    0.6045    0.6046      2303\n",
      "weighted avg     0.7782    0.7816    0.7799      2303\n",
      "\n",
      "\n",
      "Average Metrics across folds:\n",
      "Average Accuracy: 0.7834\n",
      "Average F1 Score: 0.7880\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set to CPU only\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Disable GPU\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 5000         # Limit vocabulary to 5000 words\n",
    "embedding_dim = 128       # Embedding dimensions for each word\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "l2_lambda = 0.01 \n",
    "\n",
    "# Step 1: Tokenize and Pad the Text\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(data['processed_full_review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_full_review'])\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights_values = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
    "class_weights = {i: class_weights_values[i] for i in range(len(class_weights_values))}\n",
    "\n",
    "# Define stratified 5-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X_padded, y)):\n",
    "    print(f\"\\nTraining fold {fold + 1}...\\n\")\n",
    "    \n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length, trainable=True))\n",
    "    model.add(SimpleRNN(64, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model with early stopping and class weights\n",
    "    model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=10, \n",
    "        batch_size=128,  \n",
    "        validation_split=0.2, \n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "    \n",
    "    # Predictions and evaluation for the current fold\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    \n",
    "    # Calculate metrics for the current fold\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, output_dict=True)\n",
    "    f1 = report['weighted avg']['f1-score']\n",
    "    \n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Fold {fold + 1} F1 Score: {f1:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4))\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(\"\\nAverage Metrics across folds:\")\n",
    "print(f\"Average Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN + Count Vectoriser\n",
    "\n",
    "### Loss of Sequential Information\n",
    "Poor performance because RNNs are not well-suited to the bag-of-words representation generated by `CountVectorizer`. Since `CountVectorizer` treats each document as a set of words without any order, words are represented only by their counts, not by their position in the text. Since RNNs are designed to work with ordered sequences, where the position and context of words matter, without preserving word order, the RNN cannot capture dependencies between words over time.\n",
    "\n",
    "### Sparse, non-contextual input\n",
    "`CountVectorizer` produces a sparse representation where each word is treated as an independent feature based on its frequency. There is no semantic or contextual relationship between words, and the word counts lack dense, meaningful relationships that an RNN could leverage, since RNNs perform best with dense, continuous data that represents meaningful relationships between words, typically achieved with word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set to CPU only\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Disable GPU\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "max_features = 5000  # Limit vocabulary to 5000 words\n",
    "l2_lambda = 0.01 \n",
    "\n",
    "# Step 1: Vectorize Text Data using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=max_features)\n",
    "X_count = vectorizer.fit_transform(data['processed_full_review']).toarray()\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights_values = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
    "class_weights = {i: class_weights_values[i] for i in range(len(class_weights_values))}\n",
    "\n",
    "# Define stratified 5-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X_count, y)):\n",
    "    print(f\"\\nTraining fold {fold + 1}...\\n\")\n",
    "    \n",
    "    X_train, X_test = X_count[train_index], X_count[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(64, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model with early stopping and class weights\n",
    "    model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=10, \n",
    "        batch_size=128,  \n",
    "        validation_split=0.2, \n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "    \n",
    "    # Predictions and evaluation for the current fold\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    \n",
    "    # Calculate metrics for the current fold\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, output_dict=True)\n",
    "    f1 = report['weighted avg']['f1-score']\n",
    "    \n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Fold {fold + 1} F1 Score: {f1:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4))\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(\"\\nAverage Metrics across folds:\")\n",
    "print(f\"Average Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN + Count Vectoriser + Conversion to pseudo-sequences with word indices\n",
    "\n",
    "Performance is better than Basic RNN.\n",
    "\n",
    "Over here, we transform the `CountVectorizer` output into integer sequences which is compatible with the embedding layer. \n",
    "\n",
    "Why `CountVectorizer` is better here is because sentiment analysis often hinges more on the presence of certain key words rather than on the strict order of words in a sequence. Unlike other NLP tasks where the exact sequence of words matters (e.g. translation or grammar correction), sentiment analysis can often succeed with just the occurrence or frequency of these key items. `CountVectorizer` captures this by creating a bag-of-words representation that prioritises word presence and frequency, which is often enough for sentiment detection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 0.4720 - loss: 1.0589 - val_accuracy: 0.7911 - val_loss: 0.5905\n",
      "Epoch 2/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.8687 - loss: 0.5639 - val_accuracy: 0.7629 - val_loss: 0.5588\n",
      "Epoch 3/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - accuracy: 0.9391 - loss: 0.2436 - val_accuracy: 0.7466 - val_loss: 0.6518\n",
      "Epoch 4/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 46ms/step - accuracy: 0.9750 - loss: 0.0925 - val_accuracy: 0.7982 - val_loss: 0.6056\n",
      "Epoch 5/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 53ms/step - accuracy: 0.9902 - loss: 0.0385 - val_accuracy: 0.8041 - val_loss: 0.6644\n",
      "Epoch 6/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.9983 - loss: 0.0145 - val_accuracy: 0.8041 - val_loss: 0.7157\n",
      "Epoch 7/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.9998 - loss: 0.0085 - val_accuracy: 0.7927 - val_loss: 0.7630\n",
      "Epoch 8/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 52ms/step - accuracy: 0.9991 - loss: 0.0089 - val_accuracy: 0.8068 - val_loss: 0.7934\n",
      "Epoch 9/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 0.7906 - val_loss: 0.8170\n",
      "Epoch 10/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 53ms/step - accuracy: 0.9994 - loss: 0.0037 - val_accuracy: 0.8095 - val_loss: 0.8638\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "Performance Metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7283    0.6957    0.7116       470\n",
      "     Neutral     0.2732    0.2193    0.2433       228\n",
      "    Positive     0.9067    0.9440    0.9250      1606\n",
      "\n",
      "    accuracy                         0.8216      2304\n",
      "   macro avg     0.6361    0.6197    0.6266      2304\n",
      "weighted avg     0.8076    0.8216    0.8140      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "max_features = 5000       # Limit vocabulary to 5000 words\n",
    "embedding_dim = 128        # Embedding dimensions for each word\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "\n",
    "# Step 1: Text Vectorization using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=max_features)\n",
    "X_counts = vectorizer.fit_transform(data['processed_full_review'])\n",
    "word_index = vectorizer.vocabulary_\n",
    "\n",
    "# Inverse vocabulary mapping for sequences creation\n",
    "index_to_word = {i: word for word, i in word_index.items()}\n",
    "\n",
    "def counts_to_sequences(X_counts):\n",
    "    sequences = []\n",
    "    for i in range(X_counts.shape[0]):\n",
    "        indices = X_counts[i].nonzero()[1]\n",
    "        words = [index_to_word[idx] for idx in indices]\n",
    "        seq = [word_index[word] + 1 for word in words]  # +1 because 0 is reserved for padding\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "sequences = counts_to_sequences(X_counts)\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length, trainable=True))\n",
    "model.add(SimpleRNN(64, activation='tanh', input_shape=(X_train_reshaped.shape[1], 1)))  # Input shape adjusted\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(3, activation='softmax'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64,  validation_split=0.2, verbose=1, class_weight=class_weights_dict)\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate and print classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4)\n",
    "print('Performance Metrics:\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN + Within model trained Word2Vec\n",
    "\n",
    "`Word2Vec` performs worse than `CountVectorizer`.\n",
    "\n",
    "Because our dataset is only 10k rows, Word2Vec embeddings might lack the depth needed for nuanced sentiment patterns, particularly without pre-training on a larger corpus. If Word2Vec embeddings do not generalise well or have insufficient context, the RNN might not capture subtle sentiment signals in the text, which can degrade model performance. In contrast, CountVectorizer builds a fixed vocab of words based on frequency, and does not need to learn semantic relationships among words, making it robust in cases where the model vocab size is small. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 1...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dariusng2103/projects/mla_project/tf217/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.4849 - loss: 1.9380 - val_accuracy: 0.6837 - val_loss: 1.5286\n",
      "Epoch 2/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.7381 - loss: 1.3876 - val_accuracy: 0.6609 - val_loss: 1.3610\n",
      "Epoch 3/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.7954 - loss: 1.1780 - val_accuracy: 0.7645 - val_loss: 1.1434\n",
      "Epoch 4/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8274 - loss: 1.0645 - val_accuracy: 0.7558 - val_loss: 1.1008\n",
      "Epoch 5/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.8706 - loss: 0.8937 - val_accuracy: 0.7537 - val_loss: 1.0697\n",
      "Epoch 6/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.8855 - loss: 0.7672 - val_accuracy: 0.6777 - val_loss: 1.2103\n",
      "Epoch 7/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8998 - loss: 0.6960 - val_accuracy: 0.6690 - val_loss: 1.1824\n",
      "Epoch 8/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9335 - loss: 0.5871 - val_accuracy: 0.7303 - val_loss: 1.0821\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "Fold 1 Accuracy: 0.8060\n",
      "Fold 1 F1 Score: 0.8150\n",
      "Fold 1 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.6825    0.8238    0.7465       488\n",
      "     Neutral     0.3007    0.3562    0.3261       233\n",
      "    Positive     0.9534    0.8667    0.9080      1583\n",
      "\n",
      "    accuracy                         0.8060      2304\n",
      "   macro avg     0.6456    0.6822    0.6602      2304\n",
      "weighted avg     0.8300    0.8060    0.8150      2304\n",
      "\n",
      "\n",
      "Training fold 2...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dariusng2103/projects/mla_project/tf217/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.4887 - loss: 1.8783 - val_accuracy: 0.6739 - val_loss: 1.5087\n",
      "Epoch 2/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.7657 - loss: 1.3937 - val_accuracy: 0.6848 - val_loss: 1.3813\n",
      "Epoch 3/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.7806 - loss: 1.2170 - val_accuracy: 0.6837 - val_loss: 1.2913\n",
      "Epoch 4/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.8237 - loss: 1.0858 - val_accuracy: 0.7065 - val_loss: 1.1848\n",
      "Epoch 5/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8050 - loss: 1.0093 - val_accuracy: 0.6869 - val_loss: 1.2237\n",
      "Epoch 6/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.7932 - loss: 1.0186 - val_accuracy: 0.6538 - val_loss: 1.2516\n",
      "Epoch 7/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8250 - loss: 0.8605 - val_accuracy: 0.7249 - val_loss: 1.0917\n",
      "Epoch 8/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8810 - loss: 0.7137 - val_accuracy: 0.7499 - val_loss: 1.0421\n",
      "Epoch 9/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9332 - loss: 0.5607 - val_accuracy: 0.6913 - val_loss: 1.1409\n",
      "Epoch 10/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9351 - loss: 0.4867 - val_accuracy: 0.6842 - val_loss: 1.2139\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "Fold 2 Accuracy: 0.8090\n",
      "Fold 2 F1 Score: 0.8249\n",
      "Fold 2 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7387    0.7705    0.7543       488\n",
      "     Neutral     0.3207    0.5064    0.3927       233\n",
      "    Positive     0.9601    0.8654    0.9103      1583\n",
      "\n",
      "    accuracy                         0.8090      2304\n",
      "   macro avg     0.6731    0.7141    0.6857      2304\n",
      "weighted avg     0.8485    0.8090    0.8249      2304\n",
      "\n",
      "\n",
      "Training fold 3...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dariusng2103/projects/mla_project/tf217/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.4021 - loss: 1.9702 - val_accuracy: 0.6663 - val_loss: 1.5417\n",
      "Epoch 2/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.6488 - loss: 1.4852 - val_accuracy: 0.6685 - val_loss: 1.3594\n",
      "Epoch 3/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.7712 - loss: 1.2328 - val_accuracy: 0.6647 - val_loss: 1.3095\n",
      "Epoch 4/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.7933 - loss: 1.0721 - val_accuracy: 0.7162 - val_loss: 1.1402\n",
      "Epoch 5/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.8230 - loss: 0.9758 - val_accuracy: 0.6283 - val_loss: 1.2691\n",
      "Epoch 6/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8585 - loss: 0.7959 - val_accuracy: 0.6359 - val_loss: 1.2068\n",
      "Epoch 7/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8661 - loss: 0.7027 - val_accuracy: 0.6663 - val_loss: 1.1295\n",
      "Epoch 8/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9058 - loss: 0.5997 - val_accuracy: 0.7216 - val_loss: 1.0912\n",
      "Epoch 9/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.9264 - loss: 0.5029 - val_accuracy: 0.7070 - val_loss: 1.0861\n",
      "Epoch 10/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.9287 - loss: 0.4473 - val_accuracy: 0.7499 - val_loss: 1.0306\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "Fold 3 Accuracy: 0.7834\n",
      "Fold 3 F1 Score: 0.7889\n",
      "Fold 3 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.5973    0.8176    0.6903       488\n",
      "     Neutral     0.2336    0.2146    0.2237       233\n",
      "    Positive     0.9536    0.8566    0.9025      1583\n",
      "\n",
      "    accuracy                         0.7834      2304\n",
      "   macro avg     0.5948    0.6296    0.6055      2304\n",
      "weighted avg     0.8053    0.7834    0.7889      2304\n",
      "\n",
      "\n",
      "Training fold 4...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dariusng2103/projects/mla_project/tf217/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.4166 - loss: 1.9690 - val_accuracy: 0.5789 - val_loss: 1.6101\n",
      "Epoch 2/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.7192 - loss: 1.4167 - val_accuracy: 0.5751 - val_loss: 1.4711\n",
      "Epoch 3/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.7904 - loss: 1.1667 - val_accuracy: 0.6978 - val_loss: 1.2147\n",
      "Epoch 4/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8288 - loss: 0.9883 - val_accuracy: 0.7173 - val_loss: 1.1212\n",
      "Epoch 5/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8347 - loss: 0.8659 - val_accuracy: 0.6256 - val_loss: 1.2570\n",
      "Epoch 6/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8763 - loss: 0.7570 - val_accuracy: 0.6603 - val_loss: 1.1869\n",
      "Epoch 7/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.8633 - loss: 0.7135 - val_accuracy: 0.7488 - val_loss: 1.0513\n",
      "Epoch 8/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9288 - loss: 0.5583 - val_accuracy: 0.7379 - val_loss: 1.1175\n",
      "Epoch 9/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9553 - loss: 0.4598 - val_accuracy: 0.7314 - val_loss: 1.1828\n",
      "Epoch 10/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9623 - loss: 0.4233 - val_accuracy: 0.7336 - val_loss: 1.1864\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "Fold 4 Accuracy: 0.8207\n",
      "Fold 4 F1 Score: 0.8138\n",
      "Fold 4 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7854    0.7500    0.7673       488\n",
      "     Neutral     0.3085    0.2489    0.2755       233\n",
      "    Positive     0.8890    0.9267    0.9075      1582\n",
      "\n",
      "    accuracy                         0.8207      2303\n",
      "   macro avg     0.6610    0.6419    0.6501      2303\n",
      "weighted avg     0.8083    0.8207    0.8138      2303\n",
      "\n",
      "\n",
      "Training fold 5...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dariusng2103/projects/mla_project/tf217/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.3788 - loss: 2.0308 - val_accuracy: 0.5453 - val_loss: 1.7255\n",
      "Epoch 2/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.6809 - loss: 1.4950 - val_accuracy: 0.6142 - val_loss: 1.4401\n",
      "Epoch 3/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.7348 - loss: 1.3049 - val_accuracy: 0.7103 - val_loss: 1.2388\n",
      "Epoch 4/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.8072 - loss: 1.1130 - val_accuracy: 0.6451 - val_loss: 1.2761\n",
      "Epoch 5/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.8377 - loss: 0.9486 - val_accuracy: 0.6989 - val_loss: 1.2035\n",
      "Epoch 6/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.8420 - loss: 0.8765 - val_accuracy: 0.7249 - val_loss: 1.1054\n",
      "Epoch 7/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.8897 - loss: 0.7134 - val_accuracy: 0.6647 - val_loss: 1.2494\n",
      "Epoch 8/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.9079 - loss: 0.6307 - val_accuracy: 0.7265 - val_loss: 1.0860\n",
      "Epoch 9/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9384 - loss: 0.5182 - val_accuracy: 0.6983 - val_loss: 1.2336\n",
      "Epoch 10/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9443 - loss: 0.4754 - val_accuracy: 0.6663 - val_loss: 1.4441\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "Fold 5 Accuracy: 0.8042\n",
      "Fold 5 F1 Score: 0.8157\n",
      "Fold 5 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8030    0.6585    0.7236       489\n",
      "     Neutral     0.3113    0.4871    0.3798       232\n",
      "    Positive     0.9207    0.8957    0.9080      1582\n",
      "\n",
      "    accuracy                         0.8042      2303\n",
      "   macro avg     0.6783    0.6804    0.6705      2303\n",
      "weighted avg     0.8343    0.8042    0.8157      2303\n",
      "\n",
      "\n",
      "Average Metrics across folds:\n",
      "Average Accuracy: 0.8047\n",
      "Average F1 Score: 0.8117\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure NLTK's punkt tokenizer is downloaded\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Set to CPU only\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Disable GPU\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "embedding_dim = 128       # Dimension of Word2Vec embeddings\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "l2_lambda = 0.01 \n",
    "\n",
    "# Step 1: Tokenize the text data\n",
    "tokenized_reviews = [word_tokenize(review.lower()) for review in data['processed_full_review']]\n",
    "\n",
    "# Step 2: Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_reviews, vector_size=embedding_dim, window=5, min_count=1, sg=1, seed=42)\n",
    "\n",
    "# Step 3: Prepare embedding matrix\n",
    "vocab_size = len(word2vec_model.wv.key_to_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Map Word2Vec vectors to the embedding matrix\n",
    "word_index = {word: idx + 1 for idx, word in enumerate(word2vec_model.wv.key_to_index)}\n",
    "for word, idx in word_index.items():\n",
    "    embedding_matrix[idx] = word2vec_model.wv[word]\n",
    "\n",
    "# Step 4: Convert reviews to sequences of word indices\n",
    "sequences = [[word_index.get(word, 0) for word in review] for review in tokenized_reviews]\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights_values = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
    "class_weights = {i: class_weights_values[i] for i in range(len(class_weights_values))}\n",
    "\n",
    "# Define stratified 5-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X_padded, y)):\n",
    "    print(f\"\\nTraining fold {fold + 1}...\\n\")\n",
    "    \n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
    "                        weights=[embedding_matrix], input_length=max_sequence_length, trainable=True))\n",
    "    model.add(SimpleRNN(64, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model with early stopping and class weights\n",
    "    model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=10, \n",
    "        batch_size=128,  \n",
    "        validation_split=0.2, \n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "    \n",
    "    # Predictions and evaluation for the current fold\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    \n",
    "    # Calculate metrics for the current fold\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, output_dict=True)\n",
    "    f1 = report['weighted avg']['f1-score']\n",
    "    \n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Fold {fold + 1} F1 Score: {f1:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4))\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(\"\\nAverage Metrics across folds:\")\n",
    "print(f\"Average Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN + FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 1...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dariusng2103/projects/mla_project/tf217/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.5104 - loss: 1.8729 - val_accuracy: 0.6218 - val_loss: 1.6363\n",
      "Epoch 2/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.7206 - loss: 1.4704 - val_accuracy: 0.6511 - val_loss: 1.4793\n",
      "Epoch 3/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.7615 - loss: 1.2732 - val_accuracy: 0.6853 - val_loss: 1.3166\n",
      "Epoch 4/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8114 - loss: 1.0917 - val_accuracy: 0.7493 - val_loss: 1.1498\n",
      "Epoch 5/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8090 - loss: 1.0071 - val_accuracy: 0.7417 - val_loss: 1.1711\n",
      "Epoch 6/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.7830 - loss: 1.0443 - val_accuracy: 0.7287 - val_loss: 1.1582\n",
      "Epoch 7/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8140 - loss: 0.9210 - val_accuracy: 0.6793 - val_loss: 1.2787\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "Fold 1 Accuracy: 0.7465\n",
      "Fold 1 F1 Score: 0.7573\n",
      "Fold 1 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.5529    0.9324    0.6941       488\n",
      "     Neutral     0.2089    0.2017    0.2052       233\n",
      "    Positive     0.9697    0.7694    0.8580      1583\n",
      "\n",
      "    accuracy                         0.7465      2304\n",
      "   macro avg     0.5772    0.6345    0.5858      2304\n",
      "weighted avg     0.8045    0.7465    0.7573      2304\n",
      "\n",
      "\n",
      "Training fold 2...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dariusng2103/projects/mla_project/tf217/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 0.4508 - loss: 1.9211 - val_accuracy: 0.5784 - val_loss: 1.6398\n",
      "Epoch 2/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.7369 - loss: 1.4191 - val_accuracy: 0.5654 - val_loss: 1.6890\n",
      "Epoch 3/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.7457 - loss: 1.2806 - val_accuracy: 0.7043 - val_loss: 1.2339\n",
      "Epoch 4/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8241 - loss: 1.0367 - val_accuracy: 0.7103 - val_loss: 1.1498\n",
      "Epoch 5/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8276 - loss: 0.9481 - val_accuracy: 0.6951 - val_loss: 1.1527\n",
      "Epoch 6/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8844 - loss: 0.7649 - val_accuracy: 0.6896 - val_loss: 1.1666\n",
      "Epoch 7/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8762 - loss: 0.6973 - val_accuracy: 0.6923 - val_loss: 1.1844\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "Fold 2 Accuracy: 0.7760\n",
      "Fold 2 F1 Score: 0.7930\n",
      "Fold 2 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.6244    0.7459    0.6797       488\n",
      "     Neutral     0.2636    0.3734    0.3091       233\n",
      "    Positive     0.9612    0.8446    0.8991      1583\n",
      "\n",
      "    accuracy                         0.7760      2304\n",
      "   macro avg     0.6164    0.6546    0.6293      2304\n",
      "weighted avg     0.8193    0.7760    0.7930      2304\n",
      "\n",
      "\n",
      "Training fold 3...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dariusng2103/projects/mla_project/tf217/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.4053 - loss: 1.9778 - val_accuracy: 0.5920 - val_loss: 1.6161\n",
      "Epoch 2/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.7103 - loss: 1.4139 - val_accuracy: 0.5860 - val_loss: 1.4802\n",
      "Epoch 3/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.7608 - loss: 1.2011 - val_accuracy: 0.6652 - val_loss: 1.2982\n",
      "Epoch 4/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8194 - loss: 1.0575 - val_accuracy: 0.7027 - val_loss: 1.1849\n",
      "Epoch 5/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8290 - loss: 0.9661 - val_accuracy: 0.7081 - val_loss: 1.1689\n",
      "Epoch 6/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.8699 - loss: 0.8546 - val_accuracy: 0.6858 - val_loss: 1.1345\n",
      "Epoch 7/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - accuracy: 0.8508 - loss: 0.7673 - val_accuracy: 0.7005 - val_loss: 1.1041\n",
      "Epoch 8/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.8896 - loss: 0.6561 - val_accuracy: 0.6511 - val_loss: 1.2078\n",
      "Epoch 9/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9268 - loss: 0.5574 - val_accuracy: 0.7358 - val_loss: 1.0361\n",
      "Epoch 10/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.9499 - loss: 0.4689 - val_accuracy: 0.7472 - val_loss: 1.0207\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Fold 3 Accuracy: 0.7925\n",
      "Fold 3 F1 Score: 0.8044\n",
      "Fold 3 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.6861    0.7705    0.7259       488\n",
      "     Neutral     0.3025    0.4077    0.3473       233\n",
      "    Positive     0.9397    0.8560    0.8959      1583\n",
      "\n",
      "    accuracy                         0.7925      2304\n",
      "   macro avg     0.6428    0.6781    0.6564      2304\n",
      "weighted avg     0.8215    0.7925    0.8044      2304\n",
      "\n",
      "\n",
      "Training fold 4...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dariusng2103/projects/mla_project/tf217/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 0.4473 - loss: 1.9335 - val_accuracy: 0.6712 - val_loss: 1.5310\n",
      "Epoch 2/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.7140 - loss: 1.4322 - val_accuracy: 0.6886 - val_loss: 1.3407\n",
      "Epoch 3/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.8039 - loss: 1.1686 - val_accuracy: 0.6093 - val_loss: 1.3714\n",
      "Epoch 4/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.6829 - loss: 1.1582 - val_accuracy: 0.6853 - val_loss: 1.2525\n",
      "Epoch 5/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8546 - loss: 0.9164 - val_accuracy: 0.6810 - val_loss: 1.2097\n",
      "Epoch 6/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.8769 - loss: 0.8068 - val_accuracy: 0.7781 - val_loss: 1.0003\n",
      "Epoch 7/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.8999 - loss: 0.7064 - val_accuracy: 0.6446 - val_loss: 1.2233\n",
      "Epoch 8/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.8668 - loss: 0.6772 - val_accuracy: 0.7119 - val_loss: 1.0815\n",
      "Epoch 9/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9246 - loss: 0.5306 - val_accuracy: 0.7189 - val_loss: 1.2303\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "Fold 4 Accuracy: 0.8042\n",
      "Fold 4 F1 Score: 0.8107\n",
      "Fold 4 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.6895    0.8238    0.7507       488\n",
      "     Neutral     0.2934    0.3262    0.3089       233\n",
      "    Positive     0.9405    0.8685    0.9031      1582\n",
      "\n",
      "    accuracy                         0.8042      2303\n",
      "   macro avg     0.6411    0.6728    0.6542      2303\n",
      "weighted avg     0.8218    0.8042    0.8107      2303\n",
      "\n",
      "\n",
      "Training fold 5...\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dariusng2103/projects/mla_project/tf217/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.3995 - loss: 2.0047 - val_accuracy: 0.5328 - val_loss: 1.9617\n",
      "Epoch 2/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.5888 - loss: 1.7293 - val_accuracy: 0.5898 - val_loss: 1.5581\n",
      "Epoch 3/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.7108 - loss: 1.3025 - val_accuracy: 0.7379 - val_loss: 1.1770\n",
      "Epoch 4/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.7765 - loss: 1.1194 - val_accuracy: 0.6191 - val_loss: 1.3764\n",
      "Epoch 5/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8018 - loss: 1.0019 - val_accuracy: 0.7021 - val_loss: 1.1315\n",
      "Epoch 6/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.8157 - loss: 0.9090 - val_accuracy: 0.6782 - val_loss: 1.1528\n",
      "Epoch 7/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.8993 - loss: 0.7066 - val_accuracy: 0.6614 - val_loss: 1.1845\n",
      "Epoch 8/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.9129 - loss: 0.6083 - val_accuracy: 0.6289 - val_loss: 1.2622\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "Fold 5 Accuracy: 0.7885\n",
      "Fold 5 F1 Score: 0.8117\n",
      "Fold 5 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8432    0.5828    0.6892       489\n",
      "     Neutral     0.3092    0.6983    0.4286       232\n",
      "    Positive     0.9500    0.8654    0.9057      1582\n",
      "\n",
      "    accuracy                         0.7885      2303\n",
      "   macro avg     0.7008    0.7155    0.6745      2303\n",
      "weighted avg     0.8628    0.7885    0.8117      2303\n",
      "\n",
      "\n",
      "Average Metrics across folds:\n",
      "Average Accuracy: 0.7816\n",
      "Average F1 Score: 0.7954\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure NLTK's punkt tokenizer is downloaded\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Set to CPU only\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Disable GPU\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "embedding_dim = 128       # Dimension of FastText embeddings\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "l2_lambda = 0.01 \n",
    "\n",
    "# Step 1: Tokenize the text data\n",
    "tokenized_reviews = [word_tokenize(review.lower()) for review in data['processed_full_review']]\n",
    "\n",
    "# Step 2: Train FastText model\n",
    "fasttext_model = FastText(sentences=tokenized_reviews, vector_size=embedding_dim, window=5, min_count=1, sg=1, seed=42)\n",
    "\n",
    "# Step 3: Prepare embedding matrix\n",
    "vocab_size = len(fasttext_model.wv.key_to_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Map FastText vectors to the embedding matrix\n",
    "word_index = {word: idx + 1 for idx, word in enumerate(fasttext_model.wv.key_to_index)}\n",
    "for word, idx in word_index.items():\n",
    "    embedding_matrix[idx] = fasttext_model.wv[word]\n",
    "\n",
    "# Step 4: Convert reviews to sequences of word indices\n",
    "sequences = [[word_index.get(word, 0) for word in review] for review in tokenized_reviews]\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights_values = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
    "class_weights = {i: class_weights_values[i] for i in range(len(class_weights_values))}\n",
    "\n",
    "# Define stratified 5-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X_padded, y)):\n",
    "    print(f\"\\nTraining fold {fold + 1}...\\n\")\n",
    "    \n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
    "                        weights=[embedding_matrix], input_length=max_sequence_length, trainable=True))\n",
    "    model.add(SimpleRNN(64, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model with early stopping and class weights\n",
    "    model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=10, \n",
    "        batch_size=128,  \n",
    "        validation_split=0.2, \n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "    \n",
    "    # Predictions and evaluation for the current fold\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    \n",
    "    # Calculate metrics for the current fold\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, output_dict=True)\n",
    "    f1 = report['weighted avg']['f1-score']\n",
    "    \n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Fold {fold + 1} F1 Score: {f1:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4))\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(\"\\nAverage Metrics across folds:\")\n",
    "print(f\"Average Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN + Pre-trained Word2Vec\n",
    "\n",
    "Pre-trained Word2Vec performs worse than within model trained Word2Vec.\n",
    "\n",
    "Google's Word2Vec embeddings were trained on very general Google News dataset, which may not align well with the context or vocabulary of our specific dataset, while custom embeddings trained directly on our dataset are tailored to the specific language and sentiment patterns within it.\n",
    "\n",
    "Since our dataset cotntains a lot of domain-specific terms and sentiment-heavy words that are less common in general news (like \"amazing\", \"terrible\", \"refund\"), pre-trained embeddings may not capture these terms accurately. Within-model embeddings can adapt specifically to the words and nuances in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Redbu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.4276 - loss: 1.1304 - val_accuracy: 0.7184 - val_loss: 0.6579\n",
      "Epoch 2/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.6504 - loss: 0.8589 - val_accuracy: 0.6994 - val_loss: 0.7364\n",
      "Epoch 3/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.6428 - loss: 0.8659 - val_accuracy: 0.6804 - val_loss: 0.9888\n",
      "Epoch 4/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.5904 - loss: 1.0298 - val_accuracy: 0.7303 - val_loss: 0.6904\n",
      "Epoch 5/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.7149 - loss: 0.7646 - val_accuracy: 0.7656 - val_loss: 0.5968\n",
      "Epoch 6/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.7062 - loss: 0.7707 - val_accuracy: 0.7699 - val_loss: 0.5865\n",
      "Epoch 7/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.7538 - loss: 0.7063 - val_accuracy: 0.7781 - val_loss: 0.5772\n",
      "Epoch 8/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.7300 - loss: 0.7097 - val_accuracy: 0.7629 - val_loss: 0.5952\n",
      "Epoch 9/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.7806 - loss: 0.6444 - val_accuracy: 0.7694 - val_loss: 0.6012\n",
      "Epoch 10/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.7281 - loss: 0.6718 - val_accuracy: 0.4037 - val_loss: 1.3044\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "Performance Metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.1860    0.3787    0.2495       470\n",
      "     Neutral     0.1259    0.2982    0.1771       228\n",
      "    Positive     0.7348    0.3692    0.4915      1606\n",
      "\n",
      "    accuracy                         0.3641      2304\n",
      "   macro avg     0.3489    0.3487    0.3060      2304\n",
      "weighted avg     0.5626    0.3641    0.4110      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from gensim.models import KeyedVectors\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 5000         # Limit vocabulary to 5000 words\n",
    "embedding_dim = 300        # Embedding dimensions for each word\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "\n",
    "# Step 1: Tokenize and Pad the Text\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(data['processed_full_review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_full_review'])\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Create Embedding Matrix with Pre-trained Word2Vec\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab_size:\n",
    "        # Retrieve the embedding vector for the word\n",
    "        if word in word2vec_model:\n",
    "            embedding_matrix[i] = word2vec_model[word]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Define a Simple RNN Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "model.add(SimpleRNN(64, activation='tanh'))\n",
    "model.add(Dropout(0.5))  # Add dropout for regularization\n",
    "model.add(Dense(3, activation='softmax'))   # Output layer for 3 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Step 3: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64,  validation_split=0.2, verbose=1, class_weight=class_weights_dict)\n",
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Calculate and print classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4)\n",
    "print('Performance Metrics:\\n', report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf217",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
