{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Singapore Airlines' Service Through Automated Sentiment Analysis of Customer Reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singapore Airlines Customer Reviews Dataset Information\n",
    "\n",
    "The [Singapore Airlines Customer Reviews Dataset](https://www.kaggle.com/datasets/kanchana1990/singapore-airlines-reviews) aggregates 10,000 anonymized customer reviews, providing a broad perspective on the passenger experience with Singapore Airlines. \n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`published_date`**: Date and time of review publication.\n",
    "- **`published_platform`**: Platform where the review was posted.\n",
    "- **`rating`**: Customer satisfaction rating, from 1 (lowest) to 5 (highest).\n",
    "- **`type`**: Specifies the content as a review.\n",
    "- **`text`**: Detailed customer feedback.\n",
    "- **`title`**: Summary of the review.\n",
    "- **`helpful_votes`**: Number of users finding the review helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional web scraping of online reviews\n",
    "\n",
    "During our EDA, we noticed two main trends in the distribution of our dataset:\n",
    "1. Less than 10% of our reviews were published from the years 2022 to 2024, making it hard for us to capture recent trends in sentiment.\n",
    "2. Most of the reviews were highly positive, which could mean that SIA had mostly positive reviews, nevertheless we wanted to get more information on negative reviews to improve the robustness of our model.\n",
    "\n",
    "### TripAdvisor\n",
    "\n",
    "We scraped more data for airline reviews from TripAdvisor, specifically for the years 2022 to 2024. \n",
    "(https://www.tripadvisor.com.sg/Airline_Review-d8729151-Reviews-Singapore-Airlines)\n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`Year`**: Year of review publication.\n",
    "- **`Month`**: Month of review publication.\n",
    "- **`Title`**: Title of review publication.\n",
    "- **`Review Text`**: Main text content of review publication.\n",
    "- **`Rating`**: Numerical rating provided by reviewer (Scale: 1 to 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skytrax\n",
    "\n",
    "We also scraped from Skytrax, which is another data source for online reviews. \n",
    "(https://www.airlinequality.com/airline-reviews/singapore-airlines/?sortby=post_date%3ADesc&pagesize=100)\n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`Year`**: Year of review publication.\n",
    "- **`Month`**: Month of review publication.\n",
    "- **`Title`**: Title of review publication.\n",
    "- **`Review Text`**: Main text content of review publication.\n",
    "- **`Rating`**: Numerical rating provided by reviewer (Scale: 1 to 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Please uncomment the code box below to pip install relevant dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=2.0.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.11.1)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (4.65.0)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (3.7.2)\n",
      "Requirement already satisfied: seaborn>=0.12.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (0.12.2)\n",
      "Requirement already satisfied: langdetect>=1.0.9 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.0.9)\n",
      "Requirement already satisfied: langid>=1.1.6 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (1.1.6)\n",
      "Requirement already satisfied: nltk>=3.8.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (3.8.1)\n",
      "Requirement already satisfied: wordcloud>=1.9.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (1.9.3)\n",
      "Requirement already satisfied: tensorflow>=2.17.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (2.18.0)\n",
      "Requirement already satisfied: scikeras>=0.10.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: six in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from langdetect>=1.0.9->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from nltk>=3.8.1->-r requirements.txt (line 9)) (2022.7.9)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (68.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (3.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.37.1)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from scikeras>=0.10.0->-r requirements.txt (line 12)) (1.5.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.38.4)\n",
      "Requirement already satisfied: rich in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (13.9.2)\n",
      "Requirement already satisfied: namex in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (2023.7.22)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.4.2->scikeras>=0.10.0->-r requirements.txt (line 12)) (3.5.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.17.1->-r requirements.txt (line 11)) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.17.1->-r requirements.txt (line 11)) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mayaung/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.17.1->-r requirements.txt (line 11)) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "import nltk\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# For generating n-grams\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation (Loading CSV)\n",
    "\n",
    "Load the three CSV files into a pandas DataFrame `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processed_full_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>ok use airlin go singapor london heathrow issu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>don give money book paid receiv email confirm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "      <td>best airlin world best airlin world seat food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>premium economi seat singapor airlin not worth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>imposs get promis refund book flight full mont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month sentiment                              processed_full_review\n",
       "0  2024      3   Neutral  ok use airlin go singapor london heathrow issu...\n",
       "1  2024      3  Negative  don give money book paid receiv email confirm ...\n",
       "2  2024      3  Positive  best airlin world best airlin world seat food ...\n",
       "3  2024      3  Negative  premium economi seat singapor airlin not worth...\n",
       "4  2024      3  Negative  imposs get promis refund book flight full mont..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Positive    7913\n",
       "Negative    2441\n",
       "Neutral     1164\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2019    5129\n",
       "2018    2596\n",
       "2022    1184\n",
       "2023    1111\n",
       "2020     888\n",
       "2024     514\n",
       "2021      96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LSTM\n",
    "\n",
    "LSTM Model Explanation:\n",
    "\n",
    "Model Initialization: A Sequential model is used to stack the layers in order.\n",
    "\n",
    "Embedding Layer: The first layer is an Embedding layer with input_dim=10000 (vocabulary size) and output_dim=128 (embedding dimension). This layer converts word indices into dense vector representations that the LSTM can process.\n",
    "\n",
    "LSTM Layers: The model includes two LSTM layers:\n",
    "The first LSTM layer has 64 units and return_sequences=True, allowing its output to be passed to the next LSTM layer.\n",
    "The second LSTM layer also has 64 units but return_sequences=False, indicating it outputs only the last hidden state to the next layer.\n",
    "\n",
    "Dropout Layers: Dropout layers with a rate of 0.5 are added after each LSTM and Dense layer to help prevent overfitting by randomly setting half of the input units to 0 during training.\n",
    "\n",
    "Dense Layers: A Dense layer with 32 units and tanh activation is added for further processing of the output from the last LSTM layer.\n",
    "The final Dense layer has 3 units (corresponding to the three sentiment classes: Positive, Neutral, Negative) and a softmax activation function for multi-class classification.\n",
    "\n",
    "Compilation: The model is compiled using the adam optimizer, sparse_categorical_crossentropy as the loss function (suitable for integer-encoded classes), and accuracy as a performance metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 1...\n",
      "\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 3s 39ms/step - loss: 1.9191 - accuracy: 0.6437 - val_loss: 1.4888 - val_accuracy: 0.4818\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 2s 34ms/step - loss: 0.9370 - accuracy: 0.7983 - val_loss: 0.8444 - val_accuracy: 0.6701\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 2s 35ms/step - loss: 0.6145 - accuracy: 0.8414 - val_loss: 0.7845 - val_accuracy: 0.6864\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 2s 35ms/step - loss: 0.4796 - accuracy: 0.8752 - val_loss: 0.7160 - val_accuracy: 0.7558\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.4085 - accuracy: 0.8993 - val_loss: 0.7612 - val_accuracy: 0.7287\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.6518 - accuracy: 0.8204 - val_loss: 0.9220 - val_accuracy: 0.6245\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 2s 34ms/step - loss: 0.4934 - accuracy: 0.9039 - val_loss: 0.8385 - val_accuracy: 0.6918\n",
      "72/72 [==============================] - 1s 11ms/step\n",
      "Fold 1 Accuracy: 0.8403\n",
      "Fold 1 F1 Score: 0.8432\n",
      "Fold 1 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8410    0.6721    0.7472       488\n",
      "     Neutral     0.3763    0.4764    0.4205       233\n",
      "    Positive     0.9246    0.9457    0.9350      1583\n",
      "\n",
      "    accuracy                         0.8403      2304\n",
      "   macro avg     0.7140    0.6981    0.7009      2304\n",
      "weighted avg     0.8515    0.8403    0.8432      2304\n",
      "\n",
      "\n",
      "Training fold 2...\n",
      "\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 3s 39ms/step - loss: 1.9028 - accuracy: 0.6829 - val_loss: 1.3967 - val_accuracy: 0.4341\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.9120 - accuracy: 0.7848 - val_loss: 0.9441 - val_accuracy: 0.5654\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.6359 - accuracy: 0.8400 - val_loss: 0.7225 - val_accuracy: 0.7260\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.5217 - accuracy: 0.8702 - val_loss: 0.9110 - val_accuracy: 0.6240\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.4842 - accuracy: 0.8840 - val_loss: 0.7693 - val_accuracy: 0.7401\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.4361 - accuracy: 0.9065 - val_loss: 0.7756 - val_accuracy: 0.7347\n",
      "72/72 [==============================] - 1s 10ms/step\n",
      "Fold 2 Accuracy: 0.8212\n",
      "Fold 2 F1 Score: 0.8393\n",
      "Fold 2 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8761    0.6373    0.7378       488\n",
      "     Neutral     0.3503    0.7082    0.4688       233\n",
      "    Positive     0.9581    0.8945    0.9252      1583\n",
      "\n",
      "    accuracy                         0.8212      2304\n",
      "   macro avg     0.7281    0.7467    0.7106      2304\n",
      "weighted avg     0.8792    0.8212    0.8393      2304\n",
      "\n",
      "\n",
      "Training fold 3...\n",
      "\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 3s 38ms/step - loss: 1.8400 - accuracy: 0.7291 - val_loss: 1.1425 - val_accuracy: 0.7141\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.9064 - accuracy: 0.8209 - val_loss: 0.8781 - val_accuracy: 0.7016\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.7259 - accuracy: 0.8143 - val_loss: 0.8533 - val_accuracy: 0.6598\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.5716 - accuracy: 0.8666 - val_loss: 0.9507 - val_accuracy: 0.5990\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.5228 - accuracy: 0.8835 - val_loss: 0.8550 - val_accuracy: 0.6549\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.4227 - accuracy: 0.8942 - val_loss: 0.7724 - val_accuracy: 0.7037\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.7267 - accuracy: 0.8411 - val_loss: 0.7172 - val_accuracy: 0.7754\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.4973 - accuracy: 0.9099 - val_loss: 0.6870 - val_accuracy: 0.7786\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 2s 35ms/step - loss: 0.4273 - accuracy: 0.9235 - val_loss: 0.6520 - val_accuracy: 0.7916\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.3981 - accuracy: 0.9270 - val_loss: 0.6522 - val_accuracy: 0.8020\n",
      "72/72 [==============================] - 1s 10ms/step\n",
      "Fold 3 Accuracy: 0.8342\n",
      "Fold 3 F1 Score: 0.8322\n",
      "Fold 3 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7642    0.7705    0.7673       488\n",
      "     Neutral     0.4065    0.3734    0.3893       233\n",
      "    Positive     0.9130    0.9217    0.9173      1583\n",
      "\n",
      "    accuracy                         0.8342      2304\n",
      "   macro avg     0.6946    0.6885    0.6913      2304\n",
      "weighted avg     0.8303    0.8342    0.8322      2304\n",
      "\n",
      "\n",
      "Training fold 4...\n",
      "\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 3s 38ms/step - loss: 1.8781 - accuracy: 0.6698 - val_loss: 1.2005 - val_accuracy: 0.6712\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.9322 - accuracy: 0.8007 - val_loss: 0.8454 - val_accuracy: 0.7016\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.6410 - accuracy: 0.8401 - val_loss: 0.7579 - val_accuracy: 0.7151\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.5312 - accuracy: 0.8755 - val_loss: 0.7684 - val_accuracy: 0.7146\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.5357 - accuracy: 0.8479 - val_loss: 0.7370 - val_accuracy: 0.7575\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.4236 - accuracy: 0.9143 - val_loss: 0.7617 - val_accuracy: 0.7515\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.3502 - accuracy: 0.9318 - val_loss: 0.8402 - val_accuracy: 0.7461\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.3295 - accuracy: 0.9270 - val_loss: 0.6602 - val_accuracy: 0.7927\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.2836 - accuracy: 0.9523 - val_loss: 0.7704 - val_accuracy: 0.7618\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.2984 - accuracy: 0.9497 - val_loss: 0.8603 - val_accuracy: 0.7336\n",
      "72/72 [==============================] - 1s 10ms/step\n",
      "Fold 4 Accuracy: 0.8007\n",
      "Fold 4 F1 Score: 0.8167\n",
      "Fold 4 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8466    0.6332    0.7245       488\n",
      "     Neutral     0.3423    0.6524    0.4490       233\n",
      "    Positive     0.9257    0.8742    0.8992      1582\n",
      "\n",
      "    accuracy                         0.8007      2303\n",
      "   macro avg     0.7049    0.7199    0.6909      2303\n",
      "weighted avg     0.8499    0.8007    0.8167      2303\n",
      "\n",
      "\n",
      "Training fold 5...\n",
      "\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 4s 38ms/step - loss: 1.9018 - accuracy: 0.6146 - val_loss: 1.4101 - val_accuracy: 0.5323\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.9197 - accuracy: 0.8158 - val_loss: 0.8476 - val_accuracy: 0.6441\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.6288 - accuracy: 0.8425 - val_loss: 0.8520 - val_accuracy: 0.6240\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.5207 - accuracy: 0.8665 - val_loss: 0.7693 - val_accuracy: 0.7227\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.4271 - accuracy: 0.9017 - val_loss: 0.7340 - val_accuracy: 0.7509\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.4189 - accuracy: 0.9030 - val_loss: 1.8973 - val_accuracy: 0.4314\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.4608 - accuracy: 0.8957 - val_loss: 0.7666 - val_accuracy: 0.7450\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.3003 - accuracy: 0.9398 - val_loss: 0.7968 - val_accuracy: 0.7461\n",
      "72/72 [==============================] - 1s 11ms/step\n",
      "Fold 5 Accuracy: 0.8254\n",
      "Fold 5 F1 Score: 0.8385\n",
      "Fold 5 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8182    0.7362    0.7750       489\n",
      "     Neutral     0.3711    0.6078    0.4608       232\n",
      "    Positive     0.9440    0.8850    0.9135      1582\n",
      "\n",
      "    accuracy                         0.8254      2303\n",
      "   macro avg     0.7111    0.7430    0.7165      2303\n",
      "weighted avg     0.8596    0.8254    0.8385      2303\n",
      "\n",
      "\n",
      "Average Metrics across folds:\n",
      "Average Accuracy: 0.8244\n",
      "Average F1 Score: 0.8340\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 10000  # Limit vocabulary size to 10,000 words\n",
    "embedding_dim = 128  # Dimension of embeddings\n",
    "max_sequence_length = 300  # Max number of words in each sequence\n",
    "l2_lambda = 0.01\n",
    "\n",
    "# Step 1: Tokenize and pad text data using Keras Tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(data['processed_full_review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_full_review'])\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights_values = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
    "class_weights = {i: class_weights_values[i] for i in range(len(class_weights_values))}\n",
    "\n",
    "# Define stratified 5-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X_padded, y)):\n",
    "    print(f\"\\nTraining fold {fold + 1}...\\n\")\n",
    "    \n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Define the model architecture with a trainable Embedding layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length, trainable=True))\n",
    "    model.add(LSTM(64, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model with early stopping and class weights\n",
    "    model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=10, \n",
    "        batch_size=128,  \n",
    "        validation_split=0.2, \n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "    \n",
    "    # Predictions and evaluation for the current fold\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    \n",
    "    # Calculate metrics for the current fold\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, output_dict=True)\n",
    "    f1 = report['weighted avg']['f1-score']\n",
    "    \n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Fold {fold + 1} F1 Score: {f1:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4))\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(\"\\nAverage Metrics across folds:\")\n",
    "print(f\"Average Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 1...\n",
      "\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 3s 38ms/step - loss: 1.9270 - accuracy: 0.6615 - val_loss: 1.3371 - val_accuracy: 0.7124\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 1.0415 - accuracy: 0.8137 - val_loss: 1.0041 - val_accuracy: 0.7222\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 0.8191 - accuracy: 0.8261 - val_loss: 0.9402 - val_accuracy: 0.6717\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.6798 - accuracy: 0.8585 - val_loss: 0.7544 - val_accuracy: 0.7976\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 0.6317 - accuracy: 0.8650 - val_loss: 0.7836 - val_accuracy: 0.7634\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 0.5310 - accuracy: 0.8969 - val_loss: 0.7502 - val_accuracy: 0.7846\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 0.4767 - accuracy: 0.9107 - val_loss: 0.8269 - val_accuracy: 0.7282\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 0.7374 - accuracy: 0.7429 - val_loss: 0.7543 - val_accuracy: 0.7623\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.5156 - accuracy: 0.8740 - val_loss: 0.8153 - val_accuracy: 0.7298\n",
      "72/72 [==============================] - 1s 10ms/step\n",
      "Fold 1 Accuracy: 0.8251\n",
      "Fold 1 F1 Score: 0.8415\n",
      "Fold 1 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8371    0.7582    0.7957       488\n",
      "     Neutral     0.3608    0.6395    0.4613       233\n",
      "    Positive     0.9538    0.8730    0.9116      1583\n",
      "\n",
      "    accuracy                         0.8251      2304\n",
      "   macro avg     0.7172    0.7569    0.7229      2304\n",
      "weighted avg     0.8691    0.8251    0.8415      2304\n",
      "\n",
      "\n",
      "Training fold 2...\n",
      "\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 3s 38ms/step - loss: 1.9100 - accuracy: 0.6732 - val_loss: 1.3370 - val_accuracy: 0.6630\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 1.0183 - accuracy: 0.7974 - val_loss: 1.1631 - val_accuracy: 0.5996\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.7717 - accuracy: 0.8380 - val_loss: 0.7785 - val_accuracy: 0.7922\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.7180 - accuracy: 0.8197 - val_loss: 0.7570 - val_accuracy: 0.7808\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 2s 36ms/step - loss: 0.6067 - accuracy: 0.8798 - val_loss: 0.8056 - val_accuracy: 0.7602\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 2s 36ms/step - loss: 0.5298 - accuracy: 0.8940 - val_loss: 0.7847 - val_accuracy: 0.7623\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.5111 - accuracy: 0.8902 - val_loss: 0.8237 - val_accuracy: 0.7276\n",
      "72/72 [==============================] - 1s 10ms/step\n",
      "Fold 2 Accuracy: 0.8242\n",
      "Fold 2 F1 Score: 0.8392\n",
      "Fold 2 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7840    0.8033    0.7935       488\n",
      "     Neutral     0.3450    0.5494    0.4238       233\n",
      "    Positive     0.9623    0.8711    0.9145      1583\n",
      "\n",
      "    accuracy                         0.8242      2304\n",
      "   macro avg     0.6971    0.7413    0.7106      2304\n",
      "weighted avg     0.8621    0.8242    0.8392      2304\n",
      "\n",
      "\n",
      "Training fold 3...\n",
      "\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 3s 39ms/step - loss: 1.8853 - accuracy: 0.6843 - val_loss: 1.3368 - val_accuracy: 0.6495\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 1.0070 - accuracy: 0.7969 - val_loss: 1.0072 - val_accuracy: 0.6598\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.7832 - accuracy: 0.8265 - val_loss: 0.8955 - val_accuracy: 0.7276\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.6724 - accuracy: 0.8596 - val_loss: 0.8520 - val_accuracy: 0.7151\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.6384 - accuracy: 0.8582 - val_loss: 0.8908 - val_accuracy: 0.6837\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.6317 - accuracy: 0.8379 - val_loss: 0.8522 - val_accuracy: 0.6820\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.5206 - accuracy: 0.8974 - val_loss: 0.9199 - val_accuracy: 0.6582\n",
      "72/72 [==============================] - 1s 10ms/step\n",
      "Fold 3 Accuracy: 0.8043\n",
      "Fold 3 F1 Score: 0.8221\n",
      "Fold 3 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9077    0.5041    0.6482       488\n",
      "     Neutral     0.3302    0.7639    0.4611       233\n",
      "    Positive     0.9565    0.9027    0.9288      1583\n",
      "\n",
      "    accuracy                         0.8043      2304\n",
      "   macro avg     0.7315    0.7236    0.6794      2304\n",
      "weighted avg     0.8828    0.8043    0.8221      2304\n",
      "\n",
      "\n",
      "Training fold 4...\n",
      "\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 3s 38ms/step - loss: 1.9207 - accuracy: 0.6721 - val_loss: 1.3870 - val_accuracy: 0.6793\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 1.0423 - accuracy: 0.8032 - val_loss: 0.9560 - val_accuracy: 0.7472\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.8090 - accuracy: 0.8367 - val_loss: 0.8551 - val_accuracy: 0.7585\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.6912 - accuracy: 0.8588 - val_loss: 0.8452 - val_accuracy: 0.7401\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.6175 - accuracy: 0.8738 - val_loss: 0.7673 - val_accuracy: 0.7770\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 0.5483 - accuracy: 0.8958 - val_loss: 0.7186 - val_accuracy: 0.7954\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.4803 - accuracy: 0.9143 - val_loss: 0.7552 - val_accuracy: 0.7656\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.4197 - accuracy: 0.9239 - val_loss: 0.6632 - val_accuracy: 0.8215\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.6254 - accuracy: 0.7816 - val_loss: 0.7226 - val_accuracy: 0.8106\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 0.5938 - accuracy: 0.8884 - val_loss: 0.7682 - val_accuracy: 0.7775\n",
      "72/72 [==============================] - 1s 10ms/step\n",
      "Fold 4 Accuracy: 0.8428\n",
      "Fold 4 F1 Score: 0.8516\n",
      "Fold 4 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8807    0.7111    0.7868       488\n",
      "     Neutral     0.3914    0.5880    0.4700       233\n",
      "    Positive     0.9346    0.9210    0.9277      1582\n",
      "\n",
      "    accuracy                         0.8428      2303\n",
      "   macro avg     0.7356    0.7400    0.7282      2303\n",
      "weighted avg     0.8682    0.8428    0.8516      2303\n",
      "\n",
      "\n",
      "Training fold 5...\n",
      "\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 4s 37ms/step - loss: 1.9460 - accuracy: 0.6176 - val_loss: 1.3827 - val_accuracy: 0.6479\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 1.0537 - accuracy: 0.7946 - val_loss: 1.0092 - val_accuracy: 0.7081\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 0.8847 - accuracy: 0.7750 - val_loss: 0.8600 - val_accuracy: 0.7520\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.7131 - accuracy: 0.8380 - val_loss: 0.7919 - val_accuracy: 0.7965\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 0.6371 - accuracy: 0.8672 - val_loss: 0.7249 - val_accuracy: 0.7987\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.5522 - accuracy: 0.8893 - val_loss: 0.7206 - val_accuracy: 0.7895\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.4904 - accuracy: 0.9027 - val_loss: 0.7145 - val_accuracy: 0.8063\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.4513 - accuracy: 0.9133 - val_loss: 0.7157 - val_accuracy: 0.7960\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.3839 - accuracy: 0.9310 - val_loss: 0.7555 - val_accuracy: 0.7819\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.3503 - accuracy: 0.9463 - val_loss: 0.7716 - val_accuracy: 0.7770\n",
      "72/72 [==============================] - 1s 11ms/step\n",
      "Fold 5 Accuracy: 0.8498\n",
      "Fold 5 F1 Score: 0.8567\n",
      "Fold 5 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8326    0.7935    0.8126       489\n",
      "     Neutral     0.4052    0.5345    0.4610       232\n",
      "    Positive     0.9438    0.9134    0.9284      1582\n",
      "\n",
      "    accuracy                         0.8498      2303\n",
      "   macro avg     0.7272    0.7471    0.7340      2303\n",
      "weighted avg     0.8660    0.8498    0.8567      2303\n",
      "\n",
      "\n",
      "Average Metrics across folds:\n",
      "Average Accuracy: 0.8292\n",
      "Average F1 Score: 0.8422\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure NLTK's punkt tokenizer is downloaded\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "embedding_dim = 128       # Dimension of Word2Vec embeddings\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "l2_lambda = 0.01 \n",
    "\n",
    "# Step 1: Tokenize the text data\n",
    "tokenized_reviews = [word_tokenize(review.lower()) for review in data['processed_full_review']]\n",
    "\n",
    "# Step 2: Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_reviews, vector_size=embedding_dim, window=5, min_count=1, sg=1, seed=42)\n",
    "\n",
    "# Step 3: Prepare embedding matrix\n",
    "vocab_size = len(word2vec_model.wv.key_to_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Map Word2Vec vectors to the embedding matrix\n",
    "word_index = {word: idx + 1 for idx, word in enumerate(word2vec_model.wv.key_to_index)}\n",
    "for word, idx in word_index.items():\n",
    "    embedding_matrix[idx] = word2vec_model.wv[word]\n",
    "\n",
    "# Step 4: Convert reviews to sequences of word indices\n",
    "sequences = [[word_index.get(word, 0) for word in review] for review in tokenized_reviews]\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights_values = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
    "class_weights = {i: class_weights_values[i] for i in range(len(class_weights_values))}\n",
    "\n",
    "# Define stratified 5-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X_padded, y)):\n",
    "    print(f\"\\nTraining fold {fold + 1}...\\n\")\n",
    "    \n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Define the model architecture with one LSTM layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
    "                        weights=[embedding_matrix], input_length=max_sequence_length, trainable=True))\n",
    "    model.add(LSTM(64, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model with early stopping and class weights\n",
    "    model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=10, \n",
    "        batch_size=128,  \n",
    "        validation_split=0.2, \n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "    \n",
    "    # Predictions and evaluation for the current fold\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    \n",
    "    # Calculate metrics for the current fold\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, output_dict=True)\n",
    "    f1 = report['weighted avg']['f1-score']\n",
    "    \n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Fold {fold + 1} F1 Score: {f1:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4))\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(\"\\nAverage Metrics across folds:\")\n",
    "print(f\"Average Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training fold 1...\n",
      "\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 3s 39ms/step - loss: 1.9089 - accuracy: 0.6630 - val_loss: 1.3172 - val_accuracy: 0.7184\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 1.0985 - accuracy: 0.7422 - val_loss: 0.9846 - val_accuracy: 0.7488\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.8598 - accuracy: 0.8209 - val_loss: 0.9041 - val_accuracy: 0.7325\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 2s 35ms/step - loss: 0.7508 - accuracy: 0.8364 - val_loss: 1.0468 - val_accuracy: 0.5996\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.6418 - accuracy: 0.8696 - val_loss: 0.8594 - val_accuracy: 0.7553\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.5638 - accuracy: 0.8900 - val_loss: 0.8433 - val_accuracy: 0.7401\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.5112 - accuracy: 0.8989 - val_loss: 0.7598 - val_accuracy: 0.7634\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.5398 - accuracy: 0.8782 - val_loss: 0.7079 - val_accuracy: 0.7889\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.4197 - accuracy: 0.9238 - val_loss: 0.7094 - val_accuracy: 0.8063\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.3557 - accuracy: 0.9440 - val_loss: 0.7649 - val_accuracy: 0.7857\n",
      "72/72 [==============================] - 1s 10ms/step\n",
      "Fold 1 Accuracy: 0.8134\n",
      "Fold 1 F1 Score: 0.8339\n",
      "Fold 1 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7992    0.8156    0.8073       488\n",
      "     Neutral     0.3224    0.5880    0.4164       233\n",
      "    Positive     0.9696    0.8459    0.9035      1583\n",
      "\n",
      "    accuracy                         0.8134      2304\n",
      "   macro avg     0.6970    0.7498    0.7091      2304\n",
      "weighted avg     0.8680    0.8134    0.8339      2304\n",
      "\n",
      "\n",
      "Training fold 2...\n",
      "\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 3s 39ms/step - loss: 1.9566 - accuracy: 0.6348 - val_loss: 1.3262 - val_accuracy: 0.6750\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 1.0603 - accuracy: 0.7641 - val_loss: 1.0805 - val_accuracy: 0.6403\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.7929 - accuracy: 0.8344 - val_loss: 0.8484 - val_accuracy: 0.7520\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.6661 - accuracy: 0.8590 - val_loss: 0.8467 - val_accuracy: 0.7509\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.7392 - accuracy: 0.8273 - val_loss: 0.8593 - val_accuracy: 0.7010\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.5797 - accuracy: 0.8784 - val_loss: 0.7433 - val_accuracy: 0.7851\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 0.5161 - accuracy: 0.9019 - val_loss: 0.8684 - val_accuracy: 0.6940\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 0.5463 - accuracy: 0.8805 - val_loss: 0.8498 - val_accuracy: 0.6934\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 0.4381 - accuracy: 0.9164 - val_loss: 0.8124 - val_accuracy: 0.7455\n",
      "72/72 [==============================] - 1s 10ms/step\n",
      "Fold 2 Accuracy: 0.8446\n",
      "Fold 2 F1 Score: 0.8549\n",
      "Fold 2 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8209    0.7418    0.7793       488\n",
      "     Neutral     0.3868    0.5794    0.4639       233\n",
      "    Positive     0.9571    0.9154    0.9357      1583\n",
      "\n",
      "    accuracy                         0.8446      2304\n",
      "   macro avg     0.7216    0.7455    0.7263      2304\n",
      "weighted avg     0.8705    0.8446    0.8549      2304\n",
      "\n",
      "\n",
      "Training fold 3...\n",
      "\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 4s 37ms/step - loss: 1.9025 - accuracy: 0.6934 - val_loss: 1.4083 - val_accuracy: 0.6891\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 1.0801 - accuracy: 0.7760 - val_loss: 1.0799 - val_accuracy: 0.6164\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.8135 - accuracy: 0.8225 - val_loss: 0.9399 - val_accuracy: 0.7032\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 0.6953 - accuracy: 0.8559 - val_loss: 0.9431 - val_accuracy: 0.6679\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 0.6334 - accuracy: 0.8597 - val_loss: 0.8998 - val_accuracy: 0.6853\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.5511 - accuracy: 0.8885 - val_loss: 0.7579 - val_accuracy: 0.7602\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.5446 - accuracy: 0.8813 - val_loss: 0.8197 - val_accuracy: 0.6972\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.4603 - accuracy: 0.9090 - val_loss: 0.9222 - val_accuracy: 0.7054\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 2s 31ms/step - loss: 0.4294 - accuracy: 0.9178 - val_loss: 0.7385 - val_accuracy: 0.7813\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.3515 - accuracy: 0.9445 - val_loss: 0.7767 - val_accuracy: 0.7846\n",
      "72/72 [==============================] - 1s 11ms/step\n",
      "Fold 3 Accuracy: 0.8342\n",
      "Fold 3 F1 Score: 0.8434\n",
      "Fold 3 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8337    0.6988    0.7603       488\n",
      "     Neutral     0.3950    0.6052    0.4780       233\n",
      "    Positive     0.9363    0.9097    0.9228      1583\n",
      "\n",
      "    accuracy                         0.8342      2304\n",
      "   macro avg     0.7217    0.7379    0.7204      2304\n",
      "weighted avg     0.8598    0.8342    0.8434      2304\n",
      "\n",
      "\n",
      "Training fold 4...\n",
      "\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 4s 40ms/step - loss: 1.9286 - accuracy: 0.6449 - val_loss: 1.3621 - val_accuracy: 0.6923\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 2s 34ms/step - loss: 1.0554 - accuracy: 0.7971 - val_loss: 1.1518 - val_accuracy: 0.6115\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.8333 - accuracy: 0.8327 - val_loss: 0.8192 - val_accuracy: 0.7797\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.7102 - accuracy: 0.8574 - val_loss: 0.8909 - val_accuracy: 0.7227\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 2s 34ms/step - loss: 0.6474 - accuracy: 0.8566 - val_loss: 0.7654 - val_accuracy: 0.7699\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 2s 35ms/step - loss: 0.6304 - accuracy: 0.8474 - val_loss: 0.7374 - val_accuracy: 0.7933\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.5766 - accuracy: 0.8881 - val_loss: 0.7175 - val_accuracy: 0.7998\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.4469 - accuracy: 0.9186 - val_loss: 0.6920 - val_accuracy: 0.8079\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.3930 - accuracy: 0.9356 - val_loss: 0.7111 - val_accuracy: 0.7873\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.4468 - accuracy: 0.9116 - val_loss: 0.8005 - val_accuracy: 0.7130\n",
      "72/72 [==============================] - 1s 10ms/step\n",
      "Fold 4 Accuracy: 0.7968\n",
      "Fold 4 F1 Score: 0.8129\n",
      "Fold 4 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8279    0.5717    0.6764       488\n",
      "     Neutral     0.3043    0.5837    0.4000       233\n",
      "    Positive     0.9348    0.8976    0.9158      1582\n",
      "\n",
      "    accuracy                         0.7968      2303\n",
      "   macro avg     0.6890    0.6843    0.6641      2303\n",
      "weighted avg     0.8484    0.7968    0.8129      2303\n",
      "\n",
      "\n",
      "Training fold 5...\n",
      "\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 3s 38ms/step - loss: 1.9914 - accuracy: 0.6007 - val_loss: 1.5029 - val_accuracy: 0.5404\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 2s 35ms/step - loss: 1.0794 - accuracy: 0.7888 - val_loss: 1.0155 - val_accuracy: 0.7070\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.7968 - accuracy: 0.8276 - val_loss: 0.8837 - val_accuracy: 0.7292\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 2s 33ms/step - loss: 0.7094 - accuracy: 0.8435 - val_loss: 0.8067 - val_accuracy: 0.7553\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.6085 - accuracy: 0.8730 - val_loss: 0.8193 - val_accuracy: 0.7466\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.5444 - accuracy: 0.8909 - val_loss: 0.7103 - val_accuracy: 0.7911\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.5081 - accuracy: 0.8893 - val_loss: 0.7172 - val_accuracy: 0.7851\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.4525 - accuracy: 0.9140 - val_loss: 0.7636 - val_accuracy: 0.7553\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 2s 32ms/step - loss: 0.4178 - accuracy: 0.9278 - val_loss: 0.7125 - val_accuracy: 0.7900\n",
      "72/72 [==============================] - 1s 10ms/step\n",
      "Fold 5 Accuracy: 0.8354\n",
      "Fold 5 F1 Score: 0.8484\n",
      "Fold 5 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8249    0.7710    0.7970       489\n",
      "     Neutral     0.3905    0.6379    0.4845       232\n",
      "    Positive     0.9536    0.8843    0.9177      1582\n",
      "\n",
      "    accuracy                         0.8354      2303\n",
      "   macro avg     0.7230    0.7644    0.7331      2303\n",
      "weighted avg     0.8696    0.8354    0.8484      2303\n",
      "\n",
      "\n",
      "Average Metrics across folds:\n",
      "Average Accuracy: 0.8249\n",
      "Average F1 Score: 0.8387\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "embedding_dim = 128       # Dimension of FastText embeddings\n",
    "max_sequence_length = 300 # Max number of words in each sequence\n",
    "l2_lambda = 0.01 \n",
    "\n",
    "# Step 1: Tokenize the text data\n",
    "tokenized_reviews = [word_tokenize(review.lower()) for review in data['processed_full_review']]\n",
    "\n",
    "# Step 2: Train FastText model\n",
    "fasttext_model = FastText(sentences=tokenized_reviews, vector_size=embedding_dim, window=5, min_count=1, sg=1, seed=42)\n",
    "\n",
    "# Step 3: Prepare embedding matrix\n",
    "vocab_size = len(fasttext_model.wv.key_to_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Map FastText vectors to the embedding matrix\n",
    "word_index = {word: idx + 1 for idx, word in enumerate(fasttext_model.wv.key_to_index)}\n",
    "for word, idx in word_index.items():\n",
    "    embedding_matrix[idx] = fasttext_model.wv[word]\n",
    "\n",
    "# Step 4: Convert reviews to sequences of word indices\n",
    "sequences = [[word_index.get(word, 0) for word in review] for review in tokenized_reviews]\n",
    "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Labels\n",
    "sentiment_dict = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = data['sentiment'].map(sentiment_dict).values\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights_values = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
    "class_weights = {i: class_weights_values[i] for i in range(len(class_weights_values))}\n",
    "\n",
    "# Define stratified 5-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X_padded, y)):\n",
    "    print(f\"\\nTraining fold {fold + 1}...\\n\")\n",
    "    \n",
    "    X_train, X_test = X_padded[train_index], X_padded[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Define the model architecture with one LSTM layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
    "                        weights=[embedding_matrix], input_length=max_sequence_length, trainable=True))\n",
    "    model.add(LSTM(64, activation='tanh', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(l2_lambda)))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model with early stopping and class weights\n",
    "    model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=10, \n",
    "        batch_size=128,  \n",
    "        validation_split=0.2, \n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "    \n",
    "    # Predictions and evaluation for the current fold\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    \n",
    "    # Calculate metrics for the current fold\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, output_dict=True)\n",
    "    f1 = report['weighted avg']['f1-score']\n",
    "    \n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Fold {fold + 1} F1 Score: {f1:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive'], zero_division=0, digits=4))\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(\"\\nAverage Metrics across folds:\")\n",
    "print(f\"Average Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Hashing Vectorization\n",
    "\n",
    "The text data (texts) is transformed using HashingVectorizer with n_features=5000, meaning each document is represented as a vector of 5000 features.\n",
    "The toarray() method converts the sparse matrix to a dense format for compatibility with the model.\n",
    "The transformed data (X) is then reshaped into a 3D array suitable for input into the LSTM (samples, timesteps, features).\n",
    "\n",
    "Hashing Vectorizer is much faster than the Tokenizer and Embedding approach from above code.\n",
    "\n",
    "Hashing Vectorizer directly transforms text into fixed-length numerical vectors by hsahing the terms and mapping them to a specified number of features. This eliminate the need to build a vocabulary or convert tokens into embeddings. Whereas in Tokenizer, it creates a vocabulary, then tokenizes the text into sequences of integers, which are then converted into dense vectors using an `Embedding` layer. This two-step process is more computationally intensive than direct hashing.\n",
    "\n",
    "The model using Hashing Vectorization outperformed the one with basic tokenization and an embedding layer because it provided a more diverse feature space, which allowed the LSTM to better capture complex sequential relationships in the text. The hashing approach created fixed-size, distributed representations without the need for a vocabulary, potentially capturing unique and distinguishable text features more effectively. This improved the model’s ability to generalize on unseen data, resulting in higher test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "116/116 [==============================] - 4s 14ms/step - loss: 0.7579 - accuracy: 0.7087 - val_loss: 0.5006 - val_accuracy: 0.8264\n",
      "Epoch 2/10\n",
      "116/116 [==============================] - 1s 9ms/step - loss: 0.4018 - accuracy: 0.8495 - val_loss: 0.4057 - val_accuracy: 0.8399\n",
      "Epoch 3/10\n",
      "116/116 [==============================] - 1s 8ms/step - loss: 0.3350 - accuracy: 0.8677 - val_loss: 0.3960 - val_accuracy: 0.8410\n",
      "Epoch 4/10\n",
      "116/116 [==============================] - 1s 8ms/step - loss: 0.2889 - accuracy: 0.8832 - val_loss: 0.4136 - val_accuracy: 0.8361\n",
      "Epoch 5/10\n",
      "116/116 [==============================] - 1s 8ms/step - loss: 0.2480 - accuracy: 0.9054 - val_loss: 0.4106 - val_accuracy: 0.8459\n",
      "Epoch 6/10\n",
      "116/116 [==============================] - 1s 8ms/step - loss: 0.2180 - accuracy: 0.9204 - val_loss: 0.4253 - val_accuracy: 0.8562\n",
      "Epoch 7/10\n",
      "116/116 [==============================] - 1s 8ms/step - loss: 0.1841 - accuracy: 0.9350 - val_loss: 0.4719 - val_accuracy: 0.8448\n",
      "Epoch 8/10\n",
      "116/116 [==============================] - 1s 9ms/step - loss: 0.1597 - accuracy: 0.9468 - val_loss: 0.5339 - val_accuracy: 0.8464\n",
      "Epoch 9/10\n",
      "116/116 [==============================] - 1s 9ms/step - loss: 0.1379 - accuracy: 0.9536 - val_loss: 0.5621 - val_accuracy: 0.8470\n",
      "Epoch 10/10\n",
      "116/116 [==============================] - 1s 9ms/step - loss: 0.1249 - accuracy: 0.9601 - val_loss: 0.6027 - val_accuracy: 0.8361\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5802 - accuracy: 0.8351\n",
      "Test Accuracy: 0.8351\n",
      "72/72 [==============================] - 1s 3ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7457    0.8234    0.7826       470\n",
      "     Neutral     0.3673    0.3640    0.3656       228\n",
      "    Positive     0.9326    0.9054    0.9188      1606\n",
      "\n",
      "    accuracy                         0.8351      2304\n",
      "   macro avg     0.6819    0.6976    0.6890      2304\n",
      "weighted avg     0.8386    0.8351    0.8363      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Reshape\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('final_df.csv')\n",
    "\n",
    "# Preprocess text and labels\n",
    "texts = data['processed_full_review'].astype(str)\n",
    "labels = data['sentiment']\n",
    "\n",
    "# Encode labels (e.g., Positive=2, Negative=0, Neutral=1)\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Use Hashing Vectorizer\n",
    "vectorizer = HashingVectorizer(n_features=5000, alternate_sign=False)  # Set n_features as needed\n",
    "X = vectorizer.transform(texts).toarray()\n",
    "\n",
    "# Reshape to 3D array as expected by LSTM input (samples, timesteps, features)\n",
    "X = np.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))  # 3 classes for Positive, Negative, Neutral\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate predictions for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM + Hashing Vectorizer + GridSearch CV\n",
    "\n",
    "### 1. **Defining the Model**:\n",
    "- A function `create_model()` is defined that builds and compiles an LSTM model with configurable hyperparameters (`units`, `dropout_rate`, and `optimizer`).\n",
    "- The model consists of:\n",
    "  - Two LSTM layers, each with a specified number of units.\n",
    "  - Dropout layers to prevent overfitting.\n",
    "  - A Dense layer with `tanh` activation.\n",
    "  - A final Dense output layer with `softmax` activation for multi-class classification.\n",
    "\n",
    "### 2. **Model Wrapping**:\n",
    "- The LSTM model is wrapped with `KerasClassifier` (from `scikeras`) to make it compatible with `GridSearchCV`. This wrapper allows the custom LSTM model to behave like a scikit-learn classifier, enabling hyperparameter tuning.\n",
    "\n",
    "### 3. **Hyperparameter Grid**:\n",
    "- The `param_grid` dictionary specifies the hyperparameters to be tuned and their possible values:\n",
    "  - `'model__units'`: Number of units in the LSTM layers (e.g., [32, 64]).\n",
    "  - `'model__dropout_rate'`: Dropout rate to apply after LSTM and Dense layers (e.g., [0.3, 0.5]).\n",
    "  - `'optimizer'`: Optimization algorithm for training the model (e.g., ['adam', 'rmsprop']).\n",
    "  - `'epochs'`: Number of training epochs (e.g., [5, 10]).\n",
    "\n",
    "### 4. **Grid Search Setup**:\n",
    "- `GridSearchCV` is initialized with:\n",
    "  - `estimator=model`: The wrapped LSTM model.\n",
    "  - `param_grid=param_grid`: The defined grid of hyperparameters.\n",
    "  - `cv=3`: Specifies 3-fold cross-validation, meaning the training data is split into 3 parts, and the model is trained and validated 3 times, each with a different fold as the validation set.\n",
    "- This means for each combination of hyperparameters, the model is trained and evaluated three times, and the average performance score across the folds is recorded.\n",
    "\n",
    "### 5. **Performing Grid Search**:\n",
    "- `grid.fit(X_train, y_train)` performs the grid search. For each hyperparameter combination, the model is:\n",
    "  - Trained on the training set (with cross-validation applied).\n",
    "  - Evaluated using cross-validation to find the average accuracy for that combination.\n",
    "- The process continues until all combinations in `param_grid` are tested.\n",
    "\n",
    "### 6. **Output**:\n",
    "- `grid_result.best_params_` displays the combination of hyperparameters that achieved the best average cross-validation score.\n",
    "- `grid_result.best_score_` shows the highest cross-validation accuracy achieved.\n",
    "- The best model (`best_model = grid_result.best_estimator_`) is used to make predictions on the test set (`X_test`), and a classification report is printed to show performance metrics such as precision, recall, and F1-score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Using GridSearchCV with the LSTM model combined with Hashing Vectorization leads to even better performance because it optimizes the hyperparameters of the model more effectively. GridSearchCV performs an exhaustive search over a specified parameter grid, testing different combinations of hyperparameters such as the number of LSTM units, dropout rate, and optimizer type. This systematic approach finds the most optimal configuration that maximizes model performance on the validation set, resulting in improved generalization and accuracy on the test set. By fine-tuning critical parameters, the model adapts more precisely to the data's characteristics, enhancing its predictive power and robustness compared to models trained with default or manually chosen hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 8ms/step - loss: 0.9675 - accuracy: 0.6768\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7407 - accuracy: 0.6846\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.5302 - accuracy: 0.7877\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3914 - accuracy: 0.8549\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3455 - accuracy: 0.8650\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 3s 8ms/step - loss: 0.9732 - accuracy: 0.6777\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7358 - accuracy: 0.6855\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.5318 - accuracy: 0.7980\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3959 - accuracy: 0.8528\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3489 - accuracy: 0.8647\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9704 - accuracy: 0.6783\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7375 - accuracy: 0.6855\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.5365 - accuracy: 0.7861\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4032 - accuracy: 0.8514\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3596 - accuracy: 0.8615\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9707 - accuracy: 0.6768\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7574 - accuracy: 0.6845\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.5725 - accuracy: 0.7401\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4259 - accuracy: 0.8378\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3606 - accuracy: 0.8605\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9519 - accuracy: 0.6774\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7363 - accuracy: 0.6852\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.5275 - accuracy: 0.7947\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3955 - accuracy: 0.8528\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3486 - accuracy: 0.8634\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9699 - accuracy: 0.6790\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7643 - accuracy: 0.6845\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.5954 - accuracy: 0.7238\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4340 - accuracy: 0.8353\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3636 - accuracy: 0.8597\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 8ms/step - loss: 0.8993 - accuracy: 0.6817\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.6015 - accuracy: 0.7397\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3880 - accuracy: 0.8535\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3258 - accuracy: 0.8662\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2833 - accuracy: 0.8769\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 8ms/step - loss: 0.9017 - accuracy: 0.6772\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.6121 - accuracy: 0.7412\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3943 - accuracy: 0.8489\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3362 - accuracy: 0.8662\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2984 - accuracy: 0.8747\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9167 - accuracy: 0.6772\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.6480 - accuracy: 0.7171\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4131 - accuracy: 0.8439\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3438 - accuracy: 0.8616\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2973 - accuracy: 0.8745\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 3s 8ms/step - loss: 0.9118 - accuracy: 0.6775\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.5951 - accuracy: 0.7460\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3823 - accuracy: 0.8540\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3251 - accuracy: 0.8670\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2851 - accuracy: 0.8776\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 8ms/step - loss: 0.8973 - accuracy: 0.6777\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.5801 - accuracy: 0.7594\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3826 - accuracy: 0.8527\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3268 - accuracy: 0.8662\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2908 - accuracy: 0.8755\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 3s 8ms/step - loss: 0.9096 - accuracy: 0.6798\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.6109 - accuracy: 0.7405\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3961 - accuracy: 0.8478\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3358 - accuracy: 0.8646\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2909 - accuracy: 0.8758\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9751 - accuracy: 0.6801\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7606 - accuracy: 0.6843\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.5615 - accuracy: 0.7796\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4202 - accuracy: 0.8530\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3756 - accuracy: 0.8611\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9924 - accuracy: 0.6803\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7649 - accuracy: 0.6865\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.5579 - accuracy: 0.7918\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4221 - accuracy: 0.8462\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3765 - accuracy: 0.8597\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9832 - accuracy: 0.6777\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7819 - accuracy: 0.6845\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.6148 - accuracy: 0.7316\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4574 - accuracy: 0.8286\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3946 - accuracy: 0.8486\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 3s 7ms/step - loss: 0.9829 - accuracy: 0.6757\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7700 - accuracy: 0.6846\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.5594 - accuracy: 0.7756\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.4226 - accuracy: 0.8492\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3779 - accuracy: 0.8592\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9922 - accuracy: 0.6795\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7797 - accuracy: 0.6845\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.6052 - accuracy: 0.7498\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4408 - accuracy: 0.8457\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3816 - accuracy: 0.8569\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9607 - accuracy: 0.6783\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7666 - accuracy: 0.6845\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.5822 - accuracy: 0.7519\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4363 - accuracy: 0.8384\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3809 - accuracy: 0.8566\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 8ms/step - loss: 0.9248 - accuracy: 0.6744\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.6627 - accuracy: 0.7110\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.4195 - accuracy: 0.8422\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 0.3495 - accuracy: 0.8626\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3133 - accuracy: 0.8715\n",
      "24/24 [==============================] - 1s 5ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 3s 7ms/step - loss: 0.9106 - accuracy: 0.6787\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.6091 - accuracy: 0.7501\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3988 - accuracy: 0.8501\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3499 - accuracy: 0.8647\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3119 - accuracy: 0.8717\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 8ms/step - loss: 0.9254 - accuracy: 0.6785\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.6705 - accuracy: 0.7089\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4361 - accuracy: 0.8372\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3672 - accuracy: 0.8574\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3256 - accuracy: 0.8694\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9050 - accuracy: 0.6775\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.6132 - accuracy: 0.7384\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4003 - accuracy: 0.8494\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3457 - accuracy: 0.8642\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3068 - accuracy: 0.8735\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 8ms/step - loss: 0.9159 - accuracy: 0.6785\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.6357 - accuracy: 0.7355\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.4119 - accuracy: 0.8473\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3521 - accuracy: 0.8607\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3165 - accuracy: 0.8711\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/5\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9208 - accuracy: 0.6769\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.6432 - accuracy: 0.7290\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4201 - accuracy: 0.8437\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3592 - accuracy: 0.8590\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3186 - accuracy: 0.8703\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9578 - accuracy: 0.6762\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7378 - accuracy: 0.6845\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.5246 - accuracy: 0.7883\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3927 - accuracy: 0.8523\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3483 - accuracy: 0.8642\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3134 - accuracy: 0.8709\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2884 - accuracy: 0.8782\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2634 - accuracy: 0.8864\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2428 - accuracy: 0.9026\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2167 - accuracy: 0.9186\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9615 - accuracy: 0.6765\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7286 - accuracy: 0.6876\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.5057 - accuracy: 0.8095\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3850 - accuracy: 0.8540\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3441 - accuracy: 0.8652\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3121 - accuracy: 0.8732\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2875 - accuracy: 0.8773\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2641 - accuracy: 0.8841\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2447 - accuracy: 0.8978\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2260 - accuracy: 0.9110\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9692 - accuracy: 0.6783\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7401 - accuracy: 0.6853\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.5344 - accuracy: 0.7859\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.4046 - accuracy: 0.8494\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3588 - accuracy: 0.8611\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3257 - accuracy: 0.8694\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.2987 - accuracy: 0.8774\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2780 - accuracy: 0.8823\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 1s 13ms/step - loss: 0.2558 - accuracy: 0.8901\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.2333 - accuracy: 0.9088\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 3s 8ms/step - loss: 0.9847 - accuracy: 0.6767\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.7315 - accuracy: 0.6879\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.4942 - accuracy: 0.8115\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3777 - accuracy: 0.8557\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3400 - accuracy: 0.8660\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3095 - accuracy: 0.8738\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2862 - accuracy: 0.8803\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2631 - accuracy: 0.8838\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2429 - accuracy: 0.8934\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2205 - accuracy: 0.9093\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 3s 8ms/step - loss: 1.0027 - accuracy: 0.6767\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.7693 - accuracy: 0.6845\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.5741 - accuracy: 0.7671\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4096 - accuracy: 0.8475\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3576 - accuracy: 0.8605\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3215 - accuracy: 0.8691\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2956 - accuracy: 0.8764\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2691 - accuracy: 0.8913\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2369 - accuracy: 0.9067\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2090 - accuracy: 0.9256\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9671 - accuracy: 0.6783\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7510 - accuracy: 0.6845\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.5537 - accuracy: 0.7701\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4057 - accuracy: 0.8478\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3528 - accuracy: 0.8603\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3249 - accuracy: 0.8701\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2975 - accuracy: 0.8761\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2763 - accuracy: 0.8820\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2542 - accuracy: 0.8901\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2295 - accuracy: 0.9088\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 2s 8ms/step - loss: 0.9079 - accuracy: 0.6793\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.5985 - accuracy: 0.7436\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3823 - accuracy: 0.8541\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3243 - accuracy: 0.8676\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2844 - accuracy: 0.8766\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2517 - accuracy: 0.8886\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2168 - accuracy: 0.9191\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1735 - accuracy: 0.9398\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1447 - accuracy: 0.9544\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1176 - accuracy: 0.9634\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 2s 8ms/step - loss: 0.8965 - accuracy: 0.6832\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.6062 - accuracy: 0.7438\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3952 - accuracy: 0.8506\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3323 - accuracy: 0.8652\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2965 - accuracy: 0.8743\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2658 - accuracy: 0.8833\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2389 - accuracy: 0.8971\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2113 - accuracy: 0.9171\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1753 - accuracy: 0.9375\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1510 - accuracy: 0.9484\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 4s 8ms/step - loss: 0.9164 - accuracy: 0.6778\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.6109 - accuracy: 0.7395\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3966 - accuracy: 0.8484\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3378 - accuracy: 0.8637\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2965 - accuracy: 0.8747\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2680 - accuracy: 0.8815\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2371 - accuracy: 0.9062\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2017 - accuracy: 0.9294\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1672 - accuracy: 0.9434\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1394 - accuracy: 0.9575\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9158 - accuracy: 0.6762\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.6124 - accuracy: 0.7358\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3853 - accuracy: 0.8514\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3288 - accuracy: 0.8681\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2883 - accuracy: 0.8764\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2613 - accuracy: 0.8872\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2302 - accuracy: 0.9064\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1991 - accuracy: 0.9302\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1696 - accuracy: 0.9415\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1400 - accuracy: 0.9539\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 3s 7ms/step - loss: 0.9196 - accuracy: 0.6778\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.6003 - accuracy: 0.7500\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3897 - accuracy: 0.8517\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3345 - accuracy: 0.8660\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2978 - accuracy: 0.8745\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2689 - accuracy: 0.8830\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2410 - accuracy: 0.8971\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2105 - accuracy: 0.9186\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1782 - accuracy: 0.9346\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.1493 - accuracy: 0.9492\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 2s 8ms/step - loss: 0.9197 - accuracy: 0.6806\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.6526 - accuracy: 0.7119\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.4220 - accuracy: 0.8421\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3470 - accuracy: 0.8613\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3029 - accuracy: 0.8712\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.2733 - accuracy: 0.8795\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.2392 - accuracy: 0.9064\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.1998 - accuracy: 0.9313\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.1673 - accuracy: 0.9404\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1411 - accuracy: 0.9536\n",
      "24/24 [==============================] - 1s 5ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 3s 7ms/step - loss: 0.9804 - accuracy: 0.6754\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.7770 - accuracy: 0.6845\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.5794 - accuracy: 0.7634\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4332 - accuracy: 0.8426\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3823 - accuracy: 0.8588\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3463 - accuracy: 0.8676\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3265 - accuracy: 0.8737\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2962 - accuracy: 0.8772\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2826 - accuracy: 0.8834\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2587 - accuracy: 0.8930\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9587 - accuracy: 0.6775\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7526 - accuracy: 0.6853\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.5515 - accuracy: 0.7846\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.4205 - accuracy: 0.8458\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3713 - accuracy: 0.8597\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3396 - accuracy: 0.8693\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3219 - accuracy: 0.8717\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2982 - accuracy: 0.8786\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2778 - accuracy: 0.8805\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2676 - accuracy: 0.8865\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 1.0007 - accuracy: 0.6756\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7876 - accuracy: 0.6845\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.6304 - accuracy: 0.7299\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4584 - accuracy: 0.8366\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3967 - accuracy: 0.8533\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3636 - accuracy: 0.8620\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3374 - accuracy: 0.8699\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3106 - accuracy: 0.8732\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2920 - accuracy: 0.8836\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2728 - accuracy: 0.8981\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9873 - accuracy: 0.6767\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.7825 - accuracy: 0.6843\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.6043 - accuracy: 0.7392\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.4447 - accuracy: 0.8347\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3846 - accuracy: 0.8548\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3466 - accuracy: 0.8670\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3274 - accuracy: 0.8724\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2953 - accuracy: 0.8766\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2792 - accuracy: 0.8831\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2548 - accuracy: 0.8989\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 2s 8ms/step - loss: 0.9804 - accuracy: 0.6790\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.7710 - accuracy: 0.6845\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.5913 - accuracy: 0.7573\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.4408 - accuracy: 0.8364\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3805 - accuracy: 0.8592\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3491 - accuracy: 0.8667\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3286 - accuracy: 0.8714\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3037 - accuracy: 0.8768\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2893 - accuracy: 0.8790\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2784 - accuracy: 0.8841\n",
      "24/24 [==============================] - 1s 3ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 2s 7ms/step - loss: 0.9784 - accuracy: 0.6767\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.7877 - accuracy: 0.6845\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.6493 - accuracy: 0.7029\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4822 - accuracy: 0.8069\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.4066 - accuracy: 0.8439\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.3680 - accuracy: 0.8571\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3411 - accuracy: 0.8662\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3187 - accuracy: 0.8717\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2941 - accuracy: 0.8828\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.2756 - accuracy: 0.8921\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 3s 8ms/step - loss: 0.9245 - accuracy: 0.6763\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.6309 - accuracy: 0.7330\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.4078 - accuracy: 0.8481\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3515 - accuracy: 0.8645\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3080 - accuracy: 0.8717\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2811 - accuracy: 0.8836\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2500 - accuracy: 0.9018\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2142 - accuracy: 0.9254\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.1866 - accuracy: 0.9359\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1599 - accuracy: 0.9456\n",
      "24/24 [==============================] - 1s 5ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 2s 8ms/step - loss: 0.9108 - accuracy: 0.6790\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.6347 - accuracy: 0.7351\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.4116 - accuracy: 0.8473\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3505 - accuracy: 0.8621\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3198 - accuracy: 0.8707\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.2907 - accuracy: 0.8790\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.2674 - accuracy: 0.8898\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.2445 - accuracy: 0.9043\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2151 - accuracy: 0.9178\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1936 - accuracy: 0.9298\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 2s 8ms/step - loss: 0.9198 - accuracy: 0.6770\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.6330 - accuracy: 0.7338\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.4151 - accuracy: 0.8467\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3593 - accuracy: 0.8600\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3199 - accuracy: 0.8711\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2922 - accuracy: 0.8758\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2670 - accuracy: 0.8916\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2423 - accuracy: 0.9043\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2144 - accuracy: 0.9235\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.1878 - accuracy: 0.9357\n",
      "24/24 [==============================] - 1s 5ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 3s 9ms/step - loss: 0.9266 - accuracy: 0.6763\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.6502 - accuracy: 0.7167\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.4194 - accuracy: 0.8408\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3554 - accuracy: 0.8624\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3141 - accuracy: 0.8720\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2853 - accuracy: 0.8772\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2631 - accuracy: 0.8885\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.2352 - accuracy: 0.9033\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.2158 - accuracy: 0.9162\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.1922 - accuracy: 0.9321\n",
      "24/24 [==============================] - 1s 5ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 3s 9ms/step - loss: 0.9123 - accuracy: 0.6774\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.6531 - accuracy: 0.7198\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.4171 - accuracy: 0.8457\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3567 - accuracy: 0.8623\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.3200 - accuracy: 0.8706\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2905 - accuracy: 0.8782\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.2629 - accuracy: 0.8932\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.2358 - accuracy: 0.9093\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2038 - accuracy: 0.9220\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 0.1810 - accuracy: 0.9289\n",
      "24/24 [==============================] - 1s 4ms/step\n",
      "Epoch 1/10\n",
      "48/48 [==============================] - 2s 8ms/step - loss: 0.9302 - accuracy: 0.6780\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.6769 - accuracy: 0.7062\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.4415 - accuracy: 0.8369\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.3654 - accuracy: 0.8584\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.3280 - accuracy: 0.8698\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2990 - accuracy: 0.8745\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2730 - accuracy: 0.8913\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2399 - accuracy: 0.9064\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.2110 - accuracy: 0.9230\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.1849 - accuracy: 0.9344\n",
      "24/24 [==============================] - 1s 5ms/step\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 2s 7ms/step - loss: 0.8951 - accuracy: 0.6764\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.6014 - accuracy: 0.7408\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.4111 - accuracy: 0.8500\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.3524 - accuracy: 0.8611\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.3254 - accuracy: 0.8687\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.3008 - accuracy: 0.8748\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.2807 - accuracy: 0.8789\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.2611 - accuracy: 0.8886\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.2428 - accuracy: 0.9007\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.2248 - accuracy: 0.9147\n",
      "Best Parameters: {'epochs': 10, 'model__dropout_rate': 0.3, 'model__units': 32, 'optimizer': 'rmsprop'}\n",
      "Best Score: 0.8518564333346901\n",
      "18/18 [==============================] - 1s 3ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7590    0.8043    0.7810       470\n",
      "     Neutral     0.3037    0.1798    0.2259       228\n",
      "    Positive     0.9120    0.9489    0.9301      1606\n",
      "\n",
      "    accuracy                         0.8433      2304\n",
      "   macro avg     0.6583    0.6443    0.6457      2304\n",
      "weighted avg     0.8206    0.8433    0.8300      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding\n",
    "from scikeras.wrappers import KerasClassifier  # Updated import\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('final_df.csv')\n",
    "\n",
    "# Preprocess text and labels\n",
    "texts = data['processed_full_review'].astype(str)\n",
    "labels = data['sentiment']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Use Hashing Vectorizer\n",
    "vectorizer = HashingVectorizer(n_features=5000, alternate_sign=False)  # Set n_features as needed\n",
    "X = vectorizer.transform(texts).toarray()\n",
    "X = np.reshape(X, (X.shape[0], 1, X.shape[1]))  # Reshape for LSTM\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a function to create the model (for use in KerasClassifier)\n",
    "def create_model(units=64, dropout_rate=0.5, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units, return_sequences=False))\n",
    "    model.add(Dense(units // 2, activation='tanh'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(3, activation='softmax'))  # 3 classes for Positive, Negative, Neutral\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the model using KerasClassifier from scikeras\n",
    "model = KerasClassifier(model=create_model, verbose=1, batch_size=128)  # scikeras syntax\n",
    "\n",
    "# Define the grid of hyperparameters\n",
    "param_grid = {\n",
    "    'model__units': [32, 64],\n",
    "    'model__dropout_rate': [0.3, 0.5],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'epochs': [5, 10],  # Reduced for demo; increase as needed\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "\n",
    "# Perform grid search\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Display the best parameters and accuracy\n",
    "print(\"Best Parameters:\", grid_result.best_params_)\n",
    "print(\"Best Score:\", grid_result.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_result.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
