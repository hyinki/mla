{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Singapore Airlines' Service Through Automated Sentiment Analysis of Customer Reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singapore Airlines Customer Reviews Dataset Information\n",
    "\n",
    "The [Singapore Airlines Customer Reviews Dataset](https://www.kaggle.com/datasets/kanchana1990/singapore-airlines-reviews) aggregates 10,000 anonymized customer reviews, providing a broad perspective on the passenger experience with Singapore Airlines. \n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`published_date`**: Date and time of review publication.\n",
    "- **`published_platform`**: Platform where the review was posted.\n",
    "- **`rating`**: Customer satisfaction rating, from 1 (lowest) to 5 (highest).\n",
    "- **`type`**: Specifies the content as a review.\n",
    "- **`text`**: Detailed customer feedback.\n",
    "- **`title`**: Summary of the review.\n",
    "- **`helpful_votes`**: Number of users finding the review helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional web scraping of online reviews\n",
    "\n",
    "During our EDA, we noticed two main trends in the distribution of our dataset:\n",
    "1. Less than 10% of our reviews were published from the years 2022 to 2024, making it hard for us to capture recent trends in sentiment.\n",
    "2. Most of the reviews were highly positive, which could mean that SIA had mostly positive reviews, nevertheless we wanted to get more information on negative reviews to improve the robustness of our model.\n",
    "\n",
    "### TripAdvisor\n",
    "\n",
    "We scraped more data for airline reviews from TripAdvisor, specifically for the years 2022 to 2024. \n",
    "(https://www.tripadvisor.com.sg/Airline_Review-d8729151-Reviews-Singapore-Airlines)\n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`Year`**: Year of review publication.\n",
    "- **`Month`**: Month of review publication.\n",
    "- **`Title`**: Title of review publication.\n",
    "- **`Review Text`**: Main text content of review publication.\n",
    "- **`Rating`**: Numerical rating provided by reviewer (Scale: 1 to 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skytrax\n",
    "\n",
    "We also scraped from Skytrax, which is another data source for online reviews. \n",
    "(https://www.airlinequality.com/airline-reviews/singapore-airlines/?sortby=post_date%3ADesc&pagesize=100)\n",
    "\n",
    "The dimensions are shown below:\n",
    "- **`Year`**: Year of review publication.\n",
    "- **`Month`**: Month of review publication.\n",
    "- **`Title`**: Title of review publication.\n",
    "- **`Review Text`**: Main text content of review publication.\n",
    "- **`Rating`**: Numerical rating provided by reviewer (Scale: 1 to 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Please uncomment the code box below to pip install relevant dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=2.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.24.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 4)) (4.66.5)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 5)) (3.9.2)\n",
      "Requirement already satisfied: seaborn>=0.12.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
      "Requirement already satisfied: langdetect>=1.0.9 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 7)) (1.0.9)\n",
      "Requirement already satisfied: langid>=1.1.6 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 8)) (1.1.6)\n",
      "Requirement already satisfied: nltk>=3.8.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 9)) (3.9.1)\n",
      "Requirement already satisfied: wordcloud>=1.9.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 10)) (1.9.3)\n",
      "Collecting tensorflow-gpu>=2.11.0 (from -r requirements.txt (line 11))\n",
      "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [44 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\packaging\\requirements.py\", line 36, in __init__\n",
      "          parsed = _parse_requirement(requirement_string)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\packaging\\_parser.py\", line 62, in parse_requirement\n",
      "          return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\packaging\\_parser.py\", line 80, in _parse_requirement\n",
      "          url, specifier, marker = _parse_requirement_details(tokenizer)\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\packaging\\_parser.py\", line 124, in _parse_requirement_details\n",
      "          marker = _parse_requirement_marker(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\packaging\\_parser.py\", line 145, in _parse_requirement_marker\n",
      "          tokenizer.raise_syntax_error(\n",
      "        File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\packaging\\_tokenizer.py\", line 167, in raise_syntax_error\n",
      "          raise ParserSyntaxError(\n",
      "      packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)\n",
      "          python_version>\"3.7\"\n",
      "                        ^\n",
      "      \n",
      "      The above exception was the direct cause of the following exception:\n",
      "      \n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-install-a5jns6it\\tensorflow-gpu_f404bc66823d4bcfb8a232f0b276ddd8\\setup.py\", line 40, in <module>\n",
      "          setuptools.setup()\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\setuptools\\__init__.py\", line 116, in setup\n",
      "          _install_setup_requires(attrs)\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\setuptools\\__init__.py\", line 87, in _install_setup_requires\n",
      "          dist.parse_config_files(ignore_option_errors=True)\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\setuptools\\dist.py\", line 610, in parse_config_files\n",
      "          self._finalize_requires()\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\setuptools\\dist.py\", line 344, in _finalize_requires\n",
      "          self._normalize_requires()\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\setuptools\\dist.py\", line 359, in _normalize_requires\n",
      "          self.install_requires = list(map(str, _reqs.parse(install_requires)))\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\packaging\\requirements.py\", line 38, in __init__\n",
      "          raise InvalidRequirement(str(e)) from e\n",
      "      packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)\n",
      "          python_version>\"3.7\"\n",
      "                        ^\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "import nltk\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# For generating n-grams\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation (Loading CSV)\n",
    "\n",
    "Load the three CSV files into a pandas DataFrame `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processed_full_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>ok use airlin go singapor london heathrow issu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>don give money book paid receiv email confirm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "      <td>best airlin world best airlin world seat food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>premium economi seat singapor airlin not worth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>imposs get promis refund book flight full mont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month sentiment                              processed_full_review\n",
       "0  2024      3   Neutral  ok use airlin go singapor london heathrow issu...\n",
       "1  2024      3  Negative  don give money book paid receiv email confirm ...\n",
       "2  2024      3  Positive  best airlin world best airlin world seat food ...\n",
       "3  2024      3  Negative  premium economi seat singapor airlin not worth...\n",
       "4  2024      3  Negative  imposs get promis refund book flight full mont..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Positive    7913\n",
       "Negative    2441\n",
       "Neutral     1164\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2019    5129\n",
       "2018    2596\n",
       "2022    1184\n",
       "2023    1111\n",
       "2020     888\n",
       "2024     514\n",
       "2021      96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "A Convolutional Neural Network (CNN) is a type of deep learning model that is particularly effective for pattern recognition tasks, especially in images and, increasingly, in text. Here’s how a CNN works in principle, broken down into its key components.\n",
    "\n",
    "Below is an explanation of how a basic CNN works:\n",
    "\n",
    "1. Convolutional Layer:\n",
    "\t- A CNN’s core layer is the convolutional layer, which applies filters (kernels) to small regions of the input data.\n",
    "\t- For text, a convolutional layer slides filters over sequences of words or tokens. Each filter is designed to detect specific patterns, such as n-grams (e.g., “not good” or “very interesting”) or word sequences relevant to the task.\n",
    "\t- The convolution operation outputs a feature map, where each entry represents the presence or strength of a detected pattern in a specific region of the input.\n",
    "    \n",
    "2.\tActivation Function (e.g., ReLU):\n",
    "\t- After convolution, an activation function (like ReLU) is applied to introduce non-linearity, allowing the network to model complex patterns.\n",
    "\t- This function essentially “activates” certain features, helping the network focus on meaningful patterns while ignoring less relevant details.\n",
    "\n",
    "3.\tPooling Layer:\n",
    "\t- A pooling layer (often Global Max Pooling for text data) is applied after convolution to reduce the dimensionality of the feature map, keeping only the most important features.\n",
    "\t- Pooling helps make the network more robust to minor variations and reduces the number of parameters, which speeds up training and helps prevent overfitting.\n",
    "\n",
    "4.\tFully Connected (Dense) Layer:\n",
    "\t- The pooled features are then passed through one or more fully connected (dense) layers. These layers process the extracted features, combining them to make predictions.\n",
    "\t- In text classification, a final dense layer with a sigmoid or softmax activation is often used to produce a probability score or class label for each input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Convolutional Neural Network\n",
    "\n",
    "- For our task of fake news classification, we add an embedding layer before the convolution layer. An embedding layer is often included to convert words into dense, continuous vector representations (embeddings) that capture semantic relationships.\n",
    "\n",
    "- An embedding layer provides input that’s suitable for convolution by encoding words as vectors. This way, the CNN can capture patterns in these representations rather than working with raw token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.7104 - loss: 0.7826 - val_accuracy: 0.8302 - val_loss: 0.4405\n",
      "Epoch 2/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8518 - loss: 0.3861 - val_accuracy: 0.8470 - val_loss: 0.3685\n",
      "Epoch 3/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9005 - loss: 0.2548 - val_accuracy: 0.8497 - val_loss: 0.3724\n",
      "Epoch 4/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9416 - loss: 0.1671 - val_accuracy: 0.8459 - val_loss: 0.4221\n",
      "Epoch 5/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9629 - loss: 0.1227 - val_accuracy: 0.8535 - val_loss: 0.4701\n",
      "Epoch 6/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9806 - loss: 0.0754 - val_accuracy: 0.8562 - val_loss: 0.5309\n",
      "Epoch 7/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9924 - loss: 0.0354 - val_accuracy: 0.8508 - val_loss: 0.6015\n",
      "Epoch 8/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9963 - loss: 0.0208 - val_accuracy: 0.8638 - val_loss: 0.6149\n",
      "Epoch 9/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9977 - loss: 0.0157 - val_accuracy: 0.8578 - val_loss: 0.6472\n",
      "Epoch 10/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9984 - loss: 0.0128 - val_accuracy: 0.8638 - val_loss: 0.7083\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\n",
      "Evaluation Metrics:\n",
      "Accuracy: 0.8641\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8595    0.7681    0.8112       470\n",
      "     Neutral     0.4259    0.4912    0.4562       228\n",
      "    Positive     0.9365    0.9452    0.9408      1606\n",
      "\n",
      "    accuracy                         0.8641      2304\n",
      "   macro avg     0.7406    0.7348    0.7361      2304\n",
      "weighted avg     0.8702    0.8641    0.8664      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming 'data' is your DataFrame with 'processed_full_review' and 'sentiment' columns\n",
    "\n",
    "# Step 1: Tokenization and Padding\n",
    "max_words = 10000  # Maximum vocabulary size\n",
    "max_sequence_length = 300  # Maximum length of sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['processed_full_review'])\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_full_review'])\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# One-hot encode the sentiment labels\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False) \n",
    "y = onehot_encoder.fit_transform(data[['sentiment']])\n",
    "\n",
    "# Step 2: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Define the Basic CNN Model\n",
    "def create_basic_cnn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding layer\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=128))\n",
    "    \n",
    "    # Convolutional layer\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "\n",
    "    # Max pooling layer\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    \n",
    "    # Fully connected layer\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))  # Dropout for regularization\n",
    "    \n",
    "    # Output layer for three-class classification using softmax\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # Categorical cross-entropy is used for multi-class classification\n",
    "    return model\n",
    "\n",
    "# Step 4: Train the Model\n",
    "model = create_basic_cnn()\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1) #argmax to convert one-hot encoded output to label\n",
    "y_true = np.argmax(y_test, axis=1) #argmax to convert one-hot encoded output to label\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred, target_names=onehot_encoder.categories_[0],digits=4)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network + TF-IDF Vectorizer\n",
    "\n",
    "Using TF-IDF vectorizer along with CNN led to a drastic fall in performance. Below are some reasons why we should not use TF-IDF vectorizer along with a CNN or other neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lack of Spatial Structure:\n",
    "\n",
    "TF-IDF vectors are sparse and non-sequential representations where each position in the vector represents a word, not a spatial pattern.\n",
    "CNNs are designed to detect patterns in sequential or spatially structured data (e.g., images or sentences), so they might struggle to find meaningful patterns in TF-IDF vectors.\n",
    "\n",
    "#### High-Dimensional Sparse Data:\n",
    "\n",
    "TF-IDF vectors, especially with a high max_features value (like 10,000), result in a high-dimensional but sparse input.\n",
    "CNNs are generally not well-suited for such high-dimensional sparse data; they perform better with dense embeddings where words have contextually meaningful dimensions.\n",
    "\n",
    "#### Mismatch Between Input Type and CNN Architecture:\n",
    "\n",
    "CNNs are typically effective when applied to word embeddings (like GloVe or Word2Vec) because embeddings maintain semantic relationships and neighborhood structures.\n",
    "TF-IDF, however, does not capture word order or semantic relationships, which means the convolution operation might not yield meaningful feature maps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 154ms/step - accuracy: 0.6738 - loss: 0.9241 - val_accuracy: 0.6636 - val_loss: 0.8511\n",
      "Epoch 2/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 151ms/step - accuracy: 0.6994 - loss: 0.8191 - val_accuracy: 0.6636 - val_loss: 0.8503\n",
      "Epoch 3/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 151ms/step - accuracy: 0.6926 - loss: 0.8250 - val_accuracy: 0.6636 - val_loss: 0.8525\n",
      "Epoch 4/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 152ms/step - accuracy: 0.6873 - loss: 0.8257 - val_accuracy: 0.6636 - val_loss: 0.8506\n",
      "Epoch 5/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 151ms/step - accuracy: 0.6925 - loss: 0.8239 - val_accuracy: 0.6636 - val_loss: 0.8505\n",
      "Epoch 6/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 150ms/step - accuracy: 0.6901 - loss: 0.8290 - val_accuracy: 0.6636 - val_loss: 0.8510\n",
      "Epoch 7/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 149ms/step - accuracy: 0.6851 - loss: 0.8230 - val_accuracy: 0.6636 - val_loss: 0.8525\n",
      "Epoch 8/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 148ms/step - accuracy: 0.6937 - loss: 0.8171 - val_accuracy: 0.6636 - val_loss: 0.8557\n",
      "Epoch 9/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 148ms/step - accuracy: 0.6840 - loss: 0.8319 - val_accuracy: 0.6636 - val_loss: 0.8514\n",
      "Epoch 10/10\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 151ms/step - accuracy: 0.6957 - loss: 0.8154 - val_accuracy: 0.6636 - val_loss: 0.8507\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\n",
      "Evaluation Metrics:\n",
      "Accuracy: 0.6970\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.0000    0.0000    0.0000       470\n",
      "     Neutral     0.0000    0.0000    0.0000       228\n",
      "    Positive     0.6970    1.0000    0.8215      1606\n",
      "\n",
      "    accuracy                         0.6970      2304\n",
      "   macro avg     0.2323    0.3333    0.2738      2304\n",
      "weighted avg     0.4859    0.6970    0.5726      2304\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout, Reshape, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "\n",
    "# Step 1: Apply TF-IDF Vectorization\n",
    "max_features = 10000  # Limit TF-IDF to top 10,000 features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['processed_full_review']).toarray()\n",
    "\n",
    "# Convert the labels and one-hot encode for categorical cross-entropy\n",
    "y = data['sentiment'].values\n",
    "y = pd.get_dummies(y).values  # Assuming sentiment is a categorical string label\n",
    "\n",
    "# Step 2: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Define the CNN Model for TF-IDF Input\n",
    "def create_cnn_with_tfidf():\n",
    "    inputs = Input(shape=(max_features,))\n",
    "    x = Reshape((max_features, 1))(inputs)  # Reshape TF-IDF output to be compatible with Conv1D\n",
    "\n",
    "    # Convolutional layer\n",
    "    x = Conv1D(filters=128, kernel_size=5, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)  # Dropout for regularization\n",
    "    outputs = Dense(3, activation='softmax')(x)  # Output layer for triple classification\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Step 4: Train the Model\n",
    "model = create_cnn_with_tfidf()\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred, target_names=[\"Negative\",\"Neutral\",\"Positive\"], digits=4)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network + Custom-trained Word2Vec embeddings\n",
    "\n",
    "In this case, we train the embedding layer on our dataset, which allows it to better capture domain-specific vocabulary, as compared to using pre-trained embeddings that are trained on a very large and general corpus.\n",
    "\n",
    "##### 1. Word embeddings capture the semantic relationships between words in a dense, low-dimensional space.\n",
    "Fake news often uses subtle language, and word embeddings like GloVe can capture the semantic context of words, allowing the model to understand relationships between words that simple vectorizers would miss. This helps in detecting nuanced differences in language use between real and fake news.\n",
    "\n",
    "##### 2. Word embeddings produce dense, low-dimensional vectors (e.g., 100-300 dimensions) that capture rich word information.\n",
    "Pre-trained embeddings are built on large corpora like Wikipedia and news articles, giving our model external knowledge that’s useful for distinguishing between real news and fake news. This boosts the model's ability to generalize on unseen test data from our web scraping.\n",
    "\n",
    "##### 3. Efficient Representation of Semantics\n",
    "Words in fake news can appear in different contexts, but with similar underlying meanings (e.g., \"hoax\" and \"lie\"). GloVe embeddings represent these similar words in close proximity in the vector space, helping the model recognize fake news patterns more effectively than TF-IDF or Count Vectorizer.\n",
    "\n",
    "##### 4. Handling Synonyms and Rare Words:\n",
    "Fake news often uses alternative phrases or rare terminology. Pre-trained embeddings like GloVe can handle these rare words because they’ve seen a broad variety of language during training, making our model more robust against unusual vocabulary choices in fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Input\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# # Tokenization parameters\n",
    "# max_words = 10000  # Maximum number of words to keep in the vocabulary\n",
    "# max_sequence_length = 300  # Maximum length of sequences\n",
    "\n",
    "# # Tokenize and create sequences\n",
    "# tokenizer = Tokenizer(num_words=max_words)\n",
    "# tokenizer.fit_on_texts(data['processed_full_review'])\n",
    "# sequences = tokenizer.texts_to_sequences(data['processed_full_review'])\n",
    "# X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "# y = data['sentiment'].values  # Target sentiment labels\n",
    "\n",
    "# # Step 2: Train Word2Vec Embeddings\n",
    "# # Prepare sentences as lists of words for Word2Vec training\n",
    "# sentences = [text.split() for text in data['processed_full_review']]\n",
    "\n",
    "# # Train custom Word2Vec model\n",
    "# embedding_dim = 200  # Set embedding dimension (try 100-200)\n",
    "# custom_word2vec = Word2Vec(sentences, vector_size=embedding_dim, window=5, min_count=2, workers=4)\n",
    "\n",
    "# # Step 3: Create Embedding Matrix from Custom Word2Vec\n",
    "# vocab_size = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
    "# embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# # Map words in tokenizer's vocabulary to the Word2Vec vectors\n",
    "# for word, i in tokenizer.word_index.items():\n",
    "#     if i < max_words:  # Limit to top max_words\n",
    "#         if word in custom_word2vec.wv:\n",
    "#             embedding_matrix[i] = custom_word2vec.wv[word]\n",
    "#         else:\n",
    "#             embedding_matrix[i] = np.random.normal(size=(embedding_dim,))  # Random init for OOV words\n",
    "\n",
    "# # Step 4: Train-Test Split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Step 5: Define CNN Model with Custom Word2Vec Embeddings\n",
    "# def create_cnn_with_custom_word2vec():\n",
    "#     input_layer = Input(shape=(max_sequence_length,))\n",
    "    \n",
    "#     # Embedding layer with custom Word2Vec embeddings\n",
    "#     embedding_layer = Embedding(input_dim=vocab_size,\n",
    "#                                 output_dim=embedding_dim,\n",
    "#                                 weights=[embedding_matrix],\n",
    "#                                 trainable=False)(input_layer)  # Set to non-trainable\n",
    "\n",
    "#     # Convolutional and pooling layers\n",
    "#     x = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
    "#     x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "#     # Fully connected layer with Dropout\n",
    "#     x = Dense(64, activation='relu')(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     output_layer = Dense(3, activation='softmax')(x)  # Output layer for multi-class classification\n",
    "\n",
    "#     # Compile model\n",
    "#     model = Model(inputs=input_layer, outputs=output_layer)\n",
    "#     model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Step 6: Train the CNN Model\n",
    "# model = create_cnn_with_custom_word2vec()\n",
    "# history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n",
    "\n",
    "# # Step 7: Evaluate the Model\n",
    "# y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "\n",
    "# print(\"\\nEvaluation Metrics:\")\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"Precision: {precision:.4f}\")\n",
    "# print(f\"Recall: {recall:.4f}\")\n",
    "# print(f\"F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
