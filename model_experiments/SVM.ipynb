{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# For concurrency (running functions in parallel)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# For caching (to speed up repeated function calls)\n",
    "from functools import lru_cache\n",
    "\n",
    "# For progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting and Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Language Detection packages\n",
    "# `langdetect` for detecting language\n",
    "from langdetect import detect as langdetect_detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "# `langid` for an alternative language detection method\n",
    "from langid import classify as langid_classify\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "# Regular expressions for text pattern matching\n",
    "import re\n",
    "\n",
    "# Word Cloud generation\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# For generating n-grams\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processed_full_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>ok use airlin go singapor london heathrow issu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>don give money book paid receiv email confirm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "      <td>best airlin world best airlin world seat food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>premium economi seat singapor airlin not worth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "      <td>imposs get promis refund book flight full mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11513</th>\n",
       "      <td>2021</td>\n",
       "      <td>11</td>\n",
       "      <td>Negative</td>\n",
       "      <td>websit buggi paid first busi class ticket webs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11514</th>\n",
       "      <td>2021</td>\n",
       "      <td>10</td>\n",
       "      <td>Negative</td>\n",
       "      <td>reduc level qualiti servic fear futur airlin t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11515</th>\n",
       "      <td>2021</td>\n",
       "      <td>10</td>\n",
       "      <td>Negative</td>\n",
       "      <td>chang would cost usd book ticket singapor airl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11516</th>\n",
       "      <td>2021</td>\n",
       "      <td>8</td>\n",
       "      <td>Negative</td>\n",
       "      <td>disappoint flight check secur check frankfurt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11517</th>\n",
       "      <td>2021</td>\n",
       "      <td>5</td>\n",
       "      <td>Negative</td>\n",
       "      <td>frustrat experi tri book flight not never frus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11518 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  month sentiment  \\\n",
       "0      2024      3   Neutral   \n",
       "1      2024      3  Negative   \n",
       "2      2024      3  Positive   \n",
       "3      2024      3  Negative   \n",
       "4      2024      3  Negative   \n",
       "...     ...    ...       ...   \n",
       "11513  2021     11  Negative   \n",
       "11514  2021     10  Negative   \n",
       "11515  2021     10  Negative   \n",
       "11516  2021      8  Negative   \n",
       "11517  2021      5  Negative   \n",
       "\n",
       "                                   processed_full_review  \n",
       "0      ok use airlin go singapor london heathrow issu...  \n",
       "1      don give money book paid receiv email confirm ...  \n",
       "2      best airlin world best airlin world seat food ...  \n",
       "3      premium economi seat singapor airlin not worth...  \n",
       "4      imposs get promis refund book flight full mont...  \n",
       "...                                                  ...  \n",
       "11513  websit buggi paid first busi class ticket webs...  \n",
       "11514  reduc level qualiti servic fear futur airlin t...  \n",
       "11515  chang would cost usd book ticket singapor airl...  \n",
       "11516  disappoint flight check secur check frankfurt ...  \n",
       "11517  frustrat experi tri book flight not never frus...  \n",
       "\n",
       "[11518 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../final_df.csv')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM (linear) + CountVec\n",
    "\n",
    "Linear kernel computes the dot product between 2 vectors, works best for lienarly separated data, more effective when features are numerous, as in text classification, where each word or term often represents a feature in high-dimensional space.\n",
    "\n",
    "Less computationally less intensive and faster to train.\n",
    "\n",
    "Class Imbalance Handling: We set `class_weight='balanced'` to automatically adjust the class weights inversely proportional to the class frequencies in the training data, helping the model pay more attention to minority classes.\n",
    "\n",
    "Stratified K-fold: Maintain class distribution across the folds, which is important for imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores: [0.80575149 0.81117743 0.81117743 0.8046663  0.81541802]\n",
      "Mean Cross-Validation Accuracy: 0.809638135433954\n",
      "SVM(linear) Accuracy: 0.8155381944444444\n",
      "SVM(linear) Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7008    0.7574    0.7280       470\n",
      "     Neutral     0.3514    0.4825    0.4067       228\n",
      "    Positive     0.9528    0.8798    0.9149      1606\n",
      "\n",
      "    accuracy                         0.8155      2304\n",
      "   macro avg     0.6683    0.7066    0.6832      2304\n",
      "weighted avg     0.8419    0.8155    0.8265      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=1000)\n",
    "count_matrix = count_vectorizer.fit_transform(data['processed_full_review'])\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(count_matrix, data['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the SVC model with specified parameters, using C to control L2 regularization\n",
    "# A smaller C value increases L2 regularization strength\n",
    "svm_model = SVC(kernel='linear', C=1, class_weight='balanced', random_state=42)  # Adjust C as needed for regularization strength\n",
    "\n",
    "# Perform cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cross_val_scores = cross_val_score(svm_model, X_train, y_train, cv=skf, scoring='accuracy')\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "\n",
    "# Print results\n",
    "print(\"Cross-Validation Accuracy Scores:\", cross_val_scores)\n",
    "print(\"Mean Cross-Validation Accuracy:\", cross_val_scores.mean())\n",
    "print(\"SVM(linear) Accuracy:\", accuracy_score(y_test, svm_predictions))\n",
    "print(\"SVM(linear) Classification Report:\\n\", classification_report(y_test, svm_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM (radial basis function (rbf)) + CountVec\n",
    "\n",
    "RBF kernel, a.k.a. Gaussian kernel, is a non-linear kernel that maps data to a higher-dimensional space. Allows for non-linear separation, where classes cannot be separated by a single straight line, can capture complex patterns by creating flexible decision boundaries.\n",
    "\n",
    "More computationally expensive and requires careful tuning of parameters to avoid overfitting.\n",
    "\n",
    "RBF is advantageous if sentiment classes overlap in complex ways or if there are subtle patterns in word combiantions that are harder to capture linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy Scores: [0.83396636 0.8307108  0.83776451 0.8475312  0.83170467]\n",
      "Mean Cross-Validation Accuracy: 0.8363355078316699\n",
      "SVM(linear) Accuracy: 0.8446180555555556\n",
      "SVM(linear) Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7201    0.8319    0.7720       470\n",
      "     Neutral     0.4402    0.5000    0.4682       228\n",
      "    Positive     0.9594    0.8973    0.9273      1606\n",
      "\n",
      "    accuracy                         0.8446      2304\n",
      "   macro avg     0.7065    0.7431    0.7225      2304\n",
      "weighted avg     0.8592    0.8446    0.8502      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=1000)\n",
    "count_matrix = count_vectorizer.fit_transform(data['processed_full_review'])\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(count_matrix, data['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the SVC model with specified parameters, using C to control L2 regularization\n",
    "# A smaller C value increases L2 regularization strength\n",
    "svm_model = SVC(kernel='rbf', C=1, class_weight='balanced', random_state=42)  # Adjust C as needed for regularization strength\n",
    "\n",
    "# Perform cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cross_val_scores = cross_val_score(svm_model, X_train, y_train, cv=skf, scoring='accuracy')\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "\n",
    "# Print results\n",
    "print(\"Cross-Validation Accuracy Scores:\", cross_val_scores)\n",
    "print(\"Mean Cross-Validation Accuracy:\", cross_val_scores.mean())\n",
    "print(\"SVM(linear) Accuracy:\", accuracy_score(y_test, svm_predictions))\n",
    "print(\"SVM(linear) Classification Report:\\n\", classification_report(y_test, svm_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM (linear) + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM model with Word2Vec embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance:\n",
      "Accuracy: 0.7011 (70.11%)\n",
      "F1 Score: 0.7050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation Results:\n",
      "F1 Scores: [0.73973663 0.72109434 0.74379389 0.77137242 0.65867796]\n",
      "Mean F1: 0.7269\n",
      "Std F1: 0.0377\n",
      "\n",
      "Accuracy Scores: [0.75434028 0.72048611 0.75998264 0.77203647 0.65392966]\n",
      "Mean Accuracy: 0.7322\n",
      "Std Accuracy: 0.0427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# # Download NLTK data (only needs to be done once)\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Define X (features) and y (target)\n",
    "X = data['processed_full_review']\n",
    "y = data['sentiment']\n",
    "\n",
    "# Encode sentiment labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Step 1: Tokenize sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in X]\n",
    "\n",
    "# Step 2: Train Word2Vec model\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1,  # Skip-gram model (usually better for classification)\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Step 3: Convert each sentence to an average vector\n",
    "def get_sentence_vector(sentence, model, vector_size):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    word_vectors = [\n",
    "        model.wv[word] \n",
    "        for word in words \n",
    "        if word in model.wv\n",
    "    ]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Convert all sentences to their corresponding average vectors\n",
    "X_vectors = np.array([\n",
    "    get_sentence_vector(sentence, word2vec_model, word2vec_model.vector_size) \n",
    "    for sentence in X\n",
    "])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_vectors_scaled = scaler.fit_transform(X_vectors)\n",
    "\n",
    "# Step 4: Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vectors_scaled, \n",
    "    y_encoded, \n",
    "    test_size=0.3, \n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Step 5: Define and train the SVM model\n",
    "model = SVC(\n",
    "    kernel='linear',\n",
    "    max_iter=5000,  # Increased max_iter for better convergence\n",
    "    C=1.0,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "print(\"Training SVM model with Word2Vec embeddings...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy * 100:.2f}%)\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Step 7: Stratified 5-fold cross-validation with F1 score and accuracy\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_f1_scores = cross_val_score(\n",
    "    model, \n",
    "    X_vectors_scaled, \n",
    "    y_encoded, \n",
    "    cv=skf,\n",
    "    scoring=make_scorer(f1_score, average='weighted')\n",
    ")\n",
    "\n",
    "cv_accuracy_scores = cross_val_score(\n",
    "    model, \n",
    "    X_vectors_scaled, \n",
    "    y_encoded, \n",
    "    cv=skf,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"F1 Scores: {cv_f1_scores}\")\n",
    "print(f\"Mean F1: {np.mean(cv_f1_scores):.4f}\")\n",
    "print(f\"Std F1: {np.std(cv_f1_scores):.4f}\")\n",
    "print(f\"\\nAccuracy Scores: {cv_accuracy_scores}\")\n",
    "print(f\"Mean Accuracy: {np.mean(cv_accuracy_scores):.4f}\")\n",
    "print(f\"Std Accuracy: {np.std(cv_accuracy_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM (rbf) + Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM model with Word2Vec embeddings...\n",
      "\n",
      "Model Performance:\n",
      "Accuracy: 0.8220 (82.20%)\n",
      "F1 Score: 0.8382\n",
      "\n",
      "Cross-Validation Results:\n",
      "F1 Scores: [0.84429887 0.85226461 0.83576394 0.84194134 0.84968665]\n",
      "Mean F1: 0.8448\n",
      "Std F1: 0.0058\n",
      "\n",
      "Accuracy Scores: [0.82855903 0.83637153 0.81944444 0.82544507 0.83586626]\n",
      "Mean Accuracy: 0.8291\n",
      "Std Accuracy: 0.0064\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# # Download NLTK data (only needs to be done once)\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Define X (features) and y (target)\n",
    "X = data['processed_full_review']\n",
    "y = data['sentiment']\n",
    "\n",
    "# Encode sentiment labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Step 1: Tokenize sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in X]\n",
    "\n",
    "# Step 2: Train Word2Vec model\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1,  # Skip-gram model (usually better for classification)\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Step 3: Convert each sentence to an average vector\n",
    "def get_sentence_vector(sentence, model, vector_size):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    word_vectors = [\n",
    "        model.wv[word] \n",
    "        for word in words \n",
    "        if word in model.wv\n",
    "    ]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Convert all sentences to their corresponding average vectors\n",
    "X_vectors = np.array([\n",
    "    get_sentence_vector(sentence, word2vec_model, word2vec_model.vector_size) \n",
    "    for sentence in X\n",
    "])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_vectors_scaled = scaler.fit_transform(X_vectors)\n",
    "\n",
    "# Step 4: Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vectors_scaled, \n",
    "    y_encoded, \n",
    "    test_size=0.3, \n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Step 5: Define and train the SVM model\n",
    "model = SVC(\n",
    "    kernel='rbf',\n",
    "    max_iter=5000,  # Increased max_iter for better convergence\n",
    "    C=1.0,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "print(\"Training SVM model with Word2Vec embeddings...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy * 100:.2f}%)\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Step 7: Stratified 5-fold cross-validation with F1 score and accuracy\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_f1_scores = cross_val_score(\n",
    "    model, \n",
    "    X_vectors_scaled, \n",
    "    y_encoded, \n",
    "    cv=skf,\n",
    "    scoring=make_scorer(f1_score, average='weighted')\n",
    ")\n",
    "\n",
    "cv_accuracy_scores = cross_val_score(\n",
    "    model, \n",
    "    X_vectors_scaled, \n",
    "    y_encoded, \n",
    "    cv=skf,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"F1 Scores: {cv_f1_scores}\")\n",
    "print(f\"Mean F1: {np.mean(cv_f1_scores):.4f}\")\n",
    "print(f\"Std F1: {np.std(cv_f1_scores):.4f}\")\n",
    "print(f\"\\nAccuracy Scores: {cv_accuracy_scores}\")\n",
    "print(f\"Mean Accuracy: {np.mean(cv_accuracy_scores):.4f}\")\n",
    "print(f\"Std Accuracy: {np.std(cv_accuracy_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM (linear) + FastText "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance on Test Set:\n",
      "Accuracy: 0.7121 (71.21%)\n",
      "F1 Score: 0.6934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation Results:\n",
      "F1 Scores: [0.68448241 0.70567724 0.70071486 0.6980302  0.70911626]\n",
      "Mean F1: 0.6996\n",
      "Std F1: 0.0085\n",
      "\n",
      "Accuracy Scores: [0.67491319 0.72265625 0.71961806 0.71775944 0.71081198]\n",
      "Mean Accuracy: 0.7092\n",
      "Std Accuracy: 0.0176\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# # Download NLTK data (only needs to be done once)\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Define X (features) and y (target)\n",
    "X = data['processed_full_review']\n",
    "y = data['sentiment']\n",
    "\n",
    "# Encode sentiment labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Step 1: Tokenize sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in X]\n",
    "\n",
    "# Step 2: Train FastText model\n",
    "fasttext_model = FastText(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1,  # Skip-gram model (usually better for classification)\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Step 3: Convert each sentence to an average vector\n",
    "def get_sentence_vector(sentence, model, vector_size):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    word_vectors = [\n",
    "        model.wv[word] \n",
    "        for word in words \n",
    "        if word in model.wv\n",
    "    ]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Convert all sentences to their corresponding average vectors\n",
    "X_vectors = np.array([\n",
    "    get_sentence_vector(sentence, fasttext_model, fasttext_model.vector_size) \n",
    "    for sentence in X\n",
    "])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_vectors_scaled = scaler.fit_transform(X_vectors)\n",
    "\n",
    "# Step 4: Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vectors_scaled, \n",
    "    y_encoded, \n",
    "    test_size=0.3, \n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Step 5: Define and train the SVM model\n",
    "model = SVC(\n",
    "    kernel='linear',\n",
    "    max_iter=5000,  # Increased max_iter for better convergence\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "print(\"Training SVM model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nModel Performance on Test Set:\")\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy * 100:.2f}%)\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Step 7: Stratified 5-fold cross-validation with F1 score and accuracy\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validation for F1 score\n",
    "cv_f1_scores = cross_val_score(\n",
    "    model, \n",
    "    X_vectors_scaled, \n",
    "    y_encoded, \n",
    "    cv=skf,\n",
    "    scoring=make_scorer(f1_score, average='weighted')\n",
    ")\n",
    "\n",
    "# Cross-validation for accuracy\n",
    "cv_accuracy_scores = cross_val_score(\n",
    "    model, \n",
    "    X_vectors_scaled, \n",
    "    y_encoded, \n",
    "    cv=skf,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"F1 Scores: {cv_f1_scores}\")\n",
    "print(f\"Mean F1: {np.mean(cv_f1_scores):.4f}\")\n",
    "print(f\"Std F1: {np.std(cv_f1_scores):.4f}\")\n",
    "print(f\"\\nAccuracy Scores: {cv_accuracy_scores}\")\n",
    "print(f\"Mean Accuracy: {np.mean(cv_accuracy_scores):.4f}\")\n",
    "print(f\"Std Accuracy: {np.std(cv_accuracy_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM (rbf) + FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM model...\n",
      "\n",
      "Model Performance on Test Set:\n",
      "Accuracy: 0.8302 (83.02%)\n",
      "F1 Score: 0.8449\n",
      "\n",
      "Cross-Validation Results:\n",
      "F1 Scores: [0.84781799 0.85314662 0.84234822 0.83792771 0.85410042]\n",
      "Mean F1: 0.8471\n",
      "Std F1: 0.0062\n",
      "\n",
      "Accuracy Scores: [0.83289931 0.83854167 0.82682292 0.82240556 0.8423795 ]\n",
      "Mean Accuracy: 0.8326\n",
      "Std Accuracy: 0.0073\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# # Download NLTK data (only needs to be done once)\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Define X (features) and y (target)\n",
    "X = data['processed_full_review']\n",
    "y = data['sentiment']\n",
    "\n",
    "# Encode sentiment labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Step 1: Tokenize sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in X]\n",
    "\n",
    "# Step 2: Train FastText model\n",
    "fasttext_model = FastText(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1,  # Skip-gram model (usually better for classification)\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Step 3: Convert each sentence to an average vector\n",
    "def get_sentence_vector(sentence, model, vector_size):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    word_vectors = [\n",
    "        model.wv[word] \n",
    "        for word in words \n",
    "        if word in model.wv\n",
    "    ]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Convert all sentences to their corresponding average vectors\n",
    "X_vectors = np.array([\n",
    "    get_sentence_vector(sentence, fasttext_model, fasttext_model.vector_size) \n",
    "    for sentence in X\n",
    "])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_vectors_scaled = scaler.fit_transform(X_vectors)\n",
    "\n",
    "# Step 4: Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vectors_scaled, \n",
    "    y_encoded, \n",
    "    test_size=0.3, \n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Step 5: Define and train the SVM model\n",
    "model = SVC(\n",
    "    kernel='rbf',\n",
    "    max_iter=5000,  # Increased max_iter for better convergence\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "print(\"Training SVM model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nModel Performance on Test Set:\")\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy * 100:.2f}%)\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Step 7: Stratified 5-fold cross-validation with F1 score and accuracy\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validation for F1 score\n",
    "cv_f1_scores = cross_val_score(\n",
    "    model, \n",
    "    X_vectors_scaled, \n",
    "    y_encoded, \n",
    "    cv=skf,\n",
    "    scoring=make_scorer(f1_score, average='weighted')\n",
    ")\n",
    "\n",
    "# Cross-validation for accuracy\n",
    "cv_accuracy_scores = cross_val_score(\n",
    "    model, \n",
    "    X_vectors_scaled, \n",
    "    y_encoded, \n",
    "    cv=skf,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"F1 Scores: {cv_f1_scores}\")\n",
    "print(f\"Mean F1: {np.mean(cv_f1_scores):.4f}\")\n",
    "print(f\"Std F1: {np.std(cv_f1_scores):.4f}\")\n",
    "print(f\"\\nAccuracy Scores: {cv_accuracy_scores}\")\n",
    "print(f\"Mean Accuracy: {np.mean(cv_accuracy_scores):.4f}\")\n",
    "print(f\"Std Accuracy: {np.std(cv_accuracy_scores):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf217",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
