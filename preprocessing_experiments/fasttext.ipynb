{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistical functions\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# For concurrency (running functions in parallel)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# For caching (to speed up repeated function calls)\n",
    "from functools import lru_cache\n",
    "\n",
    "# For progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotting and Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Language Detection packages\n",
    "# `langdetect` for detecting language\n",
    "from langdetect import detect as langdetect_detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "# `langid` for an alternative language detection method\n",
    "from langid import classify as langid_classify\n",
    "\n",
    "# Text Preprocessing and NLP\n",
    "# Stopwords (common words to ignore) from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Tokenizing sentences/words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Lemmatization (converting words to their base form)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "# Regular expressions for text pattern matching\n",
    "import re\n",
    "\n",
    "# Word Cloud generation\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# For generating n-grams\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Libraries for Word2Vec and Logistic Regression\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, make_scorer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"final_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText Embeddings + log regression\n",
    "FastText is an extension of Word2Vec developed by Facebookâ€™s AI Research (FAIR). While Word2Vec treats each word as a unique token, FastText breaks words into character n-grams (subword information). This means that it can generate vectors for words that were not seen during training, as long as their subwords were seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "# Ensure you have the required NLTK package\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define X (features) and y (target) based on multiclass sentiment\n",
    "X = data['processed_full_review']  # Review text\n",
    "y = data['sentiment']\n",
    "\n",
    "# Step 1: Tokenize the sentences\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence.lower()) for sentence in X]\n",
    "\n",
    "# Step 2: Train the FastText model\n",
    "fasttext_model = FastText(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 3: Function to average word vectors for each sentence\n",
    "def get_sentence_vector(sentence, model, vector_size):\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)  # Return a zero vector if no words are found\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Step 4: Convert each sentence to its FastText vector representation\n",
    "X_vectors = np.array([get_sentence_vector(sentence, fasttext_model, 100) for sentence in X])\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectors, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 6: Logistic regression model (for multiclass classification)\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predicting and evaluating\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Output the accuracy and F1 score\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Model F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Step 8: Implement Cross-Validation with F1 scoring\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_f1_scores = cross_val_score(model, X_vectors, y, cv=kf, scoring=make_scorer(f1_score, average='weighted'))\n",
    "\n",
    "# Output cross-validation F1 score results\n",
    "print(f\"Cross-Validation F1 Scores: {cv_f1_scores}\")\n",
    "print(f\"Average Cross-Validation F1 Score: {np.mean(cv_f1_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText Embeddings + random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have the required NLTK package\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define X (features) and y (target) based on multiclass sentiment\n",
    "X = data['processed_full_review']  # Review text\n",
    "y = data['sentiment']\n",
    "\n",
    "# Step 1: Tokenize the sentences\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence.lower()) for sentence in X]\n",
    "\n",
    "# Step 2: Train the FastText model\n",
    "fasttext_model = FastText(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 3: Function to average word vectors for each sentence\n",
    "def get_sentence_vector(sentence, model, vector_size):\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)  # Return a zero vector if no words are found\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Step 4: Convert each sentence to its FastText vector representation\n",
    "X_vectors = np.array([get_sentence_vector(sentence, fasttext_model, 100) for sentence in X])\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectors, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 6: Random Forest model (for multiclass classification)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predicting and evaluating\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Output the accuracy and F1 score\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Model F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Step 8: Implement Cross-Validation with F1 scoring\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_f1_scores = cross_val_score(model, X_vectors, y, cv=kf, scoring=make_scorer(f1_score, average='weighted'))\n",
    "\n",
    "# Output cross-validation F1 score results\n",
    "print(f\"Cross-Validation F1 Scores: {cv_f1_scores}\")\n",
    "print(f\"Average Cross-Validation F1 Score: {np.mean(cv_f1_scores):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
