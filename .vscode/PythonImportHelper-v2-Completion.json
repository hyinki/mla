[
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "concurrent.futures",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "Lock",
        "importPath": "threading",
        "description": "threading",
        "isExtraImport": true,
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "get_scraperapi_url",
        "kind": 2,
        "importPath": "web_scraping.skytrax_web_scraping",
        "description": "web_scraping.skytrax_web_scraping",
        "peekOfCode": "def get_scraperapi_url(url):\n    return f'http://api.scraperapi.com?api_key={SCRAPERAPI_KEY}&url={url}'\ndef scrape_page(url):\n    headers = {\"User-Agent\": random.choice(user_agents)}\n    response = requests.get(get_scraperapi_url(url), headers=headers)\n    if response.status_code != 200:\n        print(f\"Failed to retrieve data from {url}. Status code: {response.status_code}\")\n        return []\n    soup = BeautifulSoup(response.content, 'html.parser')\n    scraped_data = []",
        "detail": "web_scraping.skytrax_web_scraping",
        "documentation": {}
    },
    {
        "label": "scrape_page",
        "kind": 2,
        "importPath": "web_scraping.skytrax_web_scraping",
        "description": "web_scraping.skytrax_web_scraping",
        "peekOfCode": "def scrape_page(url):\n    headers = {\"User-Agent\": random.choice(user_agents)}\n    response = requests.get(get_scraperapi_url(url), headers=headers)\n    if response.status_code != 200:\n        print(f\"Failed to retrieve data from {url}. Status code: {response.status_code}\")\n        return []\n    soup = BeautifulSoup(response.content, 'html.parser')\n    scraped_data = []\n    # Skytrax review container\n    review_containers = soup.find_all('article', {'itemprop': 'review'})",
        "detail": "web_scraping.skytrax_web_scraping",
        "documentation": {}
    },
    {
        "label": "scrape_multiple_pages_parallel",
        "kind": 2,
        "importPath": "web_scraping.skytrax_web_scraping",
        "description": "web_scraping.skytrax_web_scraping",
        "peekOfCode": "def scrape_multiple_pages_parallel(output_file, max_pages=50):\n    current_url = BASE_URL\n    all_reviews = []\n    seen_urls = set()\n    scraped_urls = 0\n    page_count = 0\n    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n        future_to_url = {executor.submit(scrape_page, current_url): current_url}\n        while future_to_url:\n            if page_count >= max_pages:",
        "detail": "web_scraping.skytrax_web_scraping",
        "documentation": {}
    },
    {
        "label": "SCRAPERAPI_KEY",
        "kind": 5,
        "importPath": "web_scraping.skytrax_web_scraping",
        "description": "web_scraping.skytrax_web_scraping",
        "peekOfCode": "SCRAPERAPI_KEY = ''\n# Base URL of the Skytrax Singapore Airlines review page\nBASE_URL = 'https://www.airlinequality.com/airline-reviews/singapore-airlines/?sortby=post_date%3ADesc&pagesize=100'\n# User-Agent rotation list\nuser_agents = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.0; rv:91.0) Gecko/20100101 Firefox/91.0\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"\n]\ndef get_scraperapi_url(url):",
        "detail": "web_scraping.skytrax_web_scraping",
        "documentation": {}
    },
    {
        "label": "BASE_URL",
        "kind": 5,
        "importPath": "web_scraping.skytrax_web_scraping",
        "description": "web_scraping.skytrax_web_scraping",
        "peekOfCode": "BASE_URL = 'https://www.airlinequality.com/airline-reviews/singapore-airlines/?sortby=post_date%3ADesc&pagesize=100'\n# User-Agent rotation list\nuser_agents = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.0; rv:91.0) Gecko/20100101 Firefox/91.0\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"\n]\ndef get_scraperapi_url(url):\n    return f'http://api.scraperapi.com?api_key={SCRAPERAPI_KEY}&url={url}'\ndef scrape_page(url):",
        "detail": "web_scraping.skytrax_web_scraping",
        "documentation": {}
    },
    {
        "label": "user_agents",
        "kind": 5,
        "importPath": "web_scraping.skytrax_web_scraping",
        "description": "web_scraping.skytrax_web_scraping",
        "peekOfCode": "user_agents = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.0; rv:91.0) Gecko/20100101 Firefox/91.0\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"\n]\ndef get_scraperapi_url(url):\n    return f'http://api.scraperapi.com?api_key={SCRAPERAPI_KEY}&url={url}'\ndef scrape_page(url):\n    headers = {\"User-Agent\": random.choice(user_agents)}\n    response = requests.get(get_scraperapi_url(url), headers=headers)",
        "detail": "web_scraping.skytrax_web_scraping",
        "documentation": {}
    },
    {
        "label": "output_file",
        "kind": 5,
        "importPath": "web_scraping.skytrax_web_scraping",
        "description": "web_scraping.skytrax_web_scraping",
        "peekOfCode": "output_file = 'skytrax_SIA_reviews.csv'\nscrape_multiple_pages_parallel(output_file)",
        "detail": "web_scraping.skytrax_web_scraping",
        "documentation": {}
    },
    {
        "label": "get_scraperapi_url",
        "kind": 2,
        "importPath": "web_scraping.tripadvisor_web_scraping",
        "description": "web_scraping.tripadvisor_web_scraping",
        "peekOfCode": "def get_scraperapi_url(url):\n    return f'http://api.scraperapi.com?api_key={SCRAPERAPI_KEY}&url={url}'\ndef scrape_page(url, review_counts, max_reviews_per_year):\n    headers = {\"User-Agent\": random.choice(user_agents)}\n    response = requests.get(get_scraperapi_url(url), headers=headers)\n    if response.status_code != 200:\n        print(f\"Failed to retrieve data from {url}. Status code: {response.status_code}\")\n        return []\n    soup = BeautifulSoup(response.content, 'html.parser')\n    scraped_data = []",
        "detail": "web_scraping.tripadvisor_web_scraping",
        "documentation": {}
    },
    {
        "label": "scrape_page",
        "kind": 2,
        "importPath": "web_scraping.tripadvisor_web_scraping",
        "description": "web_scraping.tripadvisor_web_scraping",
        "peekOfCode": "def scrape_page(url, review_counts, max_reviews_per_year):\n    headers = {\"User-Agent\": random.choice(user_agents)}\n    response = requests.get(get_scraperapi_url(url), headers=headers)\n    if response.status_code != 200:\n        print(f\"Failed to retrieve data from {url}. Status code: {response.status_code}\")\n        return []\n    soup = BeautifulSoup(response.content, 'html.parser')\n    scraped_data = []\n    review_containers = soup.find_all('div', class_='lgfjP Gi z pBVnE MD bZHZM')\n    for container in review_containers:",
        "detail": "web_scraping.tripadvisor_web_scraping",
        "documentation": {}
    },
    {
        "label": "scrape_multiple_pages_parallel",
        "kind": 2,
        "importPath": "web_scraping.tripadvisor_web_scraping",
        "description": "web_scraping.tripadvisor_web_scraping",
        "peekOfCode": "def scrape_multiple_pages_parallel(output_file, max_reviews_per_year=1000,max_pages=300):\n    current_url = BASE_URL\n    all_reviews = []\n    review_counts = {'2022': 0, '2023': 0, '2024': 0}\n    seen_urls = set()\n    scraped_urls = 0\n    page_count = 0\n    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n        future_to_url = {executor.submit(scrape_page, current_url, review_counts, max_reviews_per_year): current_url}\n        while future_to_url and not all(count >= max_reviews_per_year for count in review_counts.values()):",
        "detail": "web_scraping.tripadvisor_web_scraping",
        "documentation": {}
    },
    {
        "label": "SCRAPERAPI_KEY",
        "kind": 5,
        "importPath": "web_scraping.tripadvisor_web_scraping",
        "description": "web_scraping.tripadvisor_web_scraping",
        "peekOfCode": "SCRAPERAPI_KEY = ''\n# Base URL of the TripAdvisor review page\nBASE_URL = 'https://www.tripadvisor.com.sg/Airline_Review-d8729151-Reviews-Singapore-Airlines'\n# User-Agent rotation list\nuser_agents = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36\"\n]\n# Lock for synchronizing review_counts updates",
        "detail": "web_scraping.tripadvisor_web_scraping",
        "documentation": {}
    },
    {
        "label": "BASE_URL",
        "kind": 5,
        "importPath": "web_scraping.tripadvisor_web_scraping",
        "description": "web_scraping.tripadvisor_web_scraping",
        "peekOfCode": "BASE_URL = 'https://www.tripadvisor.com.sg/Airline_Review-d8729151-Reviews-Singapore-Airlines'\n# User-Agent rotation list\nuser_agents = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36\"\n]\n# Lock for synchronizing review_counts updates\nlock = Lock()\ndef get_scraperapi_url(url):",
        "detail": "web_scraping.tripadvisor_web_scraping",
        "documentation": {}
    },
    {
        "label": "user_agents",
        "kind": 5,
        "importPath": "web_scraping.tripadvisor_web_scraping",
        "description": "web_scraping.tripadvisor_web_scraping",
        "peekOfCode": "user_agents = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Safari/605.1.15\",\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36\"\n]\n# Lock for synchronizing review_counts updates\nlock = Lock()\ndef get_scraperapi_url(url):\n    return f'http://api.scraperapi.com?api_key={SCRAPERAPI_KEY}&url={url}'\ndef scrape_page(url, review_counts, max_reviews_per_year):",
        "detail": "web_scraping.tripadvisor_web_scraping",
        "documentation": {}
    },
    {
        "label": "lock",
        "kind": 5,
        "importPath": "web_scraping.tripadvisor_web_scraping",
        "description": "web_scraping.tripadvisor_web_scraping",
        "peekOfCode": "lock = Lock()\ndef get_scraperapi_url(url):\n    return f'http://api.scraperapi.com?api_key={SCRAPERAPI_KEY}&url={url}'\ndef scrape_page(url, review_counts, max_reviews_per_year):\n    headers = {\"User-Agent\": random.choice(user_agents)}\n    response = requests.get(get_scraperapi_url(url), headers=headers)\n    if response.status_code != 200:\n        print(f\"Failed to retrieve data from {url}. Status code: {response.status_code}\")\n        return []\n    soup = BeautifulSoup(response.content, 'html.parser')",
        "detail": "web_scraping.tripadvisor_web_scraping",
        "documentation": {}
    },
    {
        "label": "output_file",
        "kind": 5,
        "importPath": "web_scraping.tripadvisor_web_scraping",
        "description": "web_scraping.tripadvisor_web_scraping",
        "peekOfCode": "output_file = 'tripadvisor_SIA_reviews_parallel.csv'\nscrape_multiple_pages_parallel(output_file, max_reviews_per_year=1000)",
        "detail": "web_scraping.tripadvisor_web_scraping",
        "documentation": {}
    }
]